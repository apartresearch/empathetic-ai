[{"id":2282380228,"microsoftAcademicId":2282380228,"numberInSourceReferences":109,"doi":"10.1038/SREP15924","title":"Measuring empathy for human and robot hand pain using electroencephalography.","authors":[{"LN":"Suzuki","FN":"Yutaka","affil":"Toyohashi University of Technology"},{"LN":"Galli","FN":"Lisa","affil":"Free University of Berlin"},{"LN":"Ikeda","FN":"Ayaka","affil":"Kyoto University"},{"LN":"Itakura","FN":"Shoji","affil":"Kyoto University"},{"LN":"Kitazaki","FN":"Michiteru","affil":"Toyohashi University of Technology"}],"year":2015,"journal":"Scientific Reports","references":[2110065044,1841352775,2116146623,2073825915,2096476371,2131018314,2081626486,1975132501,2157258415,2058521226,2147996559,1980083892,2139749000,2144872492,2056874158,2160683905,2005727185,2095962545,2085580706,2003350835,2125566732,2161917637,2151363564,2111385371,2019026055,1977851920,2127072348,2164704332,2114138722,2164482323,2012511508,2071833234,2026425968,2144641668,2035347516,2112386073,2061312820,2017711639,1556810297,1866272482,2137593360,2143910019,2165870631,2516929032,2087436880,2050072075],"citationsCount":108,"abstract":"This study provides the first physiological evidence of humans’ ability to empathize with robot pain and highlights the difference in empathy for humans and robots. We performed electroencephalography in 15 healthy adults who observed either human- or robot-hand pictures in painful or non-painful situations such as a finger cut by a knife. We found that the descending phase of the P3 component was larger for the painful stimuli than the non-painful stimuli, regardless of whether the hand belonged to a human or robot. In contrast, the ascending phase of the P3 component at the frontal-central electrodes was increased by painful human stimuli but not painful robot stimuli, though the interaction of ANOVA was not significant, but marginal. These results suggest that we empathize with humanoid robots in late top-down processing similarly to human others. However, the beginning of the top-down process of empathy is weaker for robots than for humans."},{"id":3121604639,"microsoftAcademicId":3121604639,"numberInSourceReferences":12,"doi":"10.1109/ROMAN.2015.7333675","title":"Empathic concern and the effect of stories in human-robot interaction","authors":[{"LN":"Darling","FN":"Kate","affil":"Massachusetts Institute of Technology"},{"LN":"Nandy","FN":"Palash","affil":"Massachusetts Institute of Technology"},{"LN":"Breazeal","FN":"Cynthia","affil":"Massachusetts Institute of Technology"}],"year":2015,"journal":"2015 24th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)","references":[1841352775,1853322427,2044663075,2021641437,2123393961,2144872492,2067609776,2160379528,1681671561,2003350835,2151363564,2152536828,1567088084,1986578153,2134818908,2148999704,1980785352,2048596731,2539563437,1556810297,2027829125,2066446105,2010883446,2547813422,3030292250],"citationsCount":74,"citationContext":{"1556810297":["have shown that we treat computers as social actors [19, 16], and others have shown similar perceptions of virtual characters [14, 24] and robots [5, 11, 31, 18, 27]."],"1567088084":["Perceived animacy relates to humans?” tendency to form emotional attachments to virtual characters [17, 21] and robotic objects [3, 4, 23, 28, 6, 13]."],"1681671561":["Robots that people become emotionally attached to are often given human names [1, 27, 23].","have shown that we treat computers as social actors [19, 16], and others have shown similar perceptions of virtual characters [14, 24] and robots [5, 11, 31, 18, 27]."],"1841352775":["have shown that we treat computers as social actors [19, 16], and others have shown similar perceptions of virtual characters [14, 24] and robots [5, 11, 31, 18, 27]."],"1980785352":["Paepcke and Takayama have demonstrated that setting people’s expectations low rather than high for a robot’s competence leads to less disappointment and more positive evaluation of the robot [18].","have shown that we treat computers as social actors [19, 16], and others have shown similar perceptions of virtual characters [14, 24] and robots [5, 11, 31, 18, 27]."],"2003350835":[", in which subjects watched videos of robots, showed a correlation between subjects’ fantasy scores and their responses to the robots being mistreated [22].",", subjects were also asked to view videos of “robot torture” [22]."],"2010883446":["Personification and stories may influence the suspension of disbelief in a machine’s inanimacy [12], adding to people’s tendency to treat robots as social actors."],"2021641437":["have shown that we treat computers as social actors [19, 16], and others have shown similar perceptions of virtual characters [14, 24] and robots [5, 11, 31, 18, 27]."],"2027829125":["Items on the personal distress scale measure a tendency to feel fearful, apprehensive, and uncomfortable when witnessing negative experiences of others [8].","The study assessed participants’ physiological arousal and selfreported emotions as well as using the Interpersonal Reactivity Index [8] to test for subjects’ trait empathy."],"2044663075":["have shown that we treat computers as social actors [19, 16], and others have shown similar perceptions of virtual characters [14, 24] and robots [5, 11, 31, 18, 27]."],"2066446105":["have shown that we treat computers as social actors [19, 16], and others have shown similar perceptions of virtual characters [14, 24] and robots [5, 11, 31, 18, 27]."],"2067609776":["replicated the famous Milgram experiment [15] with virtual characters as the recipients of shock treatment and showed that subjects responded as if they were shocking real human beings [26]."],"2123393961":["[24] Building on much of the above work, our study tries to better understand emotional reactions and their causes by testing the effect of movement and stories on people’s hesitation to strike a robot.","have shown that we treat computers as social actors [19, 16], and others have shown similar perceptions of virtual characters [14, 24] and robots [5, 11, 31, 18, 27]."],"2134818908":["Perceived animacy relates to humans?” tendency to form emotional attachments to virtual characters [17, 21] and robotic objects [3, 4, 23, 28, 6, 13].","demonstrated that increased intelligence and amiability of robots contributes to increased perception of animacy [4].","measured subjects’ hesitation to switch off a robot they had interacted with, showing that the robot’s intelligence had a strong positive effect on hesitation to switch it off, in particular if the robot acted agreeable [4]."],"2144872492":["Empathy is generally considered to be the ability to experience and understand what others feel [10]."],"2148999704":["Perceived animacy relates to humans?” tendency to form emotional attachments to virtual characters [17, 21] and robotic objects [3, 4, 23, 28, 6, 13].","The study indicated (with some limitations) that subjects struck a robot fewer times when the robot displayed intelligent behavior [3]."],"2151363564":["Perceived animacy relates to humans?” tendency to form emotional attachments to virtual characters [17, 21] and robotic objects [3, 4, 23, 28, 6, 13]."],"2152536828":["showed people videos of robots with various anthropomorphic attributes (on a scale from mechanical to humanoid) being “mistreated” by humans [20]."],"2160379528":["Further research may prove to support the suggestion that there is a divide between virtual and physical in how humans perceive and respond to robots [2], and also that emotional reactions to physical robots are not just guided by fantasy and imagination.","demonstrated that compared to virtual presence, a robot’s physical presence increases unconscious human perception of the robot as a social partner [2]."],"3030292250":["Because empathy may influence people’s reactions to robots [6, 7], exploring its effects and causes can assist in improving design and implementation of robotic technology.","Perceived animacy relates to humans?” tendency to form emotional attachments to virtual characters [17, 21] and robotic objects [3, 4, 23, 28, 6, 13]."]},"abstract":"People have been shown to project lifelike attributes onto robots and to display behavior indicative of empathy in human-robot interaction. Our work explores the role of empathy by examining how humans respond to a simple robotic object when asked to strike it. We measure the effects of lifelike movement and stories on people's hesitation to strike the robot, and we evaluate the relationship between hesitation and people's trait empathy. Our results show that people with a certain type of high trait empathy (empathic concern) hesitate to strike the robots. We also find that high empathic concern and hesitation are more strongly related for robots with stories. This suggests that high trait empathy increases people's hesitation to strike a robot, and that stories may positively influence their empathic responses."},{"id":2012372995,"microsoftAcademicId":2012372995,"numberInSourceReferences":25,"doi":"10.1145/2696454.2696495","title":"Design and Evaluation of a Peripheral Robotic Conversation Companion","authors":[{"LN":"Hoffman","FN":"Guy","affil":"Harvard University"},{"LN":"Zuckerman","FN":"Oren","affil":"Harvard University"},{"LN":"Hirschberger","FN":"Gilad","affil":"Interdisciplinary Center Herzliya"},{"LN":"Luria","FN":"Michal","affil":"Harvard University"},{"LN":"Shani-Sherman","FN":"Tal","affil":"Interdisciplinary Center Herzliya"}],"year":2015,"journal":"Proceedings of the Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction","references":[2149891956,2099019320,2082727067,2088337190,2085436006,2080543434,2006044072,2053365244,2110021966,2013040441,1992946368,2079880452,1986591913,2080321485,2044851123,2097634056,1968183910,2078753712,2143250836,2101941013,1964483583,2123356661,1994414170],"citationsCount":86,"citationContext":{"1964483583":["In separate rooms, couples completed the Couple's Problem Inventory [8], in which they rated the perceived severity of disagreement of a standard set of marital issues such as money, inlaws, and sex."],"1968183910":["Some other recent desktop robots use mobile devices as their sensor and processing platform [11, 17]."],"1986591913":["Emotional exchanges characterized by high levels of negative emotional behavior and low levels of positive emotional behavior have been associated with greater marital dissatisfaction and instability [9, 16]."],"1992946368":["In anthropomorphic robots, facial expressions are often used to express emotions, either on a screen [7, 20] or using actuated facial features [1, 19]."],"1994414170":["Secondary action is an animation principle which can improve the motion characteristic of a robot by adding a passive DoF that is influenced by the actuated movement and physical constraints, including gravity, enriching the dynamic perception of the robot [6]."],"2006044072":[", for example, found that loud voice, yelling and screaming is perceived as an expression of anger [23]."],"2013040441":["Robots that do not have an expressive face or are nonanthropomorphic at all can use gestures to express emotions [2, 13]."],"2044851123":["In some cases, for robots that have no social articulation at all, such as UAVs, path planning has been used to express emotions [22]."],"2053365244":["Emotional exchanges characterized by high levels of negative emotional behavior and low levels of positive emotional behavior have been associated with greater marital dissatisfaction and instability [9, 16]."],"2078753712":["This measure was validated and found reliable in previous studies [10, 12]."],"2079880452":["Some other recent desktop robots use mobile devices as their sensor and processing platform [11, 17].","The robot’s system design uses a smartphone as the main sensing and computing hardware [11], and includes four main components: An Android smartphone running the sensing and control software of the robot, a IOIO microcontroller board linking the smartphone to the motors, two servo motors, and a mechanical structure using a variety of linkages to express the robot’s gestures.","This module is based on the Gesture System in [11]."],"2080321485":["This measure was validated and found reliable in previous studies [10, 12]."],"2080543434":["In anthropomorphic robots, facial expressions are often used to express emotions, either on a screen [7, 20] or using actuated facial features [1, 19]."],"2085436006":["As suggested by Hoffman & Ju [13], we explored the relationship between robot appearance and movement in a series of 3D animation studies.","Robots that do not have an expressive face or are nonanthropomorphic at all can use gestures to express emotions [2, 13].","We designed Kip1 using a combined interaction, animation, and industrial design process, similar to the one proposed by Hoffman & Ju [13]."],"2088337190":["In particular, Kip1 exemplifies the following core aspects of TUI: (a) “coupling digital information to everyday physical objects and environments“ [15], and (b) “providing tangible representation to digital information” [21]."],"2097634056":["In anthropomorphic robots, facial expressions are often used to express emotions, either on a screen [7, 20] or using actuated facial features [1, 19]."],"2099019320":["describe the capability to express emotions as one of the indicators of socially interactive robots [5]."],"2101941013":["A similar study by Bergstrom & Karahalios used a “conversation clock” screen that visualized the time each participant talked, but without associating the visualization with a specific participant [3]."],"2110021966":["used a shared display in a group interaction, showing how much each participant contributed to the conversation [4]."],"2123356661":["Kip1 also follows the “Objects for Change” [24] principle of implementing established behavior change techniques in the design of a TUI device."],"2143250836":["Based on Leveson and Gottman [18], the couple was asked to discuss the chosen topic for fifteen minutes."]},"abstract":"We present the design, implementation, and evaluation of a peripheral empathy-evoking robotic conversation companion, Kip1. The robot’s function is to increase people’s awareness to the effect of their behavior towards others, potentially leading to behavior change. Specifically, Kip1 is designed to promote nonaggressive conversation between people. It monitors theconversation’s nonverbal aspects and maintains an emotional model of its reaction to the conversation. If the conversation seems calm, Kip1 responds by a gesture designed to communicate curious interest. If the conversation seems aggressive, Kip1 responds by a gesture designed to communicate fear. We describe the design process of Kip1, guided by the principles of peripheral and evocative. We detail its hardware and software systems, and a study evaluating the effects of the robot’s autonomous behavior on couples’ conversations. We find support for our design goals. A conversation companion reacting to the conversation led to more gaze attention, but not more verbal distraction, compared to a robot that moves but does not react to the conversation. This suggests that robotic devices could be designed as companions tohuman-human interaction without compromising the natural communication flow between people. Participants also rated the reacting robot as having significantly more social human charactertraits and as being significantly more similar to them. This points to the robot’s potential to elicit people’s empathy.Categories and Subject Descriptors H.1.2 [Models and Principles]: User/Machine Systems; J.4 [Computer Applications]: Social and Behavioral Sciences— psychology. General Terms Experimentation, Human Factors."},{"id":2963382311,"microsoftAcademicId":2963382311,"numberInSourceReferences":19,"doi":"10.1007/978-3-319-75487-1_14","title":"Towards Empathetic Human-Robot Interactions","authors":[{"LN":"Fung","FN":"Pascale","affil":"Hong Kong University of Science and Technology"},{"LN":"Bertero","FN":"Dario","affil":"Hong Kong University of Science and Technology"},{"LN":"Wan","FN":"Yan","affil":"Hong Kong University of Science and Technology"},{"LN":"Dey","FN":"Anik","affil":"Hong Kong University of Science and Technology"},{"LN":"Chan","FN":"Ricky Ho Yin","affil":"Hong Kong University of Science and Technology"},{"LN":"Siddique","FN":"Farhad Bin","affil":"Hong Kong University of Science and Technology"},{"LN":"Yang","FN":"Yang","affil":"Hong Kong University of Science and Technology"},{"LN":"Wu","FN":"Chien-Sheng","affil":"Hong Kong University of Science and Technology"},{"LN":"Lin","FN":"Ruixi","affil":"Hong Kong University of Science and Technology"}],"year":2016,"journal":"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","references":[2153635508,2095705004,2064675550,2158899491,1832693441,2147880316,1524333225,2120615054,38739846,2112739286,2085662862,1932847118,2962826786,1501669607,2155169782,2963921497,2152175008,2295001676,2423024114,2041400887,147964346,2251321385,2101151533,1968333171,2072919217,2004367337,2080507775,2464112344,1492025072,2574717657,1976927724,2398610312,2405202646,2402040266,56758227,2002253374,563758249],"citationsCount":20,"abstract":"Since the late 1990s when speech companies began providing their customer-service software in the market, people have gotten used to speaking to machines. As people interact more often with voice and gesture controlled machines, they expect the machines to recognize different emotions, and understand other high level communication features such as humor, sarcasm and intention. In order to make such communication possible, the machines need an empathy module in them, which is a software system that can extract emotions from human speech and behavior and can decide the correct response of the robot. Although research on empathetic robots is still in the primary stage, current methods involve using signal processing techniques, sentiment analysis and machine learning algorithms to make robots that can ‘understand’ human emotion. Other aspects of human-robot interaction include facial expression and gesture recognition, as well as robot movement to convey emotion and intent. We propose Zara the Supergirl as a prototype system of empathetic robots. It is a software-based virtual android, with an animated cartoon character to present itself on the screen. She will get ‘smarter’ and more empathetic, by having machine learning algorithms, and gathering more data and learning from it. In this paper, we present our work so far in the areas of deep learning of emotion and sentiment recognition, as well as humor recognition. We hope to explore the future direction of android development and how it can help improve people’s lives."},{"id":2048596731,"microsoftAcademicId":2048596731,"numberInSourceReferences":50,"doi":"10.1145/2696454.2696471","title":"Poor Thing! Would You Feel Sorry for a Simulated Robot?: A comparison of empathy toward a physical and a simulated robot","authors":[{"LN":"Seo","FN":"Stela H.","affil":"University of Manitoba"},{"LN":"Geiskkovitch","FN":"Denise","affil":"University of Manitoba"},{"LN":"Nakane","FN":"Masayuki","affil":"University of Manitoba"},{"LN":"King","FN":"Corey","affil":"ZenFri Inc., Winnipeg, Manitoba, Canada"},{"LN":"Young","FN":"James E.","affil":"University of Manitoba"}],"year":2015,"journal":"Proceedings of the Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction","references":[2546408443,1841352775,1992745256,2126514505,1853322427,1980083892,2043711811,2150781787,2127241481,2105569914,2082173922,2107288847,1681671561,2006179246,2072919217,2152536828,2017578285,2098225165,1989426632,2137020039,2072683973,2097634056,2148999704,2003276682,1968634614,2135115712,1994090379,2057009224,1619839657,2101542421,1963952998,2146939392,2153191985,15672360,2093354302,2471868678,2131965945,2043846795],"citationsCount":84,"citationContext":{"15672360":["This also follows for simulated work, for example, that people may prefer to interact with [22,30] or play a game with a real robot instead of a simulated one [20]."],"1619839657":["Some research has shown how people may appreciate if robots themselves demonstrate empathy toward others [25]."],"1681671561":["In such cases, the social interaction can be convincing to the point where people develop an attachment to the robot and experience negative emotions if something bad happens to it [14,35]."],"1841352775":["While there is evidence that simulated agents also have social presence [28], prior work suggests that robots may have more social presence than simulations [19,21]."],"1853322427":[", several weeks prior) [3,10,32]."],"1968634614":[") or external involuntary gestures such as facial expressions [11,16].",", [11,18]), whereas situational empathy can be used to consider the impact that a stimulus (such as something bad happening to another person, or a robot) may have on people at a specific time.",", see [9,11,18]), and so these methods are not useful for our purpose.","There is a difference between a person’s general tendency to empathize, dispositional empathy [34], and a person’s particular empathic response in a given situation, situational empathy [11]."],"1980083892":[", see [9,11,18]), and so these methods are not useful for our purpose."],"1989426632":["Another angle of research is to compare physical robots with onscreen agents, for example, showing that people may perceive agent emotions similarly between the two [5], people may engage more with a robot than a text-based computer [26], may speak differently to an on-screen agent or enjoy interacting with it less than to robot [12,22], or that there are unique trade-offs between the approaches that should be considered more deeply [36].","This also follows for simulated work, for example, that people may prefer to interact with [22,30] or play a game with a real robot instead of a simulated one [20].","This may explain a range of indirect effects reported in the literature, for example, that in comparison to a simulated robot people may trust a physical robot more [20,27], may speak differently to, and enjoy more interacting with, a physical robot [12,22], or that a person’s loneliness may impact how important having a physical robot is [21] – lonely people may appreciate a stronger social presence."],"1992745256":[", several weeks prior) [3,10,32].","This is called cognitive empathy [3,33]."],"1994090379":[") [12] may impact the results; such questions should be specifically investigated for actual simulations of robots.","Another angle of research is to compare physical robots with onscreen agents, for example, showing that people may perceive agent emotions similarly between the two [5], people may engage more with a robot than a text-based computer [26], may speak differently to an on-screen agent or enjoy interacting with it less than to robot [12,22], or that there are unique trade-offs between the approaches that should be considered more deeply [36].","This may explain a range of indirect effects reported in the literature, for example, that in comparison to a simulated robot people may trust a physical robot more [20,27], may speak differently to, and enjoy more interacting with, a physical robot [12,22], or that a person’s loneliness may impact how important having a physical robot is [21] – lonely people may appreciate a stronger social presence."],"2003276682":[", a boxed machine) [4]."],"2006179246":["More specific work on comparing real robots to simulations for task-oriented work has found that there may be an effect of the agent’s embodiment matching to the task [17] – for example, physical robots may be preferred when working in the physical world (such as receiving instructions to work on a physical button panel versus 2D on-screen panel [31])."],"2017578285":[", see [8,33]).","This is called cognitive empathy [3,33]."],"2043711811":["Much of this has been an indirect element of other work, for example, that people feel empathy toward robots is a key part of companion robots such as Paro [37].","Such robots can even be designed as social team members or personal companions, in an attempt to take advantage of social norms and people’s social tendencies: for example, to leverage existing social structures or to encourage positive empathic responses, which can have positive health benefits [37]."],"2057009224":["However, there are some studies that conversely report little effect found of simulated versus real robots [17,40].","More specific work on comparing real robots to simulations for task-oriented work has found that there may be an effect of the agent’s embodiment matching to the task [17] – for example, physical robots may be preferred when working in the physical world (such as receiving instructions to work on a physical button panel versus 2D on-screen panel [31])."],"2072683973":[", see [8,33])."],"2072919217":["Empathy can serve as an indicator of social connection with the robot, and as such can be used to analogously represent a range of social HRI scenarios that rely on such personal connections; empathy broadly is a common topic of study in HRI [15,23,38].","In social HRI, research generally reports that physical robots have higher social presence than agents or simulated robots [19,21,27, 38]."],"2082173922":[", a box) relies on self-reported engagement or preference [20,27,36].","In social HRI, research generally reports that physical robots have higher social presence than agents or simulated robots [19,21,27, 38].","This may explain a range of indirect effects reported in the literature, for example, that in comparison to a simulated robot people may trust a physical robot more [20,27], may speak differently to, and enjoy more interacting with, a physical robot [12,22], or that a person’s loneliness may impact how important having a physical robot is [21] – lonely people may appreciate a stronger social presence."],"2093354302":[", [11,18]), whereas situational empathy can be used to consider the impact that a stimulus (such as something bad happening to another person, or a robot) may have on people at a specific time.",", see [9,11,18]), and so these methods are not useful for our purpose.","Generally, empathy refers to the case where a person shares in another’s emotional state [18], where sympathy is the broader term of having concern for others [7], even if no emotional reaction takes place; these terms are often used interchangeably in practice.","The difficulty with such techniques is that they often require not only advanced equipment but also specialized expertise from an experienced team to analyze [18], making them less accessible to the broader HRI research audience."],"2097634056":[", robotic dog versus on-screen one-eyed monster) [5] and so other factors (agent shape and form, etc.","Another angle of research is to compare physical robots with onscreen agents, for example, showing that people may perceive agent emotions similarly between the two [5], people may engage more with a robot than a text-based computer [26], may speak differently to an on-screen agent or enjoy interacting with it less than to robot [12,22], or that there are unique trade-offs between the approaches that should be considered more deeply [36]."],"2098225165":["In social HRI, research generally reports that physical robots have higher social presence than agents or simulated robots [19,21,27, 38].","This may explain a range of indirect effects reported in the literature, for example, that in comparison to a simulated robot people may trust a physical robot more [20,27], may speak differently to, and enjoy more interacting with, a physical robot [12,22], or that a person’s loneliness may impact how important having a physical robot is [21] – lonely people may appreciate a stronger social presence.","While there is evidence that simulated agents also have social presence [28], prior work suggests that robots may have more social presence than simulations [19,21]."],"2101542421":[") or external involuntary gestures such as facial expressions [11,16]."],"2105569914":[", a box) relies on self-reported engagement or preference [20,27,36].","This also follows for simulated work, for example, that people may prefer to interact with [22,30] or play a game with a real robot instead of a simulated one [20].","This may explain a range of indirect effects reported in the literature, for example, that in comparison to a simulated robot people may trust a physical robot more [20,27], may speak differently to, and enjoy more interacting with, a physical robot [12,22], or that a person’s loneliness may impact how important having a physical robot is [21] – lonely people may appreciate a stronger social presence."],"2107288847":[") [7].","55 on the Batson scale [7]) than non-induced participants (M=55, SD=9, SE=2.","Generally, empathy refers to the case where a person shares in another’s emotional state [18], where sympathy is the broader term of having concern for others [7], even if no emotional reaction takes place; these terms are often used interchangeably in practice.","[7]."],"2126514505":["Further, for exploratory purposes we added an additional condition which merges the physical (real robot) and virtual conditions using a see-through mixed reality [24] display where a virtual robot appears on a physical table; thus while being a robot simulation, the interaction is still somewhat embedded into the participant’s space."],"2127241481":["Others compare real robots to videos of robots (with favorable results supporting the use of video) [13,39], or compare collocated robots to remote robots via a video feed [2], or robots to people [29], although this approach still requires realrobot programming and as such is not simulation as we address."],"2131965945":[", a box) relies on self-reported engagement or preference [20,27,36].","Another angle of research is to compare physical robots with onscreen agents, for example, showing that people may perceive agent emotions similarly between the two [5], people may engage more with a robot than a text-based computer [26], may speak differently to an on-screen agent or enjoy interacting with it less than to robot [12,22], or that there are unique trade-offs between the approaches that should be considered more deeply [36]."],"2135115712":["Empathy can serve as an indicator of social connection with the robot, and as such can be used to analogously represent a range of social HRI scenarios that rely on such personal connections; empathy broadly is a common topic of study in HRI [15,23,38].","In HRI, self-report methods for evaluating empathy have generally been scenario-specific, meeting the precise needs of the study being conducted [15,29].","More targeted work has shown that people have more empathy toward more anthropomorphic robots when shown videos of bad things happening to them [29], or that robots can encourage empathy toward them by mimicking peoples’ facial expressions or gestures [15]."],"2137020039":["Others compare real robots to videos of robots (with favorable results supporting the use of video) [13,39], or compare collocated robots to remote robots via a video feed [2], or robots to people [29], although this approach still requires realrobot programming and as such is not simulation as we address."],"2146939392":["Empathy can serve as an indicator of social connection with the robot, and as such can be used to analogously represent a range of social HRI scenarios that rely on such personal connections; empathy broadly is a common topic of study in HRI [15,23,38]."],"2148999704":["Although our gender analysis did not support this, as this observation is weakly supported by prior work [6] we believe that this should be explored further.","In previous work, people were asked to “kill” or “destroy” robots, thus making them an active participant in the negative action [6]."],"2150781787":[", several weeks prior) [3,10,32]."],"2152536828":["In HRI, self-report methods for evaluating empathy have generally been scenario-specific, meeting the precise needs of the study being conducted [15,29].","More targeted work has shown that people have more empathy toward more anthropomorphic robots when shown videos of bad things happening to them [29], or that robots can encourage empathy toward them by mimicking peoples’ facial expressions or gestures [15].","Others compare real robots to videos of robots (with favorable results supporting the use of video) [13,39], or compare collocated robots to remote robots via a video feed [2], or robots to people [29], although this approach still requires realrobot programming and as such is not simulation as we address."],"2153191985":["Another angle of research is to compare physical robots with onscreen agents, for example, showing that people may perceive agent emotions similarly between the two [5], people may engage more with a robot than a text-based computer [26], may speak differently to an on-screen agent or enjoy interacting with it less than to robot [12,22], or that there are unique trade-offs between the approaches that should be considered more deeply [36]."],"2471868678":["However, there are some studies that conversely report little effect found of simulated versus real robots [17,40]."]},"abstract":"In designing and evaluating human-robot interactions and interfaces, researchers often use a simulated robot due to the high cost of robots and time required to program them. However, it is important to consider how interaction with a simulated robot differs from a real robot; that is, do simulated robots provide authentic interaction? We contribute to a growing body of work that explores this question and maps out simulated-versus-real differences, by explicitly investigating empathy: how people empathize with a physical or simulated robot when something bad happens to it. Our results suggest that people may empathize more with a physical robot than a simulated one, a finding that has important implications on the generalizability and applicability of simulated HRI work. Empathy is particularly relevant to social HRI and is integral to, for example, companion and care robots. Our contribution additionally includes an original and reproducible HRI experimental design to induce empathy toward robots in laboratory settings, and an experimentally validated empathy-measuring instrument from psychology for use with HRI. Categories and Subject Descriptors H.5.2 [User Interfaces]: evaluation/methodology General Terms Experimentation and Human Factors."},{"id":1608468640,"microsoftAcademicId":1608468640,"numberInSourceReferences":170,"doi":"10.3389/FBIOE.2015.00064","title":"Can a Humanoid Face be Expressive? A Psychophysiological Investigation.","authors":[{"LN":"Lazzeri","FN":"Nicole","affil":"University of Pisa"},{"LN":"Mazzei","FN":"Daniele","affil":"University of Pisa"},{"LN":"Greco","FN":"Alberto","affil":"University of Pisa"},{"LN":"Rotesi","FN":"Annalisa","affil":"University of Florence"},{"LN":"Lanatà","FN":"Antonio","affil":"University of Pisa"},{"LN":"Rossi","FN":"Danilo Emilio De","affil":"University of Pisa"}],"year":2015,"journal":"Frontiers in Bioengineering and Biotechnology","references":[2285072859,2164777277,2099019320,2125127226,2084235337,2026699230,2293081071,1566413196,1993584577,2102573486,2053154970,2116628589,2911336608,1975238145,1264889757,2057409323,2053839006,2108385997,2129928382,2024218186,2118999532,1994405094,1525745722,2167741567,2098225165,2022373774,2170408452,2130571344,2026871208,2125651023,2164780916,2166393533,2025559738,2109923073,2103257739,2123912601,2039420410,2097634056,2003697795,2027309689,2083803218,2121318724,1988048624,2166527354,2162588367,2151288549,1970962031,2171069343,1973269950,1997683930,3114284325,1964351560,2001619467,2025300780,1969736828,2075818645,2041282815,1994688976,2087127635,1974971448],"citationsCount":23,"abstract":"Non-verbal signals expressed through body language play a crucial role in multi-modal human communication during social relations. Indeed, in all cultures facial expressions are the most universal and direct signs to express innate emotional cues. A human face conveys important information in social interactions and helps us to better understand our social partners and establish empathic links. Latest researches show that humanoid and social robots are becoming increasingly similar to humans, both aesthetically and expressively. However, their visual expressiveness is a crucial issue that must be improved to make these robots more realistic and intuitively perceivable by humans as not different from them. This study concerns the capability of a humanoid robot to exhibit emotion through facial expressions. More specifically, emotional signs performed by a humanoid robot have been compared with corresponding human facial expressions in terms of recognition rate and response time. The set of stimuli included standardized human expressions taken from an Ekman-based database and the same facial expressions performed by the robot. Furthermore, participants' psychophysiological responses have been explored to investigate whether there could be differences induced by interpreting robot or human emotional stimuli. Preliminary results show a trend to better recognize expressions performed by the robot than 2D photos or 3D models. Moreover no significant differences in the subjects' psychophysiological state have been found during the discrimination of facial expressions performed by the robot in comparison with the same task performed with 2D photos and 3D models."},{"id":2919701584,"microsoftAcademicId":2919701584,"numberInSourceReferences":79,"doi":"10.1098/RSTB.2018.0034","title":"A neurocognitive investigation of the impact of socializing with a robot on empathy for pain.","authors":[{"LN":"Cross","FN":"Emily S.","affil":"University of Glasgow"},{"LN":"Riddoch","FN":"Katie A.","affil":"Bangor University"},{"LN":"Pratts","FN":"Jaydan","affil":"Bangor University"},{"LN":"Titone","FN":"Simon","affil":"Bangor University"},{"LN":"Chaudhury","FN":"Bishakha","affil":"University of Glasgow"},{"LN":"Hortensius","FN":"Ruud","affil":"University of Glasgow"}],"year":2019,"journal":"Philosophical Transactions of the Royal Society B","references":[2105824687,2025175823,2017108196,1964694219,2116146623,2032568497,2011131455,2923742357,2073825915,2153480757,1989104072,2096578021,2131227926,2123950369,2021641437,2784547979,2147390495,1980083892,2139749000,1706861413,2155624710,1973030362,2100827418,2102453924,2521535695,2121533105,2085580706,2115742360,2216930413,2114700585,2009317323,2095221075,2768727455,2003350835,2155034734,2154051144,2792496527,2141563716,2052395732,2323939159,2292110642,2071802762,2029985196,2912641090,2210361125,2160475489,75277929,1994358520,2001559660,2012511508,199033134,2168883005,2799638175,2799696550,2801888051,2761505882,2223458618,2122943715,2257278489,2075074966,2117077933,1481783918,2610871231,1991496273,2610719416,2951874877],"citationsCount":12,"abstract":"To what extent can humans form social relationships with robots? In the present study, we combined functional neuroimaging with a robot socializing intervention to probe the flexibility of empathy, a core component of social relationships, towards robots. Twenty-six individuals underwent identical fMRI sessions before and after being issued a social robot to take home and interact with over the course of a week. While undergoing fMRI, participants observed videos of a human actor or a robot experiencing pain or pleasure in response to electrical stimulation. Repetition suppression of activity in the pain network, a collection of brain regions associated with empathy and emotional responding, was measured to test whether socializing with a social robot leads to greater overlap in neural mechanisms when observing human and robotic agents experiencing pain or pleasure. In contrast to our hypothesis, functional region-of-interest analyses revealed no change in neural overlap for agents after the socializing intervention. Similarly, no increase in activation when observing a robot experiencing pain emerged post-socializing. Whole-brain analysis showed that, before the socializing intervention, superior parietal and early visual regions are sensitive to novel agents, while after socializing, medial temporal regions show agent sensitivity. A region of the inferior parietal lobule was sensitive to novel emotions, but only during the pre-socializing scan session. Together, these findings suggest that a short socialization intervention with a social robot does not lead to discernible differences in empathy towards the robot, as measured by behavioural or brain responses. We discuss the extent to which long-term socialization with robots might shape social cognitive processes and ultimately our relationships with these machines. This article is part of the theme issue 'From social brains to social robots: applying neurocognitive insights to human-robot interaction'."},{"id":2001862949,"microsoftAcademicId":2001862949,"numberInSourceReferences":26,"doi":"10.1145/2696454.2696479","title":"When Children Teach a Robot to Write: An Autonomous Teachable Humanoid Which Uses Simulated Handwriting","authors":[{"LN":"Hood","FN":"Deanna","affil":"École Polytechnique Fédérale de Lausanne"},{"LN":"Lemaignan","FN":"Severin","affil":"École Polytechnique Fédérale de Lausanne"},{"LN":"Dillenbourg","FN":"Pierre","affil":"École Polytechnique Fédérale de Lausanne"}],"year":2015,"journal":"Proceedings of the Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction","references":[2012204020,2110437072,2398156266,2127241481,2053976921,2006905283,2140527237,2024212087,1571143165,2331525268,1577845799,2006593880,2110868090,59141947,2059285620,1608504125,2070191178,1993812309,2018561623],"citationsCount":156,"citationContext":{"1571143165":["The nao V4 humanoid robot, which has been purposely designed by Aldebaran Robotics to look approachable [8], is used for this work."],"1577845799":["Robots have been used as teachers or social partners to promote children’s learning in a range of contexts, most commonly related to language skills [9], and less often to physical skills (such as calligraphy [15])."],"1993812309":["Robots have been used as teachers or social partners to promote children’s learning in a range of contexts, most commonly related to language skills [9], and less often to physical skills (such as calligraphy [15])."],"2006593880":["In such activities, the embodied nature of the robot is appropriate as in interventions where motor mimicry is elicited [2] the arm motion for instance is, by itself, part of the teaching.","This is supported by the potential for motor mimicry to yield significant improvements in handwriting interventions in which letter formations are demonstrated to participants [2]."],"2006905283":["The Robot Operating System (ROS) is used for integration of the nao with external reference frames, such as the tablet’s location, using the tf transformation library [7]."],"2012204020":["Looking at the converse (humans teaching robots), Werfel notes in [22] that most of the work focuses on the robot’s benefits (in terms of language [19] or physical [17] skills, for example) rather than the learning experienced by the human tutor themselves."],"2018561623":["Handwriting difficulties in children at an early age often negatively affect the academic performance of the students [5], in addition to their self-esteem being adversely affected [14], causing them to shy away from expressing what they know [16]."],"2024212087":["The learning by teaching paradigm, which engages the target student in the act of teaching another, has been shown to produce motivational, meta-cognitive, and educational benefits in a range of disciplines [18]."],"2053976921":["Teachable computer-based agents have previously been used to encourage the “protégé effect”, wherein students invest more effort into learning when it is for a teachable agent than for themselves [4]."],"2059285620":["Handwriting difficulties in children at an early age often negatively affect the academic performance of the students [5], in addition to their self-esteem being adversely affected [14], causing them to shy away from expressing what they know [16]."],"2070191178":["Handwriting difficulties in children at an early age often negatively affect the academic performance of the students [5], in addition to their self-esteem being adversely affected [14], causing them to shy away from expressing what they know [16]."],"2110437072":["A robotic learning agent which employs the learning by teaching paradigm has previously been developed by Tanaka and Matsuzoe [21]."],"2110868090":["2696479 self-efficacy [6] results in children who are unmotivated to participate in such sessions, potentially leading to a developmental arrest in the acquisition of the skill."],"2127241481":["Furthermore, when compared to screen-based agents, robotic partners have been shown in some contexts to increase users’ compliance with tasks [1], maintain more effective long-term relationships [11], and produce greater learning gains when acting as tutors [12]."],"2140527237":["Furthermore, when compared to screen-based agents, robotic partners have been shown in some contexts to increase users’ compliance with tasks [1], maintain more effective long-term relationships [11], and produce greater learning gains when acting as tutors [12]."],"2331525268":["A conclusion drawn in a systematic review of handwriting intervention studies [10] is that any of the studies considered which involved fewer than two practice sessions per week and fewer than a total of 20 practice sessions, including homework, were not found to demonstrate effective results.","Successful interventions for children with handwriting difficulties involve the student in many sessions where they are engaged in physically practising the skill [10]."],"2398156266":["Furthermore, when compared to screen-based agents, robotic partners have been shown in some contexts to increase users’ compliance with tasks [1], maintain more effective long-term relationships [11], and produce greater learning gains when acting as tutors [12]."]},"abstract":"This article presents a novel robotic partner which children can teach handwriting. The system relies on the learning by teaching paradigm to build an interaction, so as to stimulate meta-cognition, empathy and increased self-esteem in the child user. We hypothesise that use of a humanoid robot in such a system could not just engage an unmotivated student, but could also present the opportunity for children to experience physically-induced benefits encountered during human-led handwriting interventions, such as motor mimicry. By leveraging simulated handwriting on a synchronised tablet display, a NAO humanoid robot with limited fine motor capabilities has been configured as a suitably embodied handwriting partner. Statistical shape models derived from principal component analysis of a dataset of adult-written letter trajectories allow the robot to draw purposefully deformed letters. By incorporating feedback from user demonstrations, the system is then able to learn the optimal parameters for the appropriate shape models. Preliminary in situ studies have been conducted with primary school classes to obtain insight into children's use of the novel system. Children aged 6-8 successfully engaged with the robot and improved its writing to a level which they were satisfied with. The validation of the interaction represents a significant step towards an innovative use for robotics which addresses a widespread and socially meaningful challenge in education."},{"id":2763083925,"microsoftAcademicId":2763083925,"numberInSourceReferences":82,"doi":"10.3389/FPSYG.2017.01663","title":"Robots As Intentional Agents: Using Neuroscientific Methods to Make Robots Appear More Social.","authors":[{"LN":"Wiese","FN":"Eva","affil":"George Mason University"},{"LN":"Metta","FN":"Giorgio","affil":"Istituto Italiano di Tecnologia"},{"LN":"Wykowska","FN":"Agnieszka","affil":"Istituto Italiano di Tecnologia"}],"year":2017,"journal":"Frontiers in Psychology","references":[1624854622,2125823313,2109453861,2116146623,2111613011,2052610531,2110631042,2150690631,2107544549,2120019338,1970287153,2107911338,2153480757,2124809053,2115040353,2025642716,2114414717,2128662472,2123947560,2019924339,2112433864,2136162329,2120040514,2050835671,2084260707,2131227926,2096456606,2124848740,2161222115,2118796430,1561130515,2044663075,2110997285,2110525007,1968372229,2044237036,2093410327,2143135888,2101809190,2053444187,2103551585,2047509178,2031531183,2139401165,2003653926,1479871685,2013098353,1986530040,2096453912,2158495800,2117155762,2015218205,1973030362,2108616861,2737972362,2111251561,2158907001,2098201217,2100827418,1987250353,2170217103,2102453924,2105741723,2118586055,1627886265,2139188400,2163138441,2121184995,2122535408,2046008309,2116526527,2114088572,2150854643,2005817178,2153632343,1984670080,2267107524,2101870300,1509191362,2068003671,2070844431,2149683980,2158709003,1976497813,2164318031,2163727435,2114739574,2070882801,2140527237,2112387466,1985682182,2117843795,2160871514,2012676995,2150423550,2050926560,2071042892,2149234824,2038089528,2061231754,2094482505,2108426047,2127748066,2074134568,2115673874,1995911123,1490958072,2113707145,2154851666,2165829812,2168743227,2023908618,2063238761,2068000469,1963661673,1977576450,2133968629,2168438636,2323939159,2066556081,2072267104,2154398789,2139606735,2103460252,2043356371,2111062500,1994617002,2098850353,1997730127,2112395830,2098089739,2061370872,2106559139,2008423926,1971224247,2155956364,2074869430,2066431836,2149951190,2071650248,2105535286,1968729467,2145234535,1984499509,2046345409,2140189272,1782625663,2224792037,2153396153,2144547508,2131884197,2146913448,2108126103,1807244228,2095719050,2111981270,2160475489,2116385068,2103544776,2162376483,2146989891,2103637423,2008293408,2104584247,2113517011,2096183091,2002432364,2025503365,2171264451,2027337585,1976399205,2282380228,2139203871,1970278724,2052176889,2415935486,2005774095,2032460447,2061500927,2010374223,2100531526,2152358681,2077805258,2156792113,2189715018,2172997593,2548641020,2116066868,2082752255,1994884977,2087876820,2124799425,2168883005,2010165379,2137113023,2549860918,2170638328,2341076643,2034061440,3015738550,1967598055,2160208693,2020299617,1985539757,2086213405,2147219982,1996437888,2514907158,2129199769,1972011061,2161410267,2042591137,3114284325,1996047860,2171031944,2094841258,2101882975,2082368635,2113471621,2626694174,1981919942,2565827948,2082034311,2140837820,2125972078,2016024809,2046511255,2033684779,1750687029,2155501207,1996638814,2213467510,2161973350,2029470626,3128182226,2526310956,2146783471,2759928903,2086203308,2531612369,2758275925,2477600173],"citationsCount":132,"abstract":"Robots are increasingly envisaged as our future cohabitants. However, while considerable progress has been made in recent years in terms of their technological realization, the ability of robots to interact with humans in an intuitive and social way is still quite limited. An important challenge for social robotics is to determine how to design robots that can perceive the user's needs, feelings, and intentions, and adapt to users over a broad range of cognitive abilities. It is conceivable that if robots were able to adequately demonstrate these skills, humans would eventually accept them as social companions. We argue that the best way to achieve this is using a systematic experimental approach based on behavioral and physiological neuroscience methods such as motion/eye-tracking, electroencephalography, or functional near-infrared spectroscopy embedded in interactive human-robot paradigms. This approach requires understanding how humans interact with each other, how they perform tasks together and how they develop feelings of social connection over time, and using these insights to formulate design principles that make social robots attuned to the workings of the human brain. In this review, we put forward the argument that the likelihood of artificial agents being perceived as social companions can be increased by designing them in a way that they are perceived as intentional agents that activate areas in the human brain involved in social-cognitive processing. We first review literature related to social-cognitive processes and mechanisms involved in human-human interactions, and highlight the importance of perceiving others as intentional agents to activate these social brain areas. We then discuss how attribution of intentionality can positively affect human-robot interaction by (a) fostering feelings of social connection, empathy and prosociality, and by (b) enhancing performance on joint human-robot tasks. Lastly, we describe circumstances under which attribution of intentionality to robot agents might be disadvantageous, and discuss challenges associated with designing social robots that are inspired by neuroscientific principles."},{"id":2963427815,"microsoftAcademicId":2963427815,"numberInSourceReferences":64,"doi":"10.1145/3300188","title":"Empathic Robot for Group Learning: A Field Study","authors":[{"LN":"Alves-Oliveira","FN":"Patrícia","affil":"INESC-ID"},{"LN":"Sequeira","FN":"Pedro","affil":"Northeastern University"},{"LN":"Melo","FN":"Francisco S.","affil":"Instituto Superior Técnico"},{"LN":"Castellano","FN":"Ginevra","affil":"Uppsala University"},{"LN":"Paiva","FN":"Ana","affil":"Instituto Superior Técnico"}],"year":2019,"journal":"ACM Transactions on Human-Robot Interaction (THRI)","references":[2116199508,2127076710,1966370548,2170748977,2153480757,2122575992,2887260263,2101606999,1975000068,1815090327,1593934207,2120040514,608658582,2111040806,1853322427,2112480356,2123950369,2077324971,2110437072,658821702,2060880492,2001862949,2398156266,2150854643,2053976921,2141861154,2095622232,2593982493,2150758961,103982469,2090201009,2145458549,2292558005,2594091522,2006458107,1709301467,2773817193,2792441573,2754425142,2332830836,1990266238,2616867613,13388985,2425816709,2072496161,2169143821,1981509185,2336960426,2612853719,64182589,2086106924,1986100731,2808512042,2594677124,2210403097,2782276836,2104010214,2028113924,2776198381,2791581064,1527068024,2110102748,2208253782,2092909411,2070123979,2065559115,2011578113,2483202404,2071477021,2059185220,2644731431,2165402343,2046019707,2588274542,2594633004,2279735562,2250275257,2146887121,2342425527,2784300384,2769071920,2142450231,2066532323,2089614309,2111092161,2771798189,2354029326,1508753152,2322715951,1501030528,3148826146,2550288607,2806395939,2903393431,2904728040,130529780,2121193516,298325532],"citationsCount":26,"abstract":"This work explores a group learning scenario with an autonomous empathic robot. We address two research questions: (1) Can an autonomous robot designed with empathic competencies foster collaborative learning in a group context? (2) Can an empathic robot sustain positive educational outcomes in long-term collaborative learning interactions with groups of students? To answer these questions, we developed an autonomous robot with empathic competencies that is able to interact with a group of students in a learning activity about sustainable development. Two studies were conducted. The first study compares learning outcomes in children across three conditions: learning with an empathic robot; learning with a robot without empathic capabilities; and learning without a robot. The results show that the autonomous robot with empathy fosters meaningful discussions about sustainability, which is a learning outcome in sustainability education. The second study features groups of students who interact with the robot in a school classroom for 2 months. The long-term educational interaction did not seem to provide significant learning gains, although there was a change in game-actions to achieve more sustainability during game-play. This result reflects the need to perform more long-term research in the field of educational robots for group learning."},{"id":2792480533,"microsoftAcademicId":2792480533,"numberInSourceReferences":121,"doi":"10.1145/3171221.3171247","title":"Inducing Bystander Interventions During Robot Abuse with Social Mechanisms","authors":[{"LN":"Tan","FN":"Xiang Zhi","affil":"Carnegie Mellon University"},{"LN":"Vazquez","FN":"Marynel","affil":"Stanford University"},{"LN":"Carter","FN":"Elizabeth J.","affil":"Carnegie Mellon University"},{"LN":"Morales","FN":"Cecilia G.","affil":"Carnegie Mellon University"},{"LN":"Steinfeld","FN":"Aaron","affil":"Carnegie Mellon University"}],"year":2018,"journal":"Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction","references":[2901136733,2162090451,2125188427,1514587894,1980083892,2118199364,2103530214,2031977514,1979628236,2407300725,1985855315,2003350835,2156763515,85091029,2046235077,2153457831,1990266238,13388985,1981509185,2595857079,2073979665,2001343909,2148999704,335867701,2135115712,1980907156,2143910019,2154676290,2552988712,2025583209],"citationsCount":26,"citationContext":{"13388985":["This is clearly apparent in a variety of popular culture characters, and extensive prior work on empathy and robots has documented this relationship [10, 12, 27, 33]."],"85091029":["Several of the questions were inspired by the “Interpersonal Reactivity Index” [6]."],"335867701":["These responses by robots have been used in a wide range of scenarios, such as education [32] and rehabilitation [34]."],"1514587894":["Unless otherwise noted, we used REstricted or REsidual Maximum Likelihood (REML) analyses [21, 31] to fit a linear model that evaluates the effects of our manipulation during the memorization task."],"1979628236":["Another investigation found that it can be difficult for a robot to verbally persuade children not to abuse it or impede its actions [4].","For example, it can include blocking the path of amobile robot [4, 17, 30], verbal taunting [4, 17], and physical violence, such as hitting and kicking [4, 16, 17, 30].","For example, it can include blocking the path of amobile robot [4, 17, 30], verbal taunting [4, 17], and physical violence, such as hitting and kicking [4, 16, 17, 30].","For example, it can include blocking the path of amobile robot [4, 17, 30], verbal taunting [4, 17], and physical violence, such as hitting and kicking [4, 16, 17, 30].","In this work, we follow the previous operationalization of robot abuse by Brscić and colleagues [4]: “persistent offensive action, either","These reports are in line with prior research in Human-Robot Interaction (HRI) where individuals – particularly children – abused robots [4, 17, 30].","This observation led to the development of physical strategies for robots to escape abuse by moving closer to nearby adults [4]."],"1980083892":["In a broad sense, empathy refers to the “reactions of one individual to the observed experiences of another”, and it can be measured through four aspects: Perspective Taking, Fantasy, Emotional Concern, and Personal Distress [7]."],"1980907156":["These responses by robots have been used in a wide range of scenarios, such as education [32] and rehabilitation [34]."],"1981509185":["This is clearly apparent in a variety of popular culture characters, and extensive prior work on empathy and robots has documented this relationship [10, 12, 27, 33]."],"1985855315":["Robots can display empathetic responses themselves through mimicry [8, 25] or by monitoring a user’s affective state to generate appropriate empathetic responses [11, 35]."],"1990266238":["Robots can display empathetic responses themselves through mimicry [8, 25] or by monitoring a user’s affective state to generate appropriate empathetic responses [11, 35]."],"2001343909":["For example, it can include blocking the path of amobile robot [4, 17, 30], verbal taunting [4, 17], and physical violence, such as hitting and kicking [4, 16, 17, 30].","For example, it can include blocking the path of amobile robot [4, 17, 30], verbal taunting [4, 17], and physical violence, such as hitting and kicking [4, 16, 17, 30].","In these contexts, robot abuse is often preceded by exhibitions of curiosity, such as blocking sensors or hitting bumpers to trigger responses, that then escalate to abuse [30].","Poor treatment of robots can be naturally observed in public human environments [17, 30].","These reports are in line with prior research in Human-Robot Interaction (HRI) where individuals – particularly children – abused robots [4, 17, 30]."],"2003350835":["This is clearly apparent in a variety of popular culture characters, and extensive prior work on empathy and robots has documented this relationship [10, 12, 27, 33]."],"2025583209":["The participants’ approach of indirectly commenting on the confederate’s behavior supports prior research on conflict intervention [28]."],"2031977514":[") Peers have been found to spontaneously intervene in 20 to 25% of bullying episodes [18]."],"2046235077":["The people who verbally or physically intervene across various contexts may choose to do so in either a prosocial or aggressive manner, but the specific intervention method used by the peers did not seem to change the success of the interventions [14].","This type of intervention has previously been found to stop over half of bullying situations among elementary school children [14]."],"2073979665":["This is clearly apparent in a variety of popular culture characters, and extensive prior work on empathy and robots has documented this relationship [10, 12, 27, 33]."],"2103530214":["For example, it can include blocking the path of amobile robot [4, 17, 30], verbal taunting [4, 17], and physical violence, such as hitting and kicking [4, 16, 17, 30]."],"2125188427":["The robot was equipped with a visual marker [20] for localization in the workspace area (Fig."],"2143910019":["Furthermore, prior research suggests that people are generally more inclined to cause pain to a robot than to another human being [1, 2] and are even willing to destroy small robots in some circumstances [1].","Furthermore, prior research suggests that people are generally more inclined to cause pain to a robot than to another human being [1, 2] and are even willing to destroy small robots in some circumstances [1]."],"2148999704":["People are also less likely to break a robot that seems intelligent [3]."],"2153457831":["Robots can display empathetic responses themselves through mimicry [8, 25] or by monitoring a user’s affective state to generate appropriate empathetic responses [11, 35]."],"2154676290":["Furthermore, prior research suggests that people are generally more inclined to cause pain to a robot than to another human being [1, 2] and are even willing to destroy small robots in some circumstances [1]."],"2156763515":["Group members can take the roles of (1) the bully; (2) the reinforcer of the bully, who incites the bully or provides a receptive audience; (3) the assistant to the bully, who follows the bully’s lead and joins in; (4) the victim; (5) the defender of the victim, who consoles the victim, takes his/her side, and tries to stop the aggressors; and (6) the outsider, who does nothing [29]."],"2162090451":["The experimenter administered the Ten Item Personality Measure (TIPI) [9], a standardized survey (SURVEY 1 in Fig.","We only found significant correlations with the Agreeableness and Extraversion dimension of the TIPI survey [9]."],"2407300725":["The number of participants per condition was determined by following the local standard used in similar studies [5]."],"2552988712":[", size) can impact users’ interpretations of verbal abuse [13].","The size and appearance of the robot could have influenced whether participants were willing to help [13]."],"2901136733":["2) and was controlled by a laptop running the Robot Operating System [24]."]},"abstract":"We explored whether a robot can leverage social influences to motivate nearby bystanders to intervene and defend them from human abuse. We designed a between-subjects study where 48 participants took part in a memorization task and observed a confederate mistreating a robot both verbally and physically. The robot was either empathetic towards the participant’s performance in the task or indifferent. When the robot was mistreated, it ignored the abuse, shut down in response to it, or reacted emotionally. We found that the majority of the participants intervened to help the robot after it was abused. Interventions happened for a wide range of reasons. Interestingly, the empathetic robot increased the proportion of participants that self-reported intervening in comparison to the indifferent robot, but more participants moved the robot as a response to abuse in the latter case. The participants also perceived the robot being verbally mistreated more and reported higher levels of personal distress when the robot briefly shut down after abuse in comparison to when it reacted emotionally or did not react at all."},{"id":2792400400,"microsoftAcademicId":2792400400,"numberInSourceReferences":167,"doi":"10.3390/APP8020302","title":"Designing the Mind of a Social Robot","authors":[{"LN":"Lazzeri","FN":"Nicole"},{"LN":"Mazzei","FN":"Daniele"},{"LN":"Cominelli","FN":"Lorenzo"},{"LN":"Cisternino","FN":"Antonio"},{"LN":"Rossi","FN":"Danilo De"}],"year":2018,"journal":"Applied Sciences","references":[2105103777,2097856935,2105558714,2006633893,1989104072,2090252028,2141973273,2149628368,2100119371,2026093575,2025637627,1966378118,2497635027,2160411185,1608468640,2039509869,2084263387,2064357402],"citationsCount":15,"abstract":"Humans have an innate tendency to anthropomorphize surrounding entities and have always been fascinated by the creation of machines endowed with human-inspired capabilities and traits. In the last few decades, this has become a reality with enormous advances in hardware performance, computer graphics, robotics technology, and artificial intelligence. New interdisciplinary research fields have brought forth cognitive robotics aimed at building a new generation of control systems and providing robots with social, empathetic and affective capabilities. This paper presents the design, implementation, and test of a human-inspired cognitive architecture for social robots. State-of-the-art design approaches and methods are thoroughly analyzed and discussed, cases where the developed system has been successfully used are reported. The tests demonstrated the system’s ability to endow a social humanoid robot with human social behaviors and with in-silico robotic emotions."},{"id":134062113,"microsoftAcademicId":134062113,"numberInSourceReferences":93,"doi":"10.1007/S12369-014-0262-Y","title":"A Recipe for Empathy","authors":[{"LN":"Lim","FN":"Angelica","affil":"Kyoto University"},{"LN":"Okuno","FN":"Hiroshi G.","affil":"Waseda University"}],"year":2015,"journal":"International Journal of Social Robotics","references":[1892502720,2016521818,2168175751,3022532439,2116146623,1594233936,1992745256,2153480757,2134084792,2110475150,2001362807,2468865363,2088313145,2101837595,2168222508,2135036523,2155352776,2133852590,2139749000,2016740912,2146233605,2043181832,567437002,1597563915,1499778067,2107231193,2105981921,1526930468,2141760481,1980515562,2127666085,2047920678,1976156697,1700507776,2011738260,2052177108,1508300280,2033995044,1993425853,2073979665,2065364824,2167660749,2126181565,110802701,2110102748,2146899501,2111715140,2167584524,2164617610,2121018017,2115502308,2104654551,2020299617,2157846663,2081000924,1591470100,2113244983,1885287084,2578725037,1997318488,1619839657,2162235983,2045107015,2056762707,2051153252,2624832017,2017262373,2036017692,1843631919,1971342862,2139455285,3147106146,2132892033,1995127236],"citationsCount":14,"citationContext":{"567437002":[", [5]).","Entrainment Entrainment is a term used to designate synchronizing with and adapting to the interaction partner ([5], p."],"1508300280":["provided participants with handheld devices to record their musical emotional experiences at random points throughout a 2-week period [38]."],"1591470100":["Emotional contagion is also used by recording artists and film score directors to induce emotion through music [39].","Emotions have been defined as synchronized reactions involving multiple components, including subjective feeling, expression, physiological arousal, action tendency, and regulation [39]."],"1594233936":["How exactly does the insula work? Although its exact mechanism is still not clear, Damasio and others have proposed that the human insula plays a role in mapping visceral states that are associated with emotional experience, giving rise to conscious feelings [19, 16,67].","” [19]","”[19]."],"1597563915":[", not actually feel pain themselves while torturing a victim) [7]."],"1619839657":["In terms of automatic systems, [46] [56] used a combination of facial expression recognition and rules to infer the human’s emotional state, and showed that the robot was perceived more as a friend."],"1700507776":["tive level which “assigns along two output dimensions, one of which we call “positive” and the other “negative”” [25]."],"1843631919":["also explore this idea of robot empathy based on a “ knowledge base of experiences” and “logical understanding” [4]."],"1892502720":["This has led to strong concerns, for instance by Turkle, that elderly care or child companion robots would be deceiving their owners by providing inauthentic affective relations, because robots do not have real “feelings” [70]."],"1971342862":["” [68]"],"1976156697":["Further, [55] has found evidence","The mirror neuron system has been suggested to be a major mechanism for empathy [36,55]."],"1980515562":["Neumann and Strack have found evidence of emotional contagion from speech [54].","” [54]"],"1992745256":[", empathy not only allows us to experience an emotion triggered by someone else’s emotion, but eventually also predict their behavior and understand their intentions [6]."],"1993425853":["For example, expressions of approval such as ‘Good!’ or ‘Clever girl!’ are typically spoken using exaggerated rise-fall pitch contours [and] expressions of prohibition or warning such as ‘No!’ or ‘Dont touch that!’ are spoken with low pitch and high intensity [27].","These correspond to the four categories of motherese as defined in [27]."],"2001362807":["First, while the amygdala’s role in autonomic fear responses related to survival are clear [20], the insula predominates in studies related to empathy for pain, for example [34]."],"2011738260":["a sensual touch into pleasure [51]."],"2016521818":["How exactly does the insula work? Although its exact mechanism is still not clear, Damasio and others have proposed that the human insula plays a role in mapping visceral states that are associated with emotional experience, giving rise to conscious feelings [19, 16,67].","Neuron, which are large and are hypothesized to help channel neural signals from deep within the cortex to relatively far parts of the brain [16].","pathizing with others, seeing disgust on a face, and listening to music [16,52]."],"2016740912":["The first study in 1992 showed that the neurons in the premotor cortex of a macaque monkey grasping food were also found to be active when observing a human grasping food [22]."],"2017262373":["This abstraction is chosen because dynamics such as SIRE have been shown to underlie emotion across multiple modalities [47] and even across cultures [66]."],"2020299617":["showed that robots that mirror emotional states elicit more cooperative behavior from users [31]."],"2033995044":[", who explore the idea of robot empathy built like that of children’s [44].","Cognitive empathy may be a complementary empathetic skill requiring higher levels of cognition, such as described in Kozima’s related work in empathetic developmental robots [44]."],"2036017692":["It is active when a mother hears a crying baby [40], or when a person looks at a happy face [57]."],"2043181832":["Mehrabian’s PAD emotion representation [50])."],"2045107015":["In developmental robotics, this “intuitive parenting” paradigm has been proposed for associating a viewed emotional face with a robot’s own emotional face [73,10]."],"2047920678":["According to some studies, this may even involve correlations of melody types [62].","Recent research has suggested this universal mechanism, also called infant-directed (ID) speech, is at the crossroads for developing emotions, cognition and language in humans [62]: “It has been shown that infants are attracted by and attend to motherese, which is characterized by more exaggerated intonation and higher pick than adult-to-adult speech."],"2051153252":["Lastly, at 9 months, they prefer directive vocalizations [41].","Then, at 6 months, they prefer and receive approval (praise) vocalizations [41]."],"2052177108":["In a dictator game experiment, altruistic sharing behavior was found to be related to affective empathy, whereas cognitive empathy was not [24]."],"2056762707":["In summary, mirror neurons provide a “sensory-motor gateway for forming an internal representation of an observed person’s state” [23]."],"2065364824":["In developmental psychology, [72] and","” [72]"],"2073979665":["Kwak and Kim [45] suggest that empathy – both in the human and in the robot – is important because it results in mutual emotional com-"],"2081000924":["pathizing with others, seeing disgust on a face, and listening to music [16,52]."],"2088313145":["action and when it hears the related sound [43]."],"2101837595":["Mirror neurons (also known as the mirror mechanism) are those in the motor areas of the human brain that fire both during action execution and also action observation [35].","review provides a broad summary of what is known about empathy and the brain [35]:"],"2104654551":["The mappings for SIRE voice and gesture, and SIRE GMM learning mechanism remained the same as defined in [48], shown in","We use the SIRE paradigm [48], in which vocal utterances and motor movements are abstracted into their dynamic characteristics: Speed, Intensity, irRegularity, and Extent (SIRE)."],"2105981921":["It thought to be where bad taste or smell are transformed into disgust [2,13], or"],"2107231193":["First, while the amygdala’s role in autonomic fear responses related to survival are clear [20], the insula predominates in studies related to empathy for pain, for example [34]."],"2110102748":["also showed that the iCat robot, when expressing accurate empathetic behaviors, was perceived as more dependable and trustworthy [17]."],"2110475150":["A functional MRI experiment found a mirror neuron network that was active both when observing a facial emotion and imitating the emotion [14].","A part of the brain called the insula has been suggested to be at the crux of the association between action representation and emotion [14].","” [14]"],"2111715140":["In terms of automatic systems, [46] [56] used a combination of facial expression recognition and rules to infer the human’s emotional state, and showed that the robot was perceived more as a friend."],"2115502308":["For instance, roboticists such as Nagai and Rohlfing created models to describe the use of motion as an early mechanism for teaching [53], and Ishihara has studied language development using this paradigm [37]."],"2116146623":["How exactly does the insula work? Although its exact mechanism is still not clear, Damasio and others have proposed that the human insula plays a role in mapping visceral states that are associated with emotional experience, giving rise to conscious feelings [19, 16,67].","It is active both when empathizing for others’ pain [67], and when directly feeling pain [8]."],"2121018017":["of visceral and interoceptive sensors (sensing heat, cold, pain, taste, muscle ache) sending information to the insula [12]."],"2126181565":["references its urgency or importance” [15]."],"2127666085":["This abstraction is chosen because dynamics such as SIRE have been shown to underlie emotion across multiple modalities [47] and even across cultures [66]."],"2133852590":["In a neurological music study, simply listening to music activated brain areas related to premotor representations for vocal sound production (though no singing was observed in participants) [42]."],"2134084792":["” [21] [65]."],"2135036523":["The mirror neuron system has been suggested to be a major mechanism for empathy [36,55].","simple visual observation of an action incited premotor activity in the brain [61,36]."],"2139749000":["It is active both when empathizing for others’ pain [67], and when directly feeling pain [8]."],"2141760481":["It is active when a mother hears a crying baby [40], or when a person looks at a happy face [57]."],"2146233605":["Insight can come from brain lesion studies such as [1] by Ralph Adolphs.","voice and movements being modified using SelfSIRE , a vector of four values on [0, 1], where"],"2146899501":["skin color) [60]."],"2153480757":["Preston and De Waal have contrasted emotional contagion with other kinds of empathy: in cognitive empathy and sympathy, the separation between the self and other is retained, but in emotional contagion, there is no self-other distinction [58]."],"2157846663":["cannot reliably produce motherese it in front of a microphone [29]."],"2164617610":["[32] have suggested a kind of associative learning between the affective voice and face during infancy."],"2168175751":["where Xc is a vector of SIRE tuples corresponding to the class C, and m is the optimal number of components to minimize the Bayesian Information Criterion (BIC) over Xc [64]."],"2168222508":["thy in the brain: affective empathy and cognitive empathy [65].","” [21] [65]."],"2578725037":["In developmental robotics, this “intuitive parenting” paradigm has been proposed for associating a viewed emotional face with a robot’s own emotional face [73,10]."],"2624832017":["Further, computational emotion models focusing on the fear response have been well established, for instance by Fellous and LeDoux [26]."],"3022532439":["simple visual observation of an action incited premotor activity in the brain [61,36]."]},"abstract":"Could a robot feel authentic empathy? What exactly is empathy, and why do most humans have it? We present a model which suggests that empathy is an emergent behavior with four main elements: a mirror neuron system, somatosensory cortices, an insula, and infant-directed “baby talk” or motherese. To test our hypothesis, we implemented a robot called MEI (multimodal emotional intelligence) with these functions, and allowed it to interact with human caregivers using comfort and approval motherese, the first kinds of vocalizations heard by infants at 3 and 6 months of age. The robot synchronized in real-time to the humans through voice and movement dynamics, while training statistical models associated with its low level gut feeling (“flourishing” or “distress”, based on battery or temperature). Experiments show that the post-interaction robot associates novel happy voices with physical flourishing 90 % of the time, sad voices with distress 84 % of the time. Our results also show that a robot trained with infant-directed “attention bids” can recognize adult fear voices. Importantly, this is the first emotion system to recognize adult emotional voices after training only with motherese, suggesting that this specific parental behavior may help build emotional intelligence."},{"id":2897433896,"microsoftAcademicId":2897433896,"numberInSourceReferences":84,"doi":"10.3389/FPSYG.2018.01852","title":"Emotional Empathy as a Mechanism of Synchronisation in Child-Robot Interaction.","authors":[{"LN":"Giannopulu","FN":"Irini","affil":"Bond University"},{"LN":"Terada","FN":"Kazunori","affil":"Gifu University"},{"LN":"Watanabe","FN":"Tomio","affil":"Okayama Prefectural University"}],"year":2018,"journal":"Frontiers in Psychology","references":[3022532439,2073825915,1424744123,751470531,2110475150,1968372229,2044954931,2088335634,1965401478,2148745188,1048662888,2134780975,2086568921,2136132767,2042831497,2002209067,2064422984,165886685,2110857432,2729454061,2088321173,2002807430,2089830108,2565407566,2074209190,2758432130,2083645227,2789331554,2787391982,2801970809,13022595,2253955724,2472542578,2272070881,1968711994,2030675244,24696481,2308459519,2613812712,2769943401,2885486127,2326106789],"citationsCount":9,"abstract":"Simulating emotional experience, emotional empathy is the fundamental ingredient of interpersonal communication. In the speaker-listener scenario, the speaker is always a child, the listener is a human or a toy robot. Two groups of neurotypical children aged 6 years on average composed the population: one Japanese (n = 20) and one French (n = 20). Revealing potential similarities in communicative exchanges in both groups when in contact with a human or a toy robot, the results might signify that emotional empathy requires the implication of an automatic identification. In this sense, emotional empathy might be considered a broad idiosyncrasy, a kind of synchronisation, offering the mind a peculiar form of communication. Our findings seem to be consistent with the assumption that children’s brains would be constructed to simulate the feelings of others in order to ensure interpersonal synchronisation."},{"id":20018439,"microsoftAcademicId":20018439,"numberInSourceReferences":96,"doi":"10.1007/S12369-014-0261-Z","title":"Can You Read My Face? A Methodological Variation for Assessing Facial Expressions of Robotic Heads","authors":[{"LN":"Mirnig","FN":"Nicole","affil":"University of Salzburg"},{"LN":"Strasser","FN":"Ewald","affil":"University of Salzburg"},{"LN":"Weiss","FN":"Astrid","affil":"University of Salzburg"},{"LN":"Kühnlenz","FN":"Barbara","affil":"Institute of Automatic Control Engineering (LSR), Munich, Germany"},{"LN":"Wollherr","FN":"Dirk","affil":"Institute of Automatic Control Engineering (LSR), Munich, Germany"},{"LN":"Tscheligi","FN":"Manfred","affil":"University of Salzburg"}],"year":2015,"journal":"International Journal of Social Robotics","references":[1841352775,2032568497,1595732857,2167557160,1966797434,2098676269,1424744123,751470531,1741471588,2031371003,2052579003,2150781787,2024649868,2043181832,276037850,1542318638,2157105625,2063306756,2111438996,2016433830,2089420674,1969784320,2137020039,2109543807,1965158745,2092287437,2164780916,2033640825,1567947883,2127200744,1545825584,2095033554,1984544628,2143507802,2145802841,2023033182,1926864494,2135115712,2082752255,2073395356,1968851485,2103131854,1969350109,2139706181,2136463228,2540108576,2097938758,2151288549,3114284325,2164186613,2142224342,2157000503,1231757063,44623247,2129942529,2622425642,1971068523,2159037371,1965028342,1568832394,2051959394,3115469091,1564132298],"citationsCount":7,"abstract":"Our paper reports about an online study on robot facial expressions. On the one hand, we performed this study to assess the quality of the current facial expressions of two robot heads. On the other hand, we aimed at developing a simple, easy-to-use methodological variation to evaluate facial expressions of robotic heads. Short movie clips of two different robot heads showing a happy, sad, surprised, and neutral facial expression were compiled into an online survey, to examine how people interpret these expressions. Additionally, we added a control condition with a human face showing the same four emotions. The results showed that the facial expressions could be recognized well for both heads. Even the blender emotion surprised was recognized, although it resulted in positive and negative connotations. These results underline the importance of the situational context to correctly interpret emotional facial expressions. Besides the expected finding that the human is perceived significantly more anthropomorphic and animate than both robot heads, the more human-like designed robot head was rated significantly higher with respect to anthropomorphism than the robot head using animal-like features. In terms of the validation procedure, we could provide evidence for a feasible two-step procedure. By assessing the participants’ dispositional empathy with a questionnaire it can be ensured that they are in general able to decode facial expressions into the corresponding emotion. In subsequence, robot facial expressions can be validated with a closed-question approach."},{"id":2069874839,"microsoftAcademicId":2069874839,"numberInSourceReferences":111,"doi":"10.1007/S12369-014-0268-5","title":"Testing Empathy with Robots: A Model in Four Dimensions and Sixteen Items","authors":[{"LN":"Tisseron","FN":"Serge","affil":"Paris 7 Diderot Université"},{"LN":"Tordo","FN":"Frédéric","affil":"Paris 7 Diderot Université"},{"LN":"Baddoura","FN":"Ritta","affil":"University of Lyon"}],"year":2015,"journal":"International Journal of Social Robotics","references":[2048701323,2587573685,1591132889,2012676995,2114216982,2018372581,103982469,2114550714,2080184306,2166686396,2488484850,2004382573,2016131089,2090349585,2046051388,1995364462,280266713,2157282962,3088520509,2025666315,2076379017,3030292250,2017936643,48277833,2579816272,2024999086,2174837205,2316126936,1966019348,1969461579,2152327233,1652974208,2511659778],"citationsCount":7,"citationContext":{"103982469":["Additionally, some recent studies [15,16,36] have shown that the robot’s ability to display an empathic behavior and induce empathy had an impact on how its human partners perceived it in terms of behavior, social presence, engagement and interaction ease.","Some recent studies [15,16] focus on supporting this emotional dimension of the interaction by endowing the robot with empathic capabilities thus enabling it to recognize the affective state and the personal preferences (in relation to a particular task) of its human partner and to adapt its response accordingly.","read ourminds based on emotion [16] ormotion [26] analysis and recognition."],"1591132889":["It is known that in war zones and in combat situations, some soldiers tend to become emotionally attached to their robotic aid and to show a kind of empathy towards it [27–29].","Some are even ready to risk their own lives or to endanger their comrades’ lives to help a robot [27–29]."],"1969461579":["This dimension of empathy is part of a theory of mind: an active operation, in which the subject tries to access the awareness of others, and where processes of cognitive perspective taking are used to imagine or mentally transpose oneself in the place of another [24], that is to say, in the place of his/her own cognitions."],"2017936643":["It would be a kind of emotional reproduction, in the sense that an emotion triggered in one would be similar or identical to that which we perceive in the other [22]."],"2018372581":["Indeed, some studies [11–13] have shown that humans tend to draw inferences about the robot’s"],"2024999086":["The second concerns the mediation of the human potential ofmotion in the avatar [6], so that one truly becomes a spectator of his ownactionswhendirecting it [7]."],"2048701323":["Some authors [10] proposed to designate these dimensions under the word “compassion”, taken no longer in the Christian tradition’s sense but in the meaning it has in the Buddhist tradition: human beings are considered to be in permanent suffering and in perpetual search for a happiness that is impossible to reach."],"2076379017":["A comparison between the levels of each of the four dimensions of empathy in specific experimental designs of human-robot interactions, especially in regards to the empathy of assistance, would probably contribute to clarify some results such as the ones presented in [15,30,31] as well as to distinguish between the specificities of the empathy experienced during human-human interaction and during human-robot interaction.","When it comes to interactions involving children, authors in [31] have observed that adult instructions about a robot are most likely to impact children’s perceptions and helping behaviors towards it: the children tend to help a robot after experiencing a positive introduction to it."],"2090349585":["A young woman who’s an adept of Second Life reported feeling the itchiness of the bubbles on her own skin when she plunged her avatar in a virtual Jacuzzi [4].","It is an empathetic relationship with oneself in which one has representations of his own states of subjectivity (actions, emotions and thoughts) [4].","This questionnaire is based on our first model of empathy [1–4], empathy being"],"2114216982":["Koppula and Saxena [26] have designed a domestic robot: PR2, able to analyze the human motion, and","read ourminds based on emotion [16] ormotion [26] analysis and recognition."],"2114550714":["Indeed, some studies [11–13] have shown that humans tend to draw inferences about the robot’s"],"2157282962":["For Ishiguro [9], an elderly person who has to be taken care of by a robot that is designed to help her and assist her, welcomes it as her own child.","In certain contexts, a person could also imagine the robot as being a bit like his/her own child [9]."],"2511659778":["Thismimo-gestural “echoïsation” [18] is often associated with imitation, imitation being considered here not only as a reproduction of the movements of the other, but also as an interpersonal function going beyond the acquisition of motor skills."],"3088520509":["Therefore, this object can become a possible technological version of the human image reflected by the mirror [8]."]},"abstract":"The four-dimensional model of empathy presented in this paper addresses human–human, human–avatar and human–robot interaction, and aims at better understanding the specificities of the empathy that humans might develop towards robots. Its first dimension is auto-empathy and refers to an empathetic relationship with oneself: how can a human directing a robot expand the various components of empathy he feels for himself to this robot? The second is direct empathy: what does a human attribute to a robot in terms of thoughts, emotions, action potentials or even altruism, on the model of what he imagines and attributes to himself? The third dimension is reciprocal empathy that consists of thinking that a robot is able to identify with me, feel or guess my emotions and thoughts, anticipate my actions and wear me assistance if necessary. Finally, the fourth dimension, intersubjective empathy, is about thinking and imagining that a robot can inform me of things - emotions, thoughts that I am likely to experience- that I do not know about myself. Each of these four dimensions includes four different components: (1) Action (empathy of action), (2) Emotion (emotional empathy), (3) Cognition (cognitive empathy) and (4) Assistance (empathy of assistance). This theoretical model of empathy in four dimensions and four components defines sixteen items whose relevance will be tested in the near future through comparative experimental research involving human-human and human-robot interaction."},{"id":2611049140,"microsoftAcademicId":2611049140,"numberInSourceReferences":59,"doi":"10.1145/3025453.3025496","title":"A New Chatbot for Customer Service on Social Media","authors":[{"LN":"Xu","FN":"Anbang","affil":"IBM"},{"LN":"Liu","FN":"Zhe","affil":"IBM"},{"LN":"Guo","FN":"Yufan","affil":"IBM"},{"LN":"Sinha","FN":"Vibha","affil":"IBM"},{"LN":"Akkiraju","FN":"Rama","affil":"IBM"}],"year":2017,"journal":"Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems","references":[2101105183,2949888546,10957333,2085081981,2126104150,2096099424,2079408806,3103319922,2059179344,2373570000,2152154199,131086928,2576865949,1979555337],"citationsCount":341,"abstract":"Users are rapidly turning to social media to request and receive customer service; however, a majority of these requests were not addressed timely or even not addressed at all. To overcome the problem, we create a new conversational system to automatically generate responses for users requests on social media. Our system is integrated with state-of-the-art deep learning techniques and is trained by nearly 1M Twitter conversations between users and agents from over 60 brands. The evaluation reveals that over 40% of the requests are emotional, and the system is about as good as human agents in showing empathy to help users cope with emotional situations. Results also show our system outperforms information retrieval system based on both human judgments and an automatic evaluation metric."},{"id":2948638627,"microsoftAcademicId":2948638627,"numberInSourceReferences":3,"doi":"10.1145/3311927.3326596","title":"EmotoTent: Reducing School Violence through Embodied Empathy Games","authors":[{"LN":"Antle","FN":"Alissa N.","affil":"Simon Fraser University"},{"LN":"Sadka","FN":"Ofir","affil":"Simon Fraser University"},{"LN":"Radu","FN":"Iulian","affil":"Harvard University"},{"LN":"Gong","FN":"Boxiao","affil":"Simon Fraser University"},{"LN":"Cheung","FN":"Victor","affil":"Simon Fraser University"},{"LN":"Baishya","FN":"Uddipana","affil":"Simon Fraser University"}],"year":2019,"journal":"Proceedings of the 18th ACM International Conference on Interaction Design and Children","references":[2170644967,1969977883,2147093380,2088337190,2170749155,2130994853,1911641589,2887222263,2029254010,2073747016,2027322776,1210709148,2121295338,2787184884,2603363504,1803090685,1970054502,2807947037,2808209466,2940740266,2804959432,2318758355,206509305],"citationsCount":5,"abstract":"EmotoTent is an interactive socio-emotional learning system developed in response to escalating levels of violence, inequality and marginalization in schools seen in the early 21st Century. The system is inspired by advances in biosensing wearables, tattoo displays, brain sensors, robotic agents, artificial intelligence (AI), gestural interaction and 3D holographic displays. By 2030, technological advances will enable us to prototype and investigate questions related to experiential and embodied emotional learning; emotion-based human-computer interaction, affective biosensing, empathetic AI agents, and 3D interactive holographic environments. We envision EmotoTent as a modular, emotion-sensing Holodeck. In the EmotoTent program children learn and practice emotion regulation and empathy with peers, pets and a robotic dog agent in ways that are experiential, embodied and playful. We propose EmotoTent as a core element of a K-6 socio-emotional learning curriculum designed to improve school culture through the enhancement of children's ability to regulate emotions and interact with human and non-human species with empathy and compassion. Enhancing these qualities has been shown to lead to reductions in violence and bullying, racism, gender inequality and other forms of marginalization. We predict that the EmotoTent socio-emotional learning program will improve school cultures and create a foundation for children's lifelong well-being."},{"id":2899506378,"microsoftAcademicId":2899506378,"numberInSourceReferences":171,"doi":"10.1016/J.NEUBIOREV.2018.11.001","title":"Empathy is not in our genes","authors":[{"LN":"Heyes","FN":"Cecilia","affil":"University of Oxford"}],"year":2018,"journal":"Neuroscience & Biobehavioral Reviews","references":[2133735566,2073825915,2153480757,2025642716,1975000068,2019924339,2109004781,1995296289,2150422125,2502209121,2103551585,1980083892,2071441583,2732246028,1973030362,2088335634,2132960888,1965401478,2252899440,1968351672,2056874158,2096117537,2000120390,2132578438,2112387466,2886956600,1499778067,2136530632,2107231193,2136609494,2098277620,2163150496,2559202004,2345671421,2084235460,2407208610,2161917637,2108868694,2763043462,1981060433,2119522489,2754425142,1930222993,2023709597,2138949866,2136132767,2041809125,1672372155,1998849596,1883631429,2530645238,2164482323,2616358185,2096625159,2102617823,2770116496,2050986015,2108914830,1975393967,2147347966,2060546137,2546800412,2019359292,34357710,2096844853,2069060923,2118534560,2081818022,1993467165,2149334002,2078444365,2763285577,2167824792,2139619358,2030326428,1605802884,2081268415,2068816364,2605729489,2155298835,2589660251,1600181585,2237992116,2395771257,2150831631,2108359619,2131334035,2041064272,2482189804,2578725037,1968651882,2166208575,2299591553,2097693453,2138453783,2268921580,2200716026,630774420,2767094423,2565197503,134062113,2597843411,2992375459,2740115462,1538799533,2344077824,2187696357,2069158890,1915677057,1806900685,2579961912,2749504971,2556347519,2478549067,1977965034,2071614261,2082021519,1978822289,1932990304,2062856029,2021410705,2564101860],"citationsCount":61,"abstract":"In academic and public life empathy is seen as a fundamental force of morality - a psychological phenomenon, rooted in biology, with profound effects in law, policy, and international relations. But the roots of empathy are not as firm as we like to think. The matching mechanism that distinguishes empathy from compassion, envy, schadenfreude, and sadism is a product of learning. Here I present a dual system model that distinguishes Empathy1, an automatic process that catches the feelings of others, from Empathy2, controlled processes that interpret those feelings. Research with animals, infants, adults and robots suggests that the mechanism of Empathy1, emotional contagion, is constructed in the course of development through social interaction. Learned Matching implies that empathy is both agile and fragile. It can be enhanced and redirected by novel experience, and broken by social change."},{"id":2790233857,"microsoftAcademicId":2790233857,"numberInSourceReferences":11,"doi":"10.1145/3171221.3171252","title":"Group-based Emotions in Teams of Humans and Robots","authors":[{"LN":"Correia","FN":"Filipa","affil":"Instituto Superior Técnico"},{"LN":"Mascarenhas","FN":"Samuel","affil":"Instituto Superior Técnico"},{"LN":"Prada","FN":"Rui","affil":"Instituto Superior Técnico"},{"LN":"Melo","FN":"Francisco S.","affil":"Instituto Superior Técnico"},{"LN":"Paiva","FN":"Ana","affil":"Instituto Superior Técnico"}],"year":2018,"journal":"Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction","references":[2099019320,2032568497,2135907989,1977137834,1777878807,1978493498,1989104072,2214212631,2157069646,2031531183,2109474021,2245843004,1488883230,2032497511,2001564357,263876593,2010943014,2325594263,3149129393,2225036064,2019312772,2035928776,1974768933,2085083175,2326922198,1981509185,2146565132,37077319,2475950333,1986100731,2024647503,2155504372,2562375729,2082752255,2068398621,2002807430,2083614802,1788692495,2594633004,2073414733,2250275257,2292166976,1975254636,2017742123,2552668803,2760664994,2099668852,2552733004,2992659124],"citationsCount":29,"citationContext":{"37077319":["Group-based emotions are believed to be a result of self-categorisation and appraisal theories of emotions [44].","Naturally, these emotional reactions occur during intergroup interaction and, according to Smith [44], the salience of social identity is elicited by 3 factors: (1) the presence of out-group members, (2) the perception of similarities with the in-group members, and (3) the competition between groups."],"263876593":["Each of these artificial players is composed by an emotional agent [12] and an AI [10] that are responsible for the emotional behavioural responses and the game computations, respectively.","The appraisal and emotional generation steps of the proposed model were implemented with FAtiMA [12], an existing emotional architecture that is based on the OCC theory [39]."],"1488883230":["The existing progress has enabled the successful use of robots not just in manufacturing [8] but also to provide social support [37], therapeutic aid [7], educational tutoring [45] and new forms of entertainment [6]."],"1777878807":["have proposed a process model of group-based emotions [17] that extends the “modal model” of Gross & Thompson [19]."],"1788692495":["Based on the previously discussed findings from intergroup interactions in human-human [1, 28] as well as in human-robot [11, 21, 30, 48] scenarios, we expect to check the following hypothesis: Hypothesis 1: Participants will have a stronger Group Identification with a robotic partner that expresses group-based emotions.","Our expectation was in-line with previous findings [21, 30], where the perceived group identity of a robot had a significant impact on how the robot is perceived.","found that using a minimal group paradigm is enough to obtain significant differences [30]."],"1975254636":["Another motivation for choosing a card game as our task comes from the fact that several studies [21, 35, 38, 43] have demonstrated that card games are a successful activity for creating engagement in a human-robot interaction that is designed to be primarily social.","Based on the previously discussed findings from intergroup interactions in human-human [1, 28] as well as in human-robot [11, 21, 30, 48] scenarios, we expect to check the following hypothesis: Hypothesis 1: Participants will have a stronger Group Identification with a robotic partner that expresses group-based emotions.","Our expectation was in-line with previous findings [21, 30], where the perceived group identity of a robot had a significant impact on how the robot is perceived.","where participants played a collaborative card game with two other NAO robots [21]."],"1977137834":["For instance, according to the OCC theory [39], when someone judges an event to be desirable for him or her, that person is likely to experience joy afterwards in proportion to the level of desirability attributed.","The appraisal and emotional generation steps of the proposed model were implemented with FAtiMA [12], an existing emotional architecture that is based on the OCC theory [39]."],"1978493498":["Based on the Self-Categorisation Theory [22, 47], when the robot detects a presence of an out-group then its own group identity will become more salient."],"1981509185":["Moreover, having an internal model of emotions can help the robot guide its behaviour in a more humane manner by simulating the ability to feel empathy towards others [33]."],"1989104072":["Given that they play such a crucial role in human communication, there has been substantial work in developing robots that are able to both interpret human emotion and are able to express emotional cues as well [5, 29].","One aspect of human communication that creators of social robots are well aware of its importance is the ability to recognise and express emotions [5]."],"2001564357":["When performed successfully, these capabilities have a positive impact on creating interactions that are more enjoyable [2] and more educational [42]."],"2010943014":["[4] to differentiate between self- vs."],"2017742123":["Similarly, the blame attribution by a robot after a reliability drop lowers the user trust [26]."],"2019312772":["Our final argument is that social emotions such as shame or pride, which go beyond more basic emotions [13], should be given more attention in the creation of social robots, particularly in humanrobot teams.","This is particularly important when trying to convey social emotions such as admiration or pride that can be hard to distinguish from more basic emotions as the ones proposed by Ekman [13], using only non-verbal modalities."],"2024647503":["Another motivation for choosing a card game as our task comes from the fact that several studies [21, 35, 38, 43] have demonstrated that card games are a successful activity for creating engagement in a human-robot interaction that is designed to be primarily social."],"2031531183":["The existing progress has enabled the successful use of robots not just in manufacturing [8] but also to provide social support [37], therapeutic aid [7], educational tutoring [45] and new forms of entertainment [6]."],"2032497511":["Based on the Self-Categorisation Theory [22, 47], when the robot detects a presence of an out-group then its own group identity will become more salient."],"2032568497":["• Godspeed Questionnaire [3], using the dimensions of Anthropomorphism, Animacy, Likeability, and Perceived Intelligence regarding their robotic partner; • Group Trust [1] to assess the perceived trust by the participants regarding their team in the game; • Demographic questions, i."],"2035928776":["As pointed out by Groom and Nass [18], for a robot to be a teammate it must have some sort of individual autonomy and not simply obey commands."],"2073414733":["Based on the previously discussed findings from intergroup interactions in human-human [1, 28] as well as in human-robot [11, 21, 30, 48] scenarios, we expect to check the following hypothesis: Hypothesis 1: Participants will have a stronger Group Identification with a robotic partner that expresses group-based emotions.","Group identification was known as being an antecedent of group-based emotions until Kessler and Hollbach [28] have presented how group-based emotions can influence group identification and, therefore, how they can have bidirectional causality.","Kessler and Hollbach showed that in-group identification can indeed be influenced by the experience of group-based emotion [28].","This result seem to be coherent with previous findings from the social psychology, where group-based emotions are an antecedent of Group Identification [28]."],"2082752255":["When performed successfully, these capabilities have a positive impact on creating interactions that are more enjoyable [2] and more educational [42]."],"2083614802":["blaming itself, the user, or the environment in a driving scenario affects differently the attentiveness and the responsibility felt [23]."],"2099019320":["The importance of emotions, in general, for social robots is widely acknowledged in the community [14]."],"2099668852":["Another motivation for choosing a card game as our task comes from the fact that several studies [21, 35, 38, 43] have demonstrated that card games are a successful activity for creating engagement in a human-robot interaction that is designed to be primarily social."],"2109474021":["In emotional psychology, these judgements are referred to as appraisal variables with different theories of emotion proposing different sets of variables [36]."],"2135907989":["We believe this result is relevant for the emerging field of Human-Robot Teams, as trust constitutes one the most important constructs to support effective interaction and cooperation [20]."],"2157069646":["• Group Identification [31] with the Portuguese adaptation [40] to assess the in-Group Identification with their robotic partner; Figure 3: Experimental setting for the user study."],"2214212631":["The existing progress has enabled the successful use of robots not just in manufacturing [8] but also to provide social support [37], therapeutic aid [7], educational tutoring [45] and new forms of entertainment [6]."],"2225036064":["However, there is currently a growing interest in creating robots that are capable of collaborating with other humans in a team setting [15, 16, 24, 25, 32, 46].","have looked at how the emotional behaviour of a robot can have a positive impact on improving teamwork by employing emotion regulation strategies to diffuse conflict situations [25]."],"2250275257":["However, there is currently a growing interest in creating robots that are capable of collaborating with other humans in a team setting [15, 16, 24, 25, 32, 46]."],"2292166976":["Given that they play such a crucial role in human communication, there has been substantial work in developing robots that are able to both interpret human emotion and are able to express emotional cues as well [5, 29]."],"2325594263":["Based on the previously discussed findings from intergroup interactions in human-human [1, 28] as well as in human-robot [11, 21, 30, 48] scenarios, we expect to check the following hypothesis: Hypothesis 1: Participants will have a stronger Group Identification with a robotic partner that expresses group-based emotions.","Previous studies have shown that the amount of trust placed upon a robot is dependent on multiple factors such as how reliable it is at performing the task it is designed for [11], and how well humans understand its decision-making process [48]."],"2326922198":["[17].","have proposed a process model of group-based emotions [17] that extends the “modal model” of Gross & Thompson [19]."],"2552668803":["However, there is currently a growing interest in creating robots that are capable of collaborating with other humans in a team setting [15, 16, 24, 25, 32, 46]."],"2594633004":["However, there is currently a growing interest in creating robots that are capable of collaborating with other humans in a team setting [15, 16, 24, 25, 32, 46]."],"2760664994":["Each of these artificial players is composed by an emotional agent [12] and an AI [10] that are responsible for the emotional behavioural responses and the game computations, respectively."],"2992659124":["• Group Identification [31] with the Portuguese adaptation [40] to assess the in-Group Identification with their robotic partner; Figure 3: Experimental setting for the user study."],"3149129393":["Based on the previously discussed findings from intergroup interactions in human-human [1, 28] as well as in human-robot [11, 21, 30, 48] scenarios, we expect to check the following hypothesis: Hypothesis 1: Participants will have a stronger Group Identification with a robotic partner that expresses group-based emotions.","Previous studies have shown that the amount of trust placed upon a robot is dependent on multiple factors such as how reliable it is at performing the task it is designed for [11], and how well humans understand its decision-making process [48]."]},"abstract":"Providing social robots an internal model of emotions can help them guide their behaviour in a more humane manner by simulating the ability to feel empathy towards others. Furthermore, the growing interest in creating robots that are capable of collaborating with other humans in team settings provides an opportunity to explore another side of human emotion, namely, group-based emotions. This paper contributes with the first model on group-based emotions in social robotic partners. We defined a model of group-based emotions for social robots that allowed us to create two distinct robotic characters that express either individual or group-based emotions. This paper also contributes with a user study where two autonomous robots embedded the previous characters, and formed two human-robot teams to play a competitive game. Our results showed that participants perceived the robot that expresses group-based emotions as more likeable and attributed higher levels of group identification and group trust towards their teams, when compared to the robotic partner that expresses individual-based emotions."},{"id":3008789378,"microsoftAcademicId":3008789378,"numberInSourceReferences":71,"doi":"10.3389/FROBT.2020.00005","title":"Exploring Teens as Robot Operators, Users and Witnesses in the Wild.","authors":[{"LN":"Björling","FN":"Elin A.","affil":"University of Washington"},{"LN":"Thomas","FN":"Kyle","affil":"University of Washington"},{"LN":"Rose","FN":"Emma J.","affil":"University of Washington"},{"LN":"Cakmak","FN":"Maya","affil":"University of Washington"}],"year":2020,"journal":"Frontiers in Robotics and AI","references":[2130901187,1963629744,2107911338,2137099741,2163455560,2102178526,2045899956,2537335702,2063594338,1998522059,2009078006,2467868540,2200611418,2060981264,2620350838,2164813114,1987900808,2975920545,2130543722,614149592,2744301811,2977128309,2945839590,1900701192,2057132183,2912529695,2037998595,2077007094,1969152782,766417087,1642604170,2948267740,2079231398,2790321523,2415124602,2157570155,2304422604,2175295990,2806212758,2970555352,2897433896,2085992441,91854722,2323027343,2019404244,2973355918,87874506,2946055902,2345861699,1964322579],"citationsCount":4,"abstract":"As social robots continue to show promise as assistive technologies, the exploration of appropriate and impactful robot behaviors is key to their eventual success. Teens are a unique population given their vulnerability to stress leading to both mental and physical illness. Much of teen stress stems from school, making the school environment an ideal location for a stress reducing technology. Our mixed-methods study \\hl{was an attempt to understand teens' operation of, and responsiveness to, a robot only capable of movement compared to a robot only capable of speech.} Stemming from a human-centered approach, we introduce a Participatory Wizard of Oz (PWoz) interaction method that engaged teens as \\hl{operators, users, and witnesses} in a uniquely transparent interaction. In this paper we illustrate the use of the PWoz interaction method as well as how it helps \\hl{identify engaging robot interactions}. Using this technique, we present results from a study with 62 teens that includes details of the complexity of teen stress and a significant reduction in negative attitudes toward robots after interactions. \\hl{We also analyzed the teens interactions with both the verbal and non-verbal robots} and identified strong themes of (1) authenticity, (2) empathy, (3) emotional engagement, and (4) imperfection creates connection. Finally, we reflect on the benefits and limitations of the PWoz method and our study to identify next steps toward the design and development of our social robot."},{"id":2559431840,"microsoftAcademicId":2559431840,"numberInSourceReferences":72,"doi":"10.1007/S00146-016-0684-1","title":"Nudging for good: robots and the ethical appropriateness of nurturing empathy and charitable behavior","authors":[{"LN":"Borenstein","FN":"Jason","affil":"Georgia Institute of Technology"},{"LN":"Arkin","FN":"Ronald C.","affil":"Georgia Institute of Technology"}],"year":2017,"journal":"Ai & Society","references":[1481908410,1892502720,1495038747,3094202663,1841352775,2107911338,2291822968,2097415821,2725331431,2020091143,1638773224,2132164645,1527575096,1491607089,2038256537,2012372995,1603408457,2495049331,2030219240,2135903906,2053486845,1983826652,2251637402,2212646996,1997966546,2000412033,2340448678,185356498,2092777642,2408045524,2002775149,2046873575,2396108864,2139858103,2756083326,2151419788,2494758805,2548420101,1573403122,2512296411,2056104480,2336672716],"citationsCount":15,"abstract":"An under-examined aspect of human---robot interaction that warrants further exploration is whether robots should be permitted to influence a user's behavior for that person's own good. Yet an even more controversial practice could be on the horizon, which is allowing a robot to \"nudge\" a user's behavior for the good of society. In this article, we examine the feasibility of creating companion robots that would seek to nurture a user's empathy toward other human beings. As more and more computing devices subtly and overtly influence human behavior, it is important to draw attention to whether it would be ethically appropriate for roboticists to pursue this type of design pathway. Our primary focus is on whether a companion robot could encourage humans to perform charitable acts; this design possibility illustrates the range of socially just actions that a robot could potentially elicit from a user and what the associated ethical concerns may be."},{"id":2897782875,"microsoftAcademicId":2897782875,"numberInSourceReferences":9,"doi":"10.1109/IJCNN.2018.8489158","title":"Learning Empathy-Driven Emotion Expressions using Affective Modulations","authors":[{"LN":"Churamani","FN":"Nikhil","affil":"University of Hamburg"},{"LN":"Barros","FN":"Pablo","affil":"University of Hamburg"},{"LN":"Strahl","FN":"Erik","affil":"University of Hamburg"},{"LN":"Wermter","FN":"Stefan","affil":"University of Hamburg"}],"year":2018,"journal":"2018 International Joint Conference on Neural Networks (IJCNN)","references":[1757796397,2121863487,2963864421,1444168786,2339343773,1581387623,2156503193,1588539311,2171939880,2189149359,2111040806,2143350951,2164186291,2140527237,2169166781,2046372172,1841828540,2531262049,2080593835,2016433830,1978262206,1552007786,1789721553,2771461682,1637669343,2559960928,2403257023,2061131717,1559788381,2734507482,2736127050,1997318488,2033684537,2602938566],"citationsCount":12,"citationContext":{"1444168786":["51) attempts to give an objective appraisal [37] of the robot along with the subjective participant ratings."],"1552007786":["Furthermore, some studies [8], [9] highlight that emotion recognition benefits from combining multiple modalities."],"1559788381":["The Perception GWR network is trained in an unsupervised manner using the dense layer activation (300-d vectors) from the MCCNN network for the SAVEE [33] and","The audio channel is pre-trained on the SAVEE [33]","from the SAVEE [33] and RAVDESS [34] datasets."],"1581387623":["As emotions are central to any human interaction and are used to communicate intent and motivation [3], social robots should also possess such capabilities [4] in order to understand their users and serve their needs.","As emotions form an important component in human interaction to convey intent and meaning [3], a robot which is able to estimate an emotional state for itself and convey it using expression capabilities [20], [21] will serve as a more natural social companion for humans.","effect of various psychological concepts [3], [13] and cognitive appraisals [14], [15] is evaluated to equip robots with systems that understand and exhibit emotions."],"1588539311":["Facial expressions and speech have been examined individually by researchers [5]–[7] to recognise emotions."],"1637669343":["Cooperative robots should model the long-term behaviour [16], [17] of the users, recollecting past interactions with them."],"1789721553":["effect of various psychological concepts [3], [13] and cognitive appraisals [14], [15] is evaluated to equip robots with systems that understand and exhibit emotions.","slow-evolving emotional concepts [13] such as moods and attitudes [2] in robots shall allow them to adapt to the user’s"],"1841828540":["they operate in, allowing them to become more aware of their surroundings [1] and adapt their behaviour appropriately."],"1978262206":["using the FABO [32] dataset."],"1997318488":["As emotions form an important component in human interaction to convey intent and meaning [3], a robot which is able to estimate an emotional state for itself and convey it using expression capabilities [20], [21] will serve as a more natural social companion for humans."],"2016433830":["Such an evaluation of affect allows for a holistic view on HRI [2] to be taken into consideration while designing emotionally responsive robots.","While modelling interactions with humans, emotions and affect analysis play a huge role in interaction design [2].","slow-evolving emotional concepts [13] such as moods and attitudes [2] in robots shall allow them to adapt to the user’s"],"2033684537":["The second convolutional layer uses shunting inhibition [31] resulting in filters that are robust to geometric distortions [11]."],"2046372172":["This is achieved using a Growing-When-Required (GWR) [36] network which incrementally builds the knowledge representation as it gets different inputs by adding or removing neurons based on the activations of the existing neurons."],"2061131717":["effect of various psychological concepts [3], [13] and cognitive appraisals [14], [15] is evaluated to equip robots with systems that understand and exhibit emotions."],"2111040806":["should possess in order to carry out engaging and meaningful interactions with humans [28]."],"2121863487":["Reinforcement Learning (RL) [22] approaches have proven to be successful in learning robot behaviour in different scenarios."],"2140527237":["Cooperative robots should model the long-term behaviour [16], [17] of the users, recollecting past interactions with them."],"2143350951":["and non-verbal cues to express intent and meaning in a conversation [30]."],"2164186291":["Many recent approaches [10]–[12] thus"],"2169166781":["Facial expressions and speech have been examined individually by researchers [5]–[7] to recognise emotions."],"2171939880":["As emotions are central to any human interaction and are used to communicate intent and motivation [3], social robots should also possess such capabilities [4] in order to understand their users and serve their needs."],"2189149359":["The hyper-parameters for the entire network were optimised using the Hyperopt [35] library achieving an accuracy score of 0."],"2403257023":["Many recent approaches [10]–[12] thus"],"2531262049":["The second convolutional layer uses shunting inhibition [31] resulting in filters that are robust to geometric distortions [11].","al [11], to examine facial and auditory features for emotion classification (Fig."],"2559960928":["Interactive reinforcement learning approaches [23] further"],"2602938566":["Although the classification labels from the MCCNN enable the robot to evaluate the spontaneous emotion expressed by the user, to allow for a developmental emotion perception mechanism [21], a robust approach is needed for emotion representation, accounting for the variance with which","As emotions form an important component in human interaction to convey intent and meaning [3], a robot which is able to estimate an emotional state for itself and convey it using expression capabilities [20], [21] will serve as a more natural social companion for humans.","It also models an affective memory [21] of the robot’s interaction with the user, which is used to modulate the formation and evolution of the intrinsic mood of the robot, taking into account the complete interaction with the user.","clips from the KT emotional dataset [21] with the camera focussing on one participant, who is talking to another.","to train the robot’s affective memory, which represents its recollection of the interaction with a particular user [18], [21].","which consider only instantaneous stimuli, such long-term emotional models are able to account for temporal changes in stimuli as well as the effect of such changes on the robot’s understanding of the environment [21]."],"2734507482":["Emotion-specific rules were designed based on the results of previous user studies [26], [27] where users annotated different emotion expressions on NICO.","For social robots, it is important that they are not only able to perceive and appraise affective interactions but also to express emotions [27].","For this study, a subset of five emotions, namely Anger, Happiness, Sadness, Surprise and Neutral, were chosen as NICO was shown to express these five emotions unambiguously to the users [26], [27].","It builds on previous works in this direction [27] by not only estimating dynamic models of affect but also learning dynamic and continuous representations of","This was a significant improvement from the previous studies [27] as facial representations for different emotions were not fixed beforehand.","To achieve this, previous works [27] explored interactive learning","previous user studies [27]."],"2736127050":["This affective model of the robot’s interaction with the user or the ‘Affective Memory’ [18] can be used to empathise with the user, allowing","to represent the input, accounting for the variance in the stimuli [18].","to train the robot’s affective memory, which represents its recollection of the interaction with a particular user [18], [21]."],"2771461682":["Emotion-specific rules were designed based on the results of previous user studies [26], [27] where users annotated different emotion expressions on NICO.","For this study, a subset of five emotions, namely Anger, Happiness, Sadness, Surprise and Neutral, were chosen as NICO was shown to express these five emotions unambiguously to the users [26], [27].","This study explores the facial expression capability of the Neuro-Inspired COmpanion (NICO) robot [26] to express different emotions.","on NICO [26] are generated using an LED projection system inside the NICO head."],"2963864421":["Furthermore, in recent years, actor-critic based approaches [24], [25] have proven successful in learning actions for continuous control, allowing for extremely complex tasks to be solved by robots.","Since learning with DQNs in such high-dimensional action-spaces is intractable, the proposed model implements a Deep Deterministic Policy Gradient (DDPG) [25] based actor-critic architecture to learn the optimum policy."]},"abstract":"Human-Robot Interaction (HRI) studies, particularly the ones designed around social robots, use emotions as important building blocks for interaction design. In order to provide a natural interaction experience, these social robots need to recognise the emotions expressed by the users across various modalities of communication and use them to estimate an internal affective model of the interaction. These internal emotions act as motivation for learning to respond to the user in different situations, using the physical capabilities of the robot. This paper proposes a deep hybrid neural model for multi-modal affect recognition, analysis and behaviour modelling in social robots. The model uses growing self-organising network models to encode intrinsic affective states for the robot. These intrinsic states are used to train a reinforcement learning model to learn facial expression representations on the Neuro-Inspired Companion (NICO) robot, enabling the robot to express empathy towards the users."},{"id":2794057029,"microsoftAcademicId":2794057029,"numberInSourceReferences":57,"doi":"10.1145/3183654.3183703","title":"He who hesitates is lost (...in thoughts over a robot)","authors":[{"LN":"Wen","FN":"James","affil":"United States Air Force Academy"},{"LN":"Stewart","FN":"Amanda","affil":"United States Air Force Academy"},{"LN":"Billinghurst","FN":"Mark","affil":"University of South Australia"},{"LN":"Dey","FN":"Arindam","affil":"University of South Australia"},{"LN":"Tossell","FN":"Chad","affil":"United States Air Force Academy"},{"LN":"Finomore","FN":"Victor","affil":"United States Air Force Academy"}],"year":2018,"journal":"Proceedings of the Technology, Mind, and Society","references":[2111040806,2095953405,2245843004,1976660112,2325035506,1566846815,2165542916,2152536828,3121604639,2048596731],"citationsCount":8,"abstract":"In a team, the strong bonds that can form between teammates are often seen as critical for reaching peak performance. This perspective may need to be reconsidered, however, if some team members are autonomous robots since establishing bonds with fundamentally inanimate and expendable objects may prove counterproductive. Previous work has measured empathic responses towards robots as singular events at the conclusion of experimental sessions. As relationships extend over long periods of time, sustained empathic behavior towards robots would be of interest. In order to measure user actions that may vary over time and are affected by empathy towards a robot teammate, we created the TEAMMATE simulation system. Our findings suggest that inducing empathy through a back story narrative can significantly change participant decisions in actions that may have consequences for a robot companion over time. The results of our study can have strong implications for the overall performance of human machine teams."},{"id":2396108864,"microsoftAcademicId":2396108864,"numberInSourceReferences":42,"doi":"10.1007/978-3-319-25554-5_53","title":"Towards a Robot Computational Model to Preserve Dignity in Stigmatizing Patient-Caregiver Relationships","authors":[{"LN":"Pettinati","FN":"Michael J.","affil":"Georgia Institute of Technology"},{"LN":"Arkin","FN":"Ronald C.","affil":"Georgia Institute of Technology"}],"year":2015,"journal":"International Conference on Social Robotics","references":[2901494586,1980083892,2080559986,3124189541,2149186291,2012372995,2000916150,2017515172,2135903906,571943015,2105369152,2086182437,1997966546,1590281067,1987181521,2149197338,2087077911,2047371519,1969567094,2144997512,2167185059,2010317320,1987007371,2512296411,1969527142,2103915281],"citationsCount":5,"citationContext":{"1590281067":["Dignity is consistently linked with self-respect and identity [14].","Humiliation causes a fundamental change in a person’s understanding of his “identity” and place or worth in society [14]."],"1969527142":["causes a worsening of their symptoms [15]."],"1969567094":["person experiences compassion when recognizing suffering or tenderness when recognizing vulnerability [12]."],"1980083892":["An empathetic response is motivated by both emotion and cognition [5, 8].","Davis’s [5] four Interpersonal Reactivity Index (IRI) subscales define trait empathy as a multidimensional construct.","These independent components take different names: internal/external shame [10], defensive/unworthiness shame [5], and negative-selfevaluation/withdrawal shame [3].","a cognitive dimension that allows for the person to “anticipate” the behavior of another person as well as what the needs or wants of the person might be in a certain situation; it allows for appropriate social responses to the individual [5]."],"1987007371":["Smits and De Boeck [19] first introduced a componential model for guilt."],"1987181521":["[4]) have found that these caregivers will often have significantly greater capacities for empathetic concern and perspective-taking, two dimensions on Davis’s IRI."],"1997966546":["If the caregiver does not respond empathically, this further confirms the patient’s feeling of rejection, which results in increased feelings of shame, or in the development of resentment or anger toward the caregiver [26].","It is important for a caregiver to have a sufficient empathetic response to expressions of internal shame; otherwise, the patient could view a lack of response as abandonment, which will induce additional external shame in the patient [26].","Patients may feel that, when the doctor has “abandoned” them or is not actively supporting them, it is because they are dying or a lost cause [26].","Ridicule is likely to inspire a measure of anger in the patient [26].","The caregiver commits to being a present, social ally to the patient; the patient is made to recognize his value [26].","The caregiver is deviating from questioning pertinent to the disease (and taking the time) to let the patient tell his “story” [26].","The caregiver must respond to patient shame and stigma by making a connection with the patient; that is, showing patients that they are not alone and have value [21, 26].","The caregiver must show nonverbal support for the damaged self [26], and this support can progress to bolstering the patient’s identity through praise for the self (drawing a distinction between the disease and the self).","any shame component is nonzero) and there is no empathetic response (the empathetic response components are zero) [26].","because it may come off as hackneyed [26]."],"2000916150":["The process of stigmatization culminates in the internalizing of a negative stereotype that is associated with some disease or disorder [16].","[16]."],"2010317320":["[18], shame arises as a response in someone when a fundamental flaw"],"2012372995":["Shame does not require discrimination from an external source [9].","There has been a paucity of studies on how a robotic agent can guide interactions between a human dyad (an exception being [9])."],"2017515172":["External shame, corresponding to withdrawal and unworthiness shame, is experienced when it is those external to the flawed self that present the self as flawed [10]; causing the shamed person to hide/withdraw from the situation [3, 6].","In the psychology literature, shame is often decomposed into two independent components [3, 6, 10].","These independent components take different names: internal/external shame [10], defensive/unworthiness shame [5], and negative-selfevaluation/withdrawal shame [3]."],"2080559986":["Explicitly, “empathetic opportunities” [21] are single exchanges between the patient and caregiver where the patient experiences shame and the caregiver has the opportunity to respond empathetically.","It may not be possible for the caregiver to respond to the patient’s shame with complete understanding and compassion in each “empathetic opportunity” [21].","The caregiver must respond to patient shame and stigma by making a connection with the patient; that is, showing patients that they are not alone and have value [21, 26].","This framework’s core piece, the Emotional Models component, is responsible for computing the magnitude of the patient’s experienced shame and the caregiver’s expressed empathy in each “empathetic opportunity” [21].","This is done through the use of “empathetic extenders” [21], which take standard forms such as “How sad”, “How awful” or “It’s very hard” [26].","This paper presents the basis for a computational model tasked with computing patient shame and the empathetic response of a caregiver during “empathetic opportunities” [21] in their interaction.","[21]."],"2086182437":["for the purposes of restricting weapons systems in a lethal autonomous robotic agent [2]."],"2087077911":["Finally, the Confirmation of Flaw component response is going to increase when the patient is “mortified” because of his inability to complete a common task, or he has “muddled” thoughts such that he is unable or unwilling to interact well with the caregiver [17].","Note that the sixth category introduced by Retzinger [17] (“direction indication” of shame) decomposed nicely into “ridicule” in external shame and “confusion/indifferent” in internal shame.","Retzinger [17] defined six different categories (“direct indication”, “abandonment/ separation/ isolation”, “ridicule”, “inadequate”, “discomfort”, and “confused/ indifferent”) of words that not only frequently appear in the “context of shame” but also help to define circumstances where shame arises.","Terms such as “alienated”, “deserted” or “ostracized” [17] define the Lack of Acceptance component.","The language that elucidates the lowest level components is taken directly from Retzinger’s [17] content analysis.","When the patient perceives himself to be “worthless” or “helpless” [17], as in the Perception of Flawed Self component, the patient is profoundly sad.","Within internal shame, there are two components (“inadequate” and “confused/indifferent”) drawn from the work of Retzinger [17]."],"2103915281":["Gottschalk [7] claims that the “magnitude” of a “psychological state” is proportional to the “frequency of occurrence” of words associated with that state, the degree to which the words fall in that state, and the degree to which the words refer to the self."],"2105369152":["Tickle-Degnen [23, 24] has presented a substantial body of work showing the difficulties of those with expressive disorders to attain high rapport with their caregivers due to the critical role nonverbal communication plays in establishing such rapport.","particularly vulnerable to stigmatization during interactions with their caregivers due to their inability to express affect through nonverbal channels [24].","patient is feeling, which can lead them to stigmatize the patient [24]."],"2144997512":["[22] show how, when Parkinson’s patients are discussing frustrating activities (a discussion that has a high likelihood of inducing shame), they tend to use more negative language while there is no significant difference in their nonverbal behavior due to the expressive mask."],"2149186291":["As mentioned above, anger is recognized in healthy populations, in controlled settings with easily obtainable physiological signals [11].","Fear was reliably differentiated from sadness, anger, surprise, frustration, and amusement using galvanic skin response, skin temperature and heart rate signals in healthy populations [11].","Frustration is differentiated based on simple physiological signals [11].","Sadness is assessed with the basic physiological signals as noted above [11]."],"2167185059":["Nijhof [13] explicitly described Parkinson’s disease as a “problem of shame”."],"2901494586":["An empathetic response is motivated by both emotion and cognition [5, 8].","The patient will likely avert his eyes [8] as a means of trying to withdraw.","found with the self [8]."],"3124189541":["External shame, corresponding to withdrawal and unworthiness shame, is experienced when it is those external to the flawed self that present the self as flawed [10]; causing the shamed person to hide/withdraw from the situation [3, 6].","In the psychology literature, shame is often decomposed into two independent components [3, 6, 10].","These independent components take different names: internal/external shame [10], defensive/unworthiness shame [5], and negative-selfevaluation/withdrawal shame [3].","shame also depends on an individual’s “proneness” to shame [3]."]},"abstract":"Parkinson’s disease (PD) patients with an expressive mask are particularly vulnerable to stigmatization during interactions with their caregivers due to their inability to express affect through nonverbal channels. Our approach to uphold PD patient dignity is through the use of an ethical robot that mediates patient shame when it recognizes norm violations in the patient-caregiver interaction. This paper presents the basis for a computational model tasked with computing patient shame and the empathetic response of a caregiver during “empathetic opportunities” in their interaction. A PD patient is liable to suffer indignity when there is a substantial difference between his experienced shame and the empathy shown by the caregiver. When this difference falls outside of acceptable set bounds (norms), the robotic agent will act using subtle, nonverbal cues to guide the relationship back within these bounds, preserving patient dignity."},{"id":1966669949,"microsoftAcademicId":1966669949,"numberInSourceReferences":146,"doi":"10.1109/HUMANOIDS.2014.7041349","title":"Recognition and expression of emotions by a symbiotic android head","authors":[{"LN":"Mazzei","FN":"Daniele","affil":"University of Pisa"},{"LN":"Zaraki","FN":"Abolfazl","affil":"University of Pisa"},{"LN":"Lazzeri","FN":"Nicole","affil":"University of Pisa"},{"LN":"Rossi","FN":"Danilo De","affil":"University of Pisa"}],"year":2014,"journal":"2014 IEEE-RAS International Conference on Humanoid Robots","references":[2099019320,2293081071,1989104072,2149628368,2077324971,1264889757,2108385997,2024218186,2070645165,2112065758,2167741567,2026093575,1987411016,2109923073,2123912601,2025637627,1966378118,2060349841,1544484417,2001619467,2171676448,2270113935,2605232330,1711056836,2267849,168183347,2328235269,1982838075],"citationsCount":4,"abstract":"The creation of social empathie communication channels between social robots and humans has started to become reality. Nowadays, the development of empathie and affective agents is giving to scientists another way to explore the social dimension of human beings. In this work, we introduce the FACE humanoid project that aims at creating a social and emotional android. FACE is an android head with an articulated neck mounted on a passive body. In order to enable FACE to perceive and express emotions, two dedicated engines have been developed. A sensory apparatus able to perceive the \"social world\", and a facial expressions generation engine that allows the robot to express its synthetic emotions. The system has been also integrated with an attention-based gaze generation component that allows the robot to autonomously follow a conversation between its partners. The developed framework has been implemented and tested in several standard human-robot interaction settings. Results demonstrated the promising social capabilities of the robot to perceive and convey emotions to humans through the generation of emotional perceivable facial expressions and socially aligned behaviour."},{"id":2899785433,"microsoftAcademicId":2899785433,"numberInSourceReferences":27,"doi":"10.1109/ROMAN.2018.8525652","title":"Artificial Empathy in Social Robots: An analysis of Emotions in Speech","authors":[{"LN":"James","FN":"Jesin","affil":"University of Auckland"},{"LN":"Watson","FN":"Catherine Inez","affil":"University of Auckland"},{"LN":"MacDonald","FN":"Bruce","affil":"University of Auckland"}],"year":2018,"journal":"2018 27th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)","references":[1581387623,1976869056,2110585328,1628789162,2095436958,2325035506,2150791533,2027693172,2075132641,2136132767,2022514589,2080184306,1990114866,2095508859,1753344991,2190260761,1945345301,1978952352,2126969072,2889112744,2052730064,2610161631,175021017,1497831663,2734163268],"citationsCount":6,"citationContext":{"175021017":["A lot of focus is given to synthesizing some well-defined primary emotions [33-38] which is used as the voice of the HRI agent to respond to stimuli.","such as facial expressions, gestures and speech [6-8,34,37]."],"1497831663":["Emotions are expressed by variations in prosody component (like varying intonation, speech rate, stress) [31,39]."],"1581387623":["relief, hope) [26,27]."],"1628789162":["empathetic voice received positive ratings for likeability, trustworthiness, and were also perceived as supportive [11]."],"1753344991":["module [15, 16] which defines 5 scales to rate a clinician’s empathy."],"1976869056":["factors that enhance robots’ acceptance are their appearance, humanness, personality, expressiveness and adaptability [3]."],"1978952352":["There are various theories that define primary and secondary emotions [25], but here we will be looking at"],"1990114866":["People make judgments about robots’ personalities based on their voice [9, 10].","Robots that interact in social situations are novel to people who use them due to their limited experience in actually interacting with robots [9]."],"2027693172":["relief, hope) [26,27]."],"2052730064":["such as facial expressions, gestures and speech [6-8,34,37]."],"2075132641":["It was observed that the acceptance for the robot was enhanced when a human-like voice was used compared to a robotic voice [12]."],"2080184306":["Empathetic robots reduce stress among users, improve their comfort and performance in task achievement [21].","users’ affective state [21]."],"2095436958":["Examples of social robots that use speech to interact with people are Kismet [17], a story teller robot [18], robot guide [19] and reception robots [20]."],"2095508859":["A lot of focus is given to synthesizing some well-defined primary emotions [33-38] which is used as the voice of the HRI agent to respond to stimuli."],"2110585328":["Also, [14] shows that more playful speaking style produced more willingness to perform a task, compared to a serious speaking style."],"2126969072":["A lot of focus is given to synthesizing some well-defined primary emotions [33-38] which is used as the voice of the HRI agent to respond to stimuli."],"2150791533":["A lot of focus is given to synthesizing some well-defined primary emotions [33-38] which is used as the voice of the HRI agent to respond to stimuli."],"2190260761":["A lot of focus is given to synthesizing some well-defined primary emotions [33-38] which is used as the voice of the HRI agent to respond to stimuli."],"2610161631":["Examples of social robots that use speech to interact with people are Kismet [17], a story teller robot [18], robot guide [19] and reception robots [20]."],"2734163268":["However, the labels obtained in this way are subjective and have been reported to be affected by inter-observer variations [29]."],"2889112744":["English [32] and acoustic analysis of the corpus is currently being conducted"]},"abstract":"Artificial speech developed using speech synthesizers has been used as the voice for robots in Human Robot Interaction (HRI). As humans anthropomorphize robots, an empathetically interacting robot is expected to increase the level of acceptance of social robots. Here, a human perception experiment evaluates whether human subjects perceive empathy in robot speech. For this experiment, empathy is expressed only by adding appropriate emotions to the words in speech. Also, humans' preferences for a robot interacting with empathetic speech versus a standard robotic voice are also assessed. The results show that humans are able to perceive empathy and emotions in robot speech, and prefer it over the standard robotic voice. It is important for the emotions in empathetic speech to be consistent with the language content of what is being said, and with the human users' emotional state. Analyzing emotions in empathetic speech using valence-arousal model has revealed the importance of secondary emotions in developing empathetically speaking social robots."},{"id":2899905311,"microsoftAcademicId":2899905311,"numberInSourceReferences":110,"doi":"10.1177/0301006618809919","title":"Avatars in Pain: Visible Harm Enhances Mind Perception in Humans and Robots.","authors":[{"LN":"Swiderska","FN":"Aleksandra","affil":"University of Warsaw"},{"LN":"Küster","FN":"Dennis","affil":"University of Bremen"}],"year":2018,"journal":"Perception","references":[2124809053,2148616640,2115040353,2147093380,2141973273,2131227926,2044954931,2108616861,2165113252,2049520464,1968026168,2094482505,2003350835,2157289326,2097210811,2154556191,2098850353,2087683740,2764192991,1992442759,2062412390,2791568830,2009675410,2125675267,2166757416,2037663688,2117084973,2139203871,2131619946,2041755880,2006450709,2151660013,2738048415,2227594099,1975020634,1692480425,2072958070,2751194593,2147209635],"citationsCount":6,"abstract":"Previous research has shown that when people read vignettes about the infliction of harm upon an entity appearing to have no more than a liminal mind, their attributions of mind to that entity increased. Currently, we investigated if the presence of a facial wound enhanced the perception of mental capacities (experience and agency) in response to images of robotic and human-like avatars, compared with unharmed avatars. The results revealed that harmed versions of both robotic and human-like avatars were imbued with mind to a higher degree, irrespective of the baseline level of mind attributed to their unharmed counterparts. Perceptions of capacity for pain mediated attributions of experience, while both pain and empathy mediated attributions of abilities linked to agency. The findings suggest that harm, even when it appears to have been inflicted unintentionally, may augment mind perception for robotic as well as for nearly human entities, at least as long as it is perceived to elicit pain."},{"id":2478564110,"microsoftAcademicId":2478564110,"numberInSourceReferences":131,"doi":"10.1007/978-3-319-42417-0_6","title":"A Preliminary Framework for a Social Robot “Sixth Sense”","authors":[{"LN":"Cominelli","FN":"Lorenzo","affil":"University of Pisa"},{"LN":"Mazzei","FN":"Daniele","affil":"University of Pisa"},{"LN":"Carbonaro","FN":"Nicola","affil":"University of Pisa"},{"LN":"Garofalo","FN":"Roberto","affil":"University of Pisa"},{"LN":"Zaraki","FN":"Abolfazl","affil":"University of Pisa"},{"LN":"Tognetti","FN":"Alessandro","affil":"University of Pisa"},{"LN":"Rossi","FN":"Danilo Emilio De","affil":"University of Pisa"}],"year":2016,"journal":"5th International Conference on Biomimetic and Biohybrid Systems, Living Machines 2016","references":[2099019320,2468865363,2100119371,2145710484,1599068589,1981395202,2026093575,1997714027,2095831892,2276668503,2104028188,2887590439,2043371315,2128141640,2602837613,1966669949,2328235269],"citationsCount":1,"abstract":"Building a social robot that is able to interact naturally with people is a challenging task that becomes even more ambitious if the robots’ interlocutors are children involved in crowded scenarios like a classroom or a museum. In such scenarios, the main concern is enabling the robot to track the subjects’ social and affective state modulating its behaviour on the basis of the engagement and the emotional state of its interlocutors. To reach this goal, the robot needs to gather visual and auditory data, but also to acquire physiological signals, which are fundamental for understating the interlocutors’ psycho-physiological state. Following this purpose, several Human-Robot Interaction (HRI) frameworks have been proposed in the last years, although most of them have been based on the use of wearable sensors. However, wearable equipments are not the best technology for acquisition in crowded multi-party environments for obvious reasons (e.g., all the subjects should be prepared before the experiment by wearing the acquisition devices). Furthermore, wearable sensors, also if designed to be minimally intrusive, add an extra factor to the HRI scenarios, introducing a bias in the measurements due to psychological stress. In order to overcome this limitations, in this work, we present an unobtrusive method to acquire both visual and physiological signals from multiple subjects involved in HRI. The system is able to integrate acquired data and associate them with unique subjects’ IDs. The implemented system has been tested with the FACE humanoid in order to assess integrated devices and algorithms technical features. Preliminary tests demonstrated that the developed system can be used for extending the FACE perception capabilities giving it a sort of sixth sense that will improve the robot empathic and behavioural capabilities."},{"id":2249189521,"microsoftAcademicId":2249189521,"numberInSourceReferences":45,"doi":"10.1109/SSCI.2015.26","title":"Empathic Interaction Using the Computational Emotion Model","authors":[{"LN":"Rasool","FN":"Zeeshan"},{"LN":"Masuyama","FN":"Naoki","affil":"Information Technology University"},{"LN":"Islam","FN":"Md. Nazrul","affil":"Information Technology University"},{"LN":"Loo","FN":"Chu Kiong","affil":"Information Technology University"}],"year":2015,"journal":"2015 IEEE Symposium Series on Computational Intelligence","references":[2164598857,1841352775,2042962409,2096476371,1975000068,1991015565,2544981553,2156516654,2159316671,2033702744,2092804630,2135370090,2032099985,2010943014,2166783360,614096023,1981509185,1978847468,2117917085,2102548748,1968634614,2105119762,2035056751,2006609774,2022053372,2159698665,2143925122,2093354302,1490647195,2141931716,652662681,2109916008,2027014613,67469161],"citationsCount":3,"citationContext":{"67469161":["The empathic response is affected by the factors like personality and mood [16].","The personality and mood are directly affected over the different type empathic response [16].","[16]."],"652662681":["Personality is one of the key factors to construct the individual differences, such as perception, motivation and cognition [12], [13]."],"1490647195":["Personality is one of the key factors to construct the individual differences, such as perception, motivation and cognition [12], [13]."],"1968634614":["Individuals who can self-regulate increase in their emotional arousal are more likely to respond with sympathy and sympathy focuses on reducing some of the other person’s distress [10]."],"1975000068":["In human social interaction, empathy promotes pro-social and cooperative behavior leading to moral acts like helping, caring, and justice [2]."],"1981509185":["Moreover, several studies discussed that the presence of empathic emotion in a robot has significant positive effects on a user’s impression of robot and friendly relationship between human and robot [1] [5]-[7].","Such interaction is believed to enhance supportive and communicative social skills of robots in human-robot interaction [1].","ca ∈ [0, 1]"],"1991015565":["Earlier research indicate the presence of similarity of social behavior between humanhuman and human-robot interactions [3]–[5]."],"2010943014":["Earlier research indicate the presence of similarity of social behavior between humanhuman and human-robot interactions [3]–[5].","Moreover, several studies discussed that the presence of empathic emotion in a robot has significant positive effects on a user’s impression of robot and friendly relationship between human and robot [1] [5]-[7]."],"2022053372":["Conventionally there are several studies to extract the facial features [19], [20]."],"2027014613":["Furthermore, TGART [23] is applied to build a structure model.","In this study in order to generate the artificial emotions in robot from human facial expressions, we improved the facial expression recognition algorithms [21] based on Constrained Local Model (CLM) with LeaderP clustering algorithms [22] and topological Gaussian Adaptive Resonance theory algorithm (TGART) [23]."],"2032099985":["Moreover, several studies discussed that the presence of empathic emotion in a robot has significant positive effects on a user’s impression of robot and friendly relationship between human and robot [1] [5]-[7]."],"2033702744":["According to experimental results in reference [21], this face tracking framework has sufficient ability to recognize 6 basic facial expressions such as neutral, surprise, sadness, fear, angry and happy based on Ekman’s emotional model [27], [28]."],"2035056751":["The combination of parallel and reactive empathic responses is identified as positive and helpful towards improving the emotional state of other person from negative emotions to positive [36].","The parallel emotions are helpful towards maintaining the other person’s current emotional state [36]."],"2042962409":["The relationship between the five factors of personality and PAD model is derived through the linear regression analysis [32]."],"2092804630":["In addition, researches in the human psychology field have been expected that the human emotional function is composition result of core affect, emotion and mood states [29], [30]."],"2093354302":["For the empathic interaction, it is important to communicate the empathic responses instead of inhibiting [37]."],"2096476371":["Empathy refers to a communicative process in which we understand and respond to the feelings of other person [8]."],"2102548748":["Cluster representative will be formed using Zero-mean Normalized CrossCorrelation (ZNCC) method [26]."],"2105119762":["Mehrabian utilized the five factors of personality to represent the emotional information as Pleasure-Arousal-Dominance (PAD) temperament model [31]."],"2109916008":["Conventionally there are several studies to extract the facial features [19], [20]."],"2117917085":["The results of the study of associations between personality dimensions and empathy also confirmed positive associations between agreeableness, openness and neuroticism to experience and empathy [15]."],"2143925122":["In this study in order to generate the artificial emotions in robot from human facial expressions, we improved the facial expression recognition algorithms [21] based on Constrained Local Model (CLM) with LeaderP clustering algorithms [22] and topological Gaussian Adaptive Resonance theory algorithm (TGART) [23].","LeaderP clustering algorithm [22] is applied to build an appearance model."],"2159316671":["Among several models of personality, one of the widely accepted personality models is five factors (Openness, Conscientiousness, Extraversion, Agreeableness and Neuroticism) model that is proposed by McCrae and Costa [14].","In addition, Point Distribution Model (PDM) is applied to TABLE I: Five Factors of Personality [14]"],"2164598857":["Static or dynamic face is captured to detect a face applying Viola-Jones cascade classified [24] from each face frame."]},"abstract":"This paper describes the empathy oriented human-robot interaction model. It is projected to design the model capable of different empathic responses (parallel and reactive) during the course of interaction with the user, depending upon the personality and mood factors of the robot. The proposed model encompasses three main stages i.e., Perception, empathic appraisal and empathic expression. Perception refers to capturing user's emotion state via facial expression recognition. Empathic appraisal is based on the computational emotional model for generating its internal emotions, mood state and empathic responses. The internal emotions are defined using psychological studies and generated on 2D (pleasure-arousal) scaling model, whereas, fuzzy logic is used to calculate the intensity of the each emotion. A virtual facial expression simulator is applied for expression of resultant empathic emotions. Preliminary experimental results show that the proposed model is capable of exhibiting different empathic responses with respect to the personality and mood factors."},{"id":2592980642,"microsoftAcademicId":2592980642,"numberInSourceReferences":129,"doi":"10.3233/978-1-61499-708-5-195","title":"An interdisciplinary approach to improving cognitive human-robot interaction: a novel emotion-based model","authors":[{"LN":"Fosch-Villaronga","FN":"Eduard","affil":"Department of Governance and Technology for Sustainability"},{"LN":"Barco","FN":"Alex"},{"LN":"Özcan","FN":"Beste"},{"LN":"Shukla","FN":"Jainendra"}],"year":2016,"journal":"International Research Conference Robophilosophy 2016 / TRANSOR 2016: What Social Robots Can and Should Do","references":[],"citationsCount":2,"abstract":"Socially Assistive Robotics (SAR) aims to provide robot-assisted therapy, for physical as well as cognitive rehabilitation. The paper analyzes two distinct use cases of cognitive rehabilitation therapies, one among involving children with Traumatic Brain Injury (TBI); and another one; second among involving individuals with Intellectual Disability (ID), and raises concerns regarding emotional adaptation, personalization, design, and ELS issues of human-robot interaction in such cases. The paper's aim is to provide some guidance on how social robots should be designed in order to accommodate emotions in HRI as well as to respect the rights of the persons with disabilities. We argue that it is critically important to address the concerns highlighted in order to empower robots with empathetic behavior and to deliver effective cognitive rehabilitation therapies."},{"id":3048638976,"microsoftAcademicId":3048638976,"numberInSourceReferences":99,"doi":"10.1109/ACCESS.2020.3015533","title":"A Computation Model for Learning Programming and Emotional Intelligence","authors":[{"LN":"Rafique","FN":"Memoona","affil":"University of Engineering and Technology, Lahore"},{"LN":"Hassan","FN":"Muhammad Awais","affil":"University of Engineering and Technology, Lahore"},{"LN":"Jaleel","FN":"Abdul","affil":"University of Engineering and Technology, Lahore"},{"LN":"Khalid","FN":"Hina","affil":"University of Engineering and Technology, Lahore"},{"LN":"Bano","FN":"Gulshan","affil":"[University of Sialkot, Sialkot, Pakistan]"}],"year":2020,"journal":"IEEE Access","references":[2402144811,2109255472,1581387623,2125416623,2051339053,1714410028,2572303978,2120640415,2120040514,2110437072,2062845262,2146567769,2160851124,2115087102,2560862806,1531613484,2178774099,2238413030,1946459050,2082666068,127404051,2516608199,2014899553,2182252378,1965609069,1973287994,2475950333,2516665170,2889256612,2940999120,1969034164,1976874614,2520903787,2782410497,2623699854,2763721606,2060711128,1876390046,2055955637,2003893392,2883303805],"citationsCount":1,"abstract":"Introducing coding in early education improves the logical and computational thinking in kids. However, cognitive skills are not sufficient for a successful life. Understanding and managing the emotions of oneself is another crucial factor in success. The current state of the art teaching methods educates the kids about programming and emotional intelligence independently. In our opinion, it is advantageous to teach kids emotional intelligence, along with the programming concepts. However, the literature lacks the studies that make students emotionally aware while teaching them programming. This research aims to prepare students to be cognitively healthy as well as emotionally intelligent with the hypothesis that a kid's emotional intelligence can be enhanced while teaching them cognitive skills. We proposed a computational model that teaches programming and emotional intelligence side by side to students. The model provides a curriculum and related tools. For evaluations, five hundred students of a public school were involved in different activities to find the effectiveness of the proposed model. These students were divided into five groups (A, B, C, D, and E), each having a mean age of 4, 5, 6, 7, and 8 years, respectively. Students performed multiple adaptive scenarios of path-finding that were based on self-awareness, social-awareness, sharing, and empathy emotions. Students provide the programming instructions such as sequencing, conditional statements, and looping to a robot. The children have successfully improved in both fundamental programming constructs and emotional intelligence skills. The research also successfully reduced screen time problem by providing a screen-free student interface."},{"id":2998563994,"microsoftAcademicId":2998563994,"numberInSourceReferences":156,"doi":"10.1162/COLI_A_00368","title":"The Design and Implementation of XiaoIce, an Empathetic Social Chatbot","authors":[{"LN":"Zhou","FN":"Li","affil":"Microsoft"},{"LN":"Gao","FN":"Jianfeng","affil":"Microsoft"},{"LN":"Li","FN":"Di","affil":"Microsoft"},{"LN":"Shum","FN":"Heung-Yeung","affil":"Microsoft"}],"year":2020,"journal":"Computational Linguistics","references":[2130942839,2162040794,1532325895,2101105183,1895577753,2745461083,2964199361,2136189984,2339343773,2963206148,2962883855,2154652894,1956340063,1591706642,1518951372,2963084599,1931639407,2123301721,2740168486,2963963856,2506483933,2131876387,2964352131,2963790827,2109910161,2410983263,2963475460,2963167310,2963527228,2251008987,2162059449,2586847566,2963035145,2963903950,2949868354,2625940279,1646777016,2140016149,2406390611,3104546989,2783549597,2907283777,2963062932,2082468228,2140054881,2583186419,2113033979,2963371754,2962821719,2806344213,2783215745,2134566648,2270499809,2161466446,2887153785,2945397504,2963382311,2573143556,2584575645,2737041661,2912913215,2902504655,2911833075,3099743661],"citationsCount":132,"citationContext":{"1931639407":["64 DMSM (Fang et al. 2015) 2.","The generation-based approach uses an image-to-text generator, an extension of the Microsoft Image Captioning system (Fang et al. 2015) that is re-trained on the image-","This is achieved by using the Deep Multimodal Similarity Model (Fang et al. 2015) trained on a large amount of image–comment pairs."],"2101105183":["8%) (Papineni et al. 2002).","skill outperforms several state-of-the-art image captioning systems on a test set consisting of 5K image–comment pairs whose ratings are 2, in terms of BLEU-4 (Papineni et al. 2002), METEOR (Banerjee and Lavie 2005), CIDEr (Vedantam, Lawrence, Zitnick, and Parikh 2015), ROUGE-L (Lin 2004), and SPICE (Anderson et al.","text generation tasks like machine translation and text summarization, using string and n-gram matching metrics such as BLEU (Papineni et al. 2002), METEOR (Banerjee and Lavie 2005), and ROUGE (Lin 2004)."],"2131876387":["4 DSSM stands for Deep Structured Semantic Models (Huang et al. 2013; Shen et al. 2014), or more generally, Deep Semantic Similarity Model (Gao et al."],"2136189984":["4 DSSM stands for Deep Structured Semantic Models (Huang et al. 2013; Shen et al. 2014), or more generally, Deep Semantic Similarity Model (Gao et al."],"2140016149":["We use an off-the-shelf named entity recognizer (Gao et al. 2005) for identifying 13 types of"],"2162059449":["The comment candidates generated by the generators are aggregated and ranked using a boosted tree ranker (Wu et al. 2010).","The response candidates generated by three generators are aggregated and ranked using a boosted tree ranker (Wu et al. 2010).","” These topics are scored by their relevance using a boosted tree ranker (Wu et al. 2010) trained on manually labeled training data."],"2251008987":["2014), or more generally, Deep Semantic Similarity Model (Gao et al. 2014)."],"2270499809":["2 At runtime, we use Qc in s as query to retrieve up to 400 response candidates using keyword search and semantic search based on machine learning representations of the paired database [Wu, Wang, and Xue 2016; Zhang et al. 2016]."],"2586847566":["12 As pointed out in Ghazvininejad et al. (2018), E2E models are usually good at producing responses that have plausible overall structure, but often struggle when it comes to generating names and facts that connect to the real world, due to the lack of grounding."],"2625940279":["comment pairs we have collected for XiaoIce and has incorporated additional modules to control high-level sentiment and style factors in comment generation (Mathews, Xie, and He 2016; Gan et al. 2017)."],"2745461083":["63 Up-Down (Anderson et al. 2018) 3."],"2963084599":["27 LSTM-RL (Rennie et al. 2017) 3."],"2963167310":["But, as pointed out in Gao, Galley, and Li (2019), the correlation analysis by Liu et al. (2016) is performed at the sentence level whereas BLEU is designed from the outset to be used as a corpus-level metric.","For example, some recent studies (Li et al. 2016c; Fang et al. 2017) show that encompassing bland but"],"2963206148":["Adapted from Li et al. (2016b).","In the first pilot study reported in Li et al. (2016b), we compare the persona model","The second is the LSTM-MMI model (Li et al. 2016a), which is one of the state-of-the-art neural response generation models."],"2963382311":["Galley et al. (2015) showed that the correlation of string-based metrics (e.","XiaoIce is developed on an empathetic computing framework (Cai 2006; Fung et al. 2016) that enables the machine (social chatbot in our case) to recognize human feelings and states, understand user intents, and respond to user needs dynamically."],"2963475460":["Some recent dialogue challenges (Dinan et al. 2018; Ram et al. 2018) also take a similar, manual evaluation approach, using paid workers and unpaid volunteers."],"2964199361":["The neural response generator in XiaoIce follows the sequence-to-sequence (seq2seq) framework (Cho et al. 2014; Sutskever, Vinyals, and Le 2014) used for conversation response generation (Shang, Lu, and Li 2015; Sordoni et al."],"2964352131":["As shown in Figure 7, although a typical seq2seq model that is not grounded in any persona often outputs inconsistent responses (Li et al. 2016b), XiaoIce is able to generate consistent and humorous responses.","Figure 7 (Left) Examples of inconsistent responses generated using a seq2seq model (S2S-Bot) that is not grounded in persona (Li et al. 2016b).","For such scenarios, E2E approaches often lead to a very simple system architecture, such as RNNbased systems (Shang, Lu, and Li 2015; Vinyals et al. 2015; Li et al. 2016b), where the neural network–based response generation models can be easily trained on large-scale free-form, open-domain data sets (e.","The generator is based on a GRU-RNN model, similar to the Speaker-Addressee model (Li et al. 2016b).","The neural generation component of Replika is persona-based (Li et al. 2016b), similar to the neural response generator in XiaoIce."]},"abstract":"This article describes the development of Microsoft XiaoIce, the most popular social chatbot in the world. XiaoIce is uniquely designed as an artifical intelligence companion with an emotional conn..."},{"id":2963270767,"microsoftAcademicId":2963270767,"numberInSourceReferences":122,"doi":"10.1145/3173574.3173989","title":"Touch Your Heart: A Tone-aware Chatbot for Customer Care on Social Media","authors":[{"LN":"Hu","FN":"Tianran","affil":"University of Rochester"},{"LN":"Xu","FN":"Anbang","affil":"IBM"},{"LN":"Liu","FN":"Zhe","affil":"IBM"},{"LN":"You","FN":"Quanzeng","affil":"University of Rochester"},{"LN":"Guo","FN":"Yufan","affil":"IBM"},{"LN":"Sinha","FN":"Vibha","affil":"IBM"},{"LN":"Luo","FN":"Jiebo","affil":"University of Rochester"},{"LN":"Akkiraju","FN":"Rama","affil":"IBM"}],"year":2018,"journal":"Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems","references":[2949888546,1924770834,2339343773,2402268235,3121590070,10957333,2135555017,2611049140,2311783643,2138794141,2605133118,2163986367,2040384293,2010943014,2145855633,2053782908,2464981846,131086928,2115648139,2038549278,2566531585,2250511935,2407706885,2574065282,1976543439,2964091292,2014723826,2611556044,2110432162,2162643738,2606357051,2594125959,2069873412,2767313619,2140885637,2963758320,3098801554,2560283201,2119275253,2590528524,2025416816,94370087,1572240091,2143197761],"citationsCount":84,"abstract":"Chatbot has become an important solution to rapidly increasing customer care demands on social media in recent years. However, current work on chatbot for customer care ignores a key to impact user experience - tones. In this work, we create a novel tone-aware chatbot that generates toned responses to user requests on social media. We first conduct a formative research, in which the effects of tones are studied. Significant and various influences of different tones on user experience are uncovered in the study. With the knowledge of effects of tones, we design a deep learning based chatbot that takes tone information into account. We train our system on over 1.5 million real customer care conversations collected from Twitter. The evaluation reveals that our tone-aware chatbot generates as appropriate responses to user requests as human agents. More importantly, our chatbot is perceived to be even more empathetic than human agents."},{"id":2996665814,"microsoftAcademicId":2996665814,"numberInSourceReferences":143,"doi":"10.1109/ACIIW.2019.8925190","title":"Detection of Real-World Driving-Induced Affective State Using Physiological Signals and Multi-View Multi-Task Machine Learning","authors":[{"LN":"Lopez-Martinez","FN":"Daniel","affil":"Massachusetts Institute of Technology"},{"LN":"El-Haouij","FN":"Neska","affil":"Massachusetts Institute of Technology"},{"LN":"Picard","FN":"Rosalind","affil":"Massachusetts Institute of Technology"}],"year":2019,"journal":"2019 8th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)","references":[2121947440,2162800060,2594475271,2171801645,2913340405,2131213359,2282606863,1518433044,2000503364,2593216954,2164809575,2888700752,2963665779,2164215987,2599893746,2890362123,2769022455,290165499,2768582154],"citationsCount":1,"citationContext":{"290165499":["For example, Nass and team [1], [2] showed that if the car navigation system’s tone of voice is kept the same, then the tone that works best when a driver is in a happy state is the tone that works worst when the driver is in a mildly upset state, and vice-versa."],"1518433044":["For peaks (startle) detection, we used the same approach as used in Healey’s thesis [18].","The EDA signal was passed through a low pass filter to eliminate high frequency noise, similar to Healey [18]."],"2000503364":["The EDA and HR were selected since they are relevant signals in stress recognition, especially for driving task performance [3], [6], [7], [11]."],"2121947440":["1) [21].","Algorithm 1 Normalized spectral clustering [21]"],"2164215987":["The HciLab database [15] consists of data collected during"],"2171801645":["AffectiveROAD [16] is a publicly available dataset [14] collected in Tunisia following the driving protocol of the MIT drivedb dataset [3].","The EDA and HR were selected since they are relevant signals in stress recognition, especially for driving task performance [3], [6], [7], [11].","The MIT drivedb [3] dataset contains data from 20 miles of","This computed score was found correlated with the road type [3].","rate (BR), electromyogram (EMG) and electroencephalogram (EEG), showing promising accuracy for driver’s affective state recognition [3]–[11].","the work of Healey and Picard [3] validated the assumption of low, medium and high stress levels during the rest, highway and city driving, respectively."],"2282606863":["The EDA and HR were selected since they are relevant signals in stress recognition, especially for driving task performance [3], [6], [7], [11]."],"2599893746":["The EDA and HR were selected since they are relevant signals in stress recognition, especially for driving task performance [3], [6], [7], [11].","This result confirms the finding of [11] where the EDA was found more important compared to the different physiological signals, especially HR, when using a random forest approach for stress level classification on the MIT drivedb database.","rate (BR), electromyogram (EMG) and electroencephalogram (EEG), showing promising accuracy for driver’s affective state recognition [3]–[11]."],"2913340405":["The other concept, multi-task learning (MTL), focuses on learning several related prediction tasks simultaneously using a shared representation [22], where each task corresponds to each of the different profiles defined in Sec."],"2963665779":["multi-task neural networks [9], [24])."]},"abstract":"Affective states have a critical role in driving performance and safety. They can degrade driver situation awareness and negatively impact cognitive processes, severely diminishing road safety. Therefore, detecting and assessing drivers' affective states is crucial in order to help improve the driving experience, and increase safety, comfort and well-being. Recent advances in affective computing have enabled the detection of such states. This may lead to empathic automotive user interfaces that account for the driver's emotional state and influence the driver in order to improve safety. In this work, we propose a multiview multi-task machine learning method for the detection of driver's affective states using physiological signals. The proposed approach is able to account for inter-drive variability in physiological responses while enabling interpretability of the learned models, a factor that is especially important in systems deployed in the real world. We evaluate the models on three different datasets containing real-world driving experiences. Our results indicate that accounting for drive-specific differences significantly improves model performance."},{"id":3158675732,"microsoftAcademicId":3158675732,"numberInSourceReferences":175,"doi":"10.1007/S12369-021-00779-5","title":"Enhancing Emotional Support: The Effect of a Robotic Object on Human–Human Support Quality","authors":[{"LN":"Erel","FN":"Hadas","affil":"Harvard University"},{"LN":"Trayman","FN":"Denis","affil":"Harvard University"},{"LN":"Levy","FN":"Chen","affil":"Harvard University"},{"LN":"Manor","FN":"Adi","affil":"Harvard University"},{"LN":"Mikulincer","FN":"Mario","affil":"Interdisciplinary Center Herzliya"},{"LN":"Zuckerman","FN":"Oren","affil":"Harvard University"}],"year":2021,"journal":"International Journal of Social Robotics","references":[1951008065,2032568497,2623779865,2098676269,2153480757,1424744123,751470531,2099386544,2021641437,2085436006,2143427281,2145978062,2024649868,2162465941,2156897926,1648167046,2063594338,2010943014,1998187122,2012372995,2135080807,2252981981,2761423999,2053782908,2955088691,2611961900,2225036064,2062086084,2801150160,2792441573,2066391335,2754425142,2889357810,1990266238,2030648697,2157257781,2931992356,13388985,1981509185,2046993223,2241879647,1964417192,1975468505,2070727659,2080184306,2160449350,2897690397,2963427815,2285151045,2095508859,2888290426,2326503051,1997983973,2152051696,2494735004,2416739770,2069588696,2926787163,2942103387,1971039120,2790233857,2140321956,2023745022,3008551301,2034762802,2129526419,2900191078,2140778623,2540373225,2794264240,2982897089,3006245018,2346321436,2003798535,2611610300,2157652638,2013207517,2117916015,3114284325,1276312578,2092740615,1619839657,2890914488,2125123910,1978650987,2146939392,2901582010,2014725961,2063102691,2344835716,2078173020,2123973075,2972516722,2085239081,2002775149,3005609231,2070923916,3006584130,2900024863,2942706600,2041794987,2081673523,572390860,2104899019,2096416813,1973203431,3030076837,2048744599,2126689622,2543230841,2076276390,3044709562,3147106146,2139490082,2045854600,1972213162,2057308851,2125790208,2399180483,3148123701,1970053597,2940625855,2767443199],"citationsCount":0,"abstract":"Emotional support in the context of psychological caregiving is an important aspect of human–human interaction that can significantly increase well-being. In this study, we tested if non-verbal gestures of a non-humanoid robot can increase emotional support in a human–human interaction. Sixty-four participants were invited in pairs to take turns in disclosing a personal problem and responding in a supportive manner. In the experimental condition, the robotic object performed emphatic gestures, modeled according to the behavior of a trained therapist. In the baseline condition, the robotic object performed up-and-down gestures, without directing attention towards the participants. Findings show that the robot’s empathy-related gestures significantly improved the emotional support quality provided by one participant to another, as indicated by both subjective and objective measures. The non-humanoid robot was perceived as peripheral to the natural human–human interaction and influenced participants’ behavior without interfering. We conclude that non-humanoid gestures of a robotic object can enhance the quality of emotional support in intimate human–human interaction."},{"id":2516684112,"microsoftAcademicId":2516684112,"numberInSourceReferences":22,"doi":"10.21437/INTERSPEECH.2016-554","title":"A Deep Learning Approach to Modeling Empathy in Addiction Counseling.","authors":[{"LN":"Gibson","FN":"James","affil":"University of Southern California"},{"LN":"Can","FN":"Dogan","affil":"University of Southern California"},{"LN":"Xiao","FN":"Bo","affil":"University of Southern California"},{"LN":"Imel","FN":"Zac E.","affil":"EDUCATIONAL PSYCHOLOGY"},{"LN":"Atkins","FN":"David C.","affil":"University of Washington"},{"LN":"Georgiou","FN":"Panayiotis G.","affil":"University of Southern California"},{"LN":"Narayanan","FN":"Shrikanth S.","affil":"University of Southern California"}],"year":2016,"journal":"Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH","references":[2950577311,1606347560,2119466907,3099884890,2014399678,2181445435,1965394545,2397246247,2077641600,2405501373,1753344991,2408504567,2252086645],"citationsCount":23},{"id":1989499057,"microsoftAcademicId":1989499057,"numberInSourceReferences":86,"doi":"10.1007/S11423-015-9374-9","title":"Examining young children’s perception toward augmented reality-infused dramatic play","authors":[{"LN":"Han","FN":"Jeonghye","affil":"Cheongju National University of Education"},{"LN":"Jo","FN":"Miheon","affil":"Cheongju National University of Education"},{"LN":"Hyun","FN":"Eunja","affil":"Sungkyunkwan University"},{"LN":"So","FN":"Hyo-jeong","affil":"Pohang University of Science and Technology"}],"year":2015,"journal":"Educational Technology Research and Development","references":[1841352775,1970370179,3128690923,2170748977,2068391117,1993806858,1815090327,2123950369,2043975518,2116598827,1976868498,2160835417,1980763109,1569132324,2160851124,2083285440,1609508426,2916341084,2138439798,2639421075,1480358052,2082666068,2044766160,2097714524,2073508127,2047060504,2041065123,2018732718,2044436724,2993014201,172333647,2068801184,2080188495,1991263344,2043421012,1607885439,2162802129,1969807276,2053432561,1486544673,1578819508,2029763320,2053676691,2123486991,2039705815,2178896373,1707819230,2025965332,2050552455,147821797,2083636002,2750959279,602738865,150702117,2001535865],"citationsCount":129,"citationContext":{"1980763109":["First, AR technology allows learners to experience immersive learning environments by manipulating virtual content and objects through a tangible interface (Billinghurst et al. 2005; Kesim and Ozarslan 2012; Lee et al. 2004)."],"1991263344":["Yet, how to increase children’s immersion into dramatic play activities with limited resources available in kindergarten settings remains a challenge for many teachers (Sheridan et al. 2009)."],"2025965332":["Jo et al. (2011) developed an AR-based dramatic play projector robot with the capability of facial expression, text-to-speech (TTS), and action service.","Our prior research also found that young children tend to perceive that they could learn something educational from the robot, and the robot could assume some aspects of the teacher’s roles (Hyun and Yoon 2009).","We found that children aged below 10 perceived the great differences between computers and robots (Han 2010; Hyun and Yoon 2009)."],"2039705815":["Finally, AR technology enables a smooth transition from the mere presentation of information to the active exploration and knowledge construction (New Media Consortium 2012; Pemberton and Winter 2009)."],"2043975518":["Similarly, Kamarainen et al. (2013) research on the EcoMOBILE shows positive impacts of AR applications in students’ cognitive understanding."],"2047060504":["First, AR technology allows learners to experience immersive learning environments by manipulating virtual content and objects through a tangible interface (Billinghurst et al. 2005; Kesim and Ozarslan 2012; Lee et al. 2004).","For instance, research studies by Billinghurst et al. (2005) and Seichter (2007) have demonstrated that AR applications visualizing complex concepts and principles can promote experiential learning, and enhance the learners’ spatial abilities."],"2050552455":["Drenten et al. (2008) have found that even very young children (3–6 years old) were able to successfully utilize shopping scripts during their dramatic play in a grocery store context simulated in a classroom."],"2073508127":["Equation theory to children’s use of media, Chiasson and Gutwin (2005) found that children and adults did not show significantly different patterns in their response to the social characteristics added to a computer system.","RoboStage is another example of AR-augmented robot systems that users can take on either physical or virtual characters with authentic scenes by integrating mixed-reality technology and robot technology (Chang et al. 2010)."],"2083285440":["through such direct manipulation of virtual objects, learners can deepen their understanding, grasp causal relationships and organize knowledge (Kaufmann and Schmalstieg 2003).","through such direct manipulation of virtual objects, learners can deepen their understanding, grasp causal relationships and organize knowledge (Kaufmann and Schmalstieg 2003). Moreover, AR reinforces the users’ context awareness, which can help to better understand context-specific concepts, skills, and knowledge. Squire and Klopfer (2007) investigated the application of location-based AR simulation games on handheld computers."],"2116598827":["For instance, Adalgeirsson and Breazeal (2010) examined how people perceived a robot-mediated operator differently when they used a static tele-robot versus a physically-embodied and expressive tele-robot.","The unique affordance of robots lies in their technical power for enabling physical embodiment and immersion (Adalgeirsson and Breazeal 2010)."],"2138439798":["First, AR technology allows learners to experience immersive learning environments by manipulating virtual content and objects through a tangible interface (Billinghurst et al. 2005; Kesim and Ozarslan 2012; Lee et al. 2004)."],"2170748977":["This speculation is consistent with the previous studies reporting that children’s interaction with a robot could not be sustained for longer engagement despite the use of humanoid robots (Kanda et al. 2004; Park et al. 2011)."],"2750959279":["ment is well-documented (Mendoza and Katz 2008)."],"2993014201":["Yoon 2009; Han 2012). For instance, Tanaka et al. (2007) found that 18–24 month old toddlers treated a robot as a peer rather than a toy after interacting with a social robot for 5 months."]},"abstract":"Amid the increasing interest in applying augmented reality (AR) in educational settings, this study explores the design and enactment of an AR-infused robot system to enhance children’s satisfaction and sensory engagement with dramatic play activities. In particular, we conducted an exploratory study to empirically examine children’s perceptions toward the computer- and robot-mediated AR systems designed to make dramatic play activities interactive and participatory. A multi-disciplinary expert group consisting of early childhood education experts, preschool teachers, AR specialists, and robot engineers collaborated to develop a learning scenario and technological systems for dramatic play. The experiment was conducted in a kindergarten setting in Korea, with 81 children (aged 5–6 years old). The participants were placed either in the computer-mediated AR condition (n = 40) or the robot-mediated AR condition (n = 41). We administered an instrument to measure children’s perceived levels of the following variables: (a) satisfaction (i.e., interest in dramatic play & user-friendliness), (b) sensory immersion (i.e., self-engagement, environment-engagement & interaction-engagement), and (c) media recognition (i.e., collaboration with media, media function & empathy with media). Data analysis indicates that children in the robot-mediated condition showed significantly higher perceptions than those in the computer-mediated condition regarding the following aspects: interest in dramatic play (satisfaction), interactive engagement (sensory immersion), and empathy with media (media recognition). Furthermore, it was found that the younger-aged children and girls, in particular, perceived AR-infused dramatic play more positively than the older-aged children and boys, respectively. The contribution of this study is to provide empirical evidence about the affordances of robots and AR-based learning systems for young children. This remains a relatively unexplored area of research in the field of learning technologies. Implications of the current study and future research directions are also discussed."},{"id":2909084724,"microsoftAcademicId":2909084724,"numberInSourceReferences":32,"doi":"10.1109/SMC.2018.00297","title":"Counseling Robot Implementation and Evaluation","authors":[{"LN":"Kurashige","FN":"Kentarou","affil":"Muroran Institute of Technology"},{"LN":"Tsuruta","FN":"Setsuo","affil":"Tokyo Denki University"},{"LN":"Sakurai","FN":"Eriko","affil":"Bunri University of Hospitality"},{"LN":"Sakurai","FN":"Yoshitaka","affil":"ネットワークデザイン学科"},{"LN":"Knauf","FN":"Rainer","affil":"Technische Universität Ilmenau"},{"LN":"Damiani","FN":"Ernesto","affil":"University of Milan"},{"LN":"Kutics","FN":"Andrea","affil":"International Christian University"}],"year":2018,"journal":"2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","references":[2081056190,2041889512,2043711811,2111579636,1964549822,2161466446,2162236696,2321205984,2551337155,2322584079],"citationsCount":3,"citationContext":{"1964549822":["Recently, robots for mental care, rehabilitation support, and chatting friends are getting much attention [21][12][20]."],"2043711811":["Recently, robots for mental care, rehabilitation support, and chatting friends are getting much attention [21][12][20]."],"2111579636":["60% of them suffer from mental problems [17].","CRECA [17] adopts this assumption for building up and keeping mental trust and context-respectfully promoting their conversation and reflection towards self-awareness of solutions.","Our original/baseline counseling agent [17] called CRECA (Context Respectful Counseling Agent) engages a dialogue session like human counselors.","Therefore, CRECA [17] was proposed."],"2161466446":["Currently, only 30 % of IT projects can be finished with success [23]."],"2321205984":["An early attempt to build such an agent was ELIZA [22].","For conversational/ counseling agents, an early work was the dialog agent ELIZA [22] for client-centered therapy."]},"abstract":"A lot of IT personnel have psychological distress and counselors to help them are lack in number. Therefore, we proposed a counseling agent (CA) called CRECA (context respectful counseling agent), which listens to clients and promotes their reflection context respectfully namely in a context preserving way. This agent is now enhanced using a body language called \"unazuki\" in Japanese, a kind of nodding to greatly promote dialogue, often accompanying \"un-un\" (meaning \"exactly\") of Japanese onomatopoeia. This body language significantly helps represent empathy or entire approval. Our agent is enhanced with such dialog promotion nodding robot to continue the conversation naturally or context respectfully towards clients' further reflection. To realize it, the robot nods twice at each end of dialog sentence input by clients. Here, we introduce a robot that behaves human-like by an appropriate nodding behavior. The motivation for such a more human-like robot was the extension of application fields from IT workers' counselling to people, who suffer from more social problems such as financial debt, or anxiety of victory or defeat. For such applications, it is important that the agent behaves as much as possible human-like. Here, we present an enhanced experimental evaluation. The quantitative evaluation is based on the utterance amounts of a test group of individuals. These amount with and without the nodding feature are compared. Additionally, the robots with and without nodding are compared"},{"id":2612208817,"microsoftAcademicId":2612208817,"numberInSourceReferences":149,"doi":"10.1007/978-3-319-58750-9_36","title":"Neurophysiological Indices of Human Social Interactions Between Humans and Robots","authors":[{"LN":"Smith","FN":"S. J.","affil":"Research Advanced Brain Monitoring Inc."},{"LN":"Stone","FN":"B. T.","affil":"Research Advanced Brain Monitoring Inc."},{"LN":"Ranatunga","FN":"T.","affil":"Fellow Robots"},{"LN":"Nel","FN":"K.","affil":"Harvard University"},{"LN":"Ramsøy","FN":"Thomas Z.","affil":"Neurons Inc."},{"LN":"Berka","FN":"C.","affil":"Research Advanced Brain Monitoring Inc."}],"year":2017,"journal":"International Conference on Human-Computer Interaction","references":[2081895431,2098676269,2304108689,1986163111,1969675008,2142766726,2049377101,2154625376,2136511704,2122802803,1974768933,1906761640,2101384347,1985683154,2011111325,2155286729,2131180276,2159887240,2038546371,2155367101,2086979072,614565287,1804712606,2133060275,1612561000,13950139,2016833963,1971199318],"citationsCount":1,"abstract":"Technology continues to advance at exponential rates and we are exposed to a multitude of electronic interfaces in almost every aspect of our lives. In order to achieve seamless integration of both, human and technology, we must examine the objective and subjective responses to such interactions. The goal of this study was to examine neurophysiological responses to movement, communication, and usability with a robot assistant, in comparison to human assistant, in a real-world setting. OSHbot (robot assistants designed by Fellow Robots) were utilized as mobile store clerks to identify and locate merchandise in order to assist customers in finding items within a hardware store. By acquiring neurophysiological measures (electroencephalogram; EEG and electrocardiogram; ECG) of human perception and interaction with robots, we found evidence of Mirror Neuron System (MNS) elicitation and motor imagery processing, which is consistent with other studies examining human-robot interactions. Multiple analyses were conducted to assess differences between human-human interaction and human-robot interaction. Several EEG metrics were identified that were distinguishable based on interaction type; among these was the change observed across the Mu bandwidth (8–13 Hz). The variance in this EEG correlate has been related to empathetic state change. In order to explore differences in the interactions related to gender and age additional analyses were conducted to compare the effects of human-human interaction versus human-robot interaction with data stratified by gender and age. This analysis yielded significant differences across these categories between human-human interaction and human-robot interaction within EEG metrics. These preliminary data show promise for future research in the field of human-robot relations in contributing to the design and implementation of machines that not only deliver basic services but also create a social connection with humans."},{"id":2991493714,"microsoftAcademicId":2991493714,"numberInSourceReferences":81,"doi":"10.1016/J.TOURMAN.2019.104042","title":"Leveraging human-robot interaction in hospitality services: Incorporating the role of perceived value, empathy, and information sharing into visitors’ intentions to use social robots","authors":[{"LN":"Kervenoael","FN":"Ronan Jouan De","affil":"ESC Rennes School of Business"},{"LN":"Hasan","FN":"Rajibul","affil":"ESC Rennes School of Business"},{"LN":"Schwob","FN":"Alexandre","affil":"Department of Marketing, La Rochelle Business School, La Rochelle, France"},{"LN":"Goh","FN":"Edwin","affil":"Singapore Institute of Management"},{"LN":"Goh","FN":"Edwin","affil":"University of Birmingham"}],"year":2020,"journal":"Tourism Management","references":[2100379340,2106096361,2526781987,1791587663,1631150390,2168569455,3125976894,1689771227,2026368098,2098685541,2086642814,2119797181,2038125955,2150376909,2786141192,2100739170,2149967206,1683742771,2894528744,1990513740,2065771650,2502209121,3013114114,2019950119,2558287871,2110890492,2125706233,2617211984,2078789144,1978125974,2890440186,2756038645,2073762417,2591835862,2521535695,2055963727,2599446660,1540110233,2068533618,84957946,2130711665,2993535427,2609652260,2117799331,2570814811,2057643324,2898939387,2602156528,2790629405,2945049087,2123875695,2751092903,2913916101,2885538622,2071464654,2754550828,2747740261,2946552193,2260549038,2794509949,2605153561,2779762830,2911379028,2912861911,2805041655,2059543902,134370719,2115065131,2913494137,2488322715,2918214187,2514747792,587875510,2409462557,2022365405,2944475534,2568895109,2153900621,2468987637,2207103552,2905572601,1992117686,2010101930,2553027802,2911718048,2620219623,2886820228,3027272437,2663886357,2938991657,614131420,2793717674,2901254240,2346804704,2959323350,2789669838,2913338654,2794785839,2466575568,2891943141,1673409399,2534010549,2517781801,3170872038,2968188677,2491357008],"citationsCount":54,"abstract":"Abstract  Social robots have become pervasive in the tourism and hospitality service environments. The empirical understanding of the drivers of visitors' intentions to use robots in such services has become an urgent necessity for their sustainable deployment. Certainly, using social androids within hospitality services requires organisations' attentive commitment to value creation and fulfilling service quality expectations. In this paper, via structural equation modelling (SEM) and semi-structured interviews with managers, we conceptualise and empirically test visitors' intentions to use social robots in hospitality services. With data collected in Singapore's hospitality settings, we found visitors' intentions to use social robots stem from the effects of technology acceptance variables, service quality dimensions leading to perceived value, and two further dimensions from human robot interaction (HRI): empathy and information sharing. Analysis of these dimensions' importance provides a deeper understanding of novel opportunities managers may take advantage of to position social robot-delivered services in tourism and hospitality strategies."},{"id":2957783762,"microsoftAcademicId":2957783762,"numberInSourceReferences":35,"doi":"10.1007/978-3-030-22649-7_19","title":"High Sensitivity Layer Feature Analysis in Food Market","authors":[{"LN":"Matsuyama","FN":"Yoshio","affil":"Tokai University"},{"LN":"Asahi","FN":"Yumi","affil":"Tokai University"}],"year":2019,"journal":"Thematic Area on Human Interface and the Management of Information, HIMI 2019, held as part of the 21st International Conference on Human-Computer Interaction, HCI International 2019","references":[2095705004,6908809,2798659734,1582774210,2040870580,2570900015],"citationsCount":1,"abstract":"It is not uncommon to conduct test marketing for the purpose of market research when dropping new products to the market. However, if you actually drop it you will need a lot of money. This study, we pay attention to innovation theory. In Japan, the study reported a long sales period. However, the study didn’t report a short sales period. Therefore, we report of a short sales period, especially food. This study, we call “High Sensitivity Layer” the innovators and the early adopters in innovation theory in term of to be interested in the innovation of products, sensitive to trends and constantly collecting new information by themselves and to have greater influence on other consumers. We think that those that collect a lot of empathy in the “High Sensitivity Layer” are diffusive in the innovators and the early adopters, and grab the characteristics of highly sensitive consumers who gather many empathies. I think that it may be able to fulfill the purpose of test marketing by seeing the response of new products of food to this consumer. We prepare a generalized model with a deep learning model and report features of highly sensitive consumers, visually and numerically clearly, using decision tree analysis from that model. From the analysis results, attached more images, and the older, the better it got a report that empathizes with sensitive consumers. When conducting test marketing, it is predicted that high-sensitivity consumers will be able to obtain preferable results by targeting people with this characteristic. Also, it was found that gender and emotion are not related to the characteristics of the person who writes the report sympathized with the consumer. In the future, I would like to further accurate classification by text mining of posted characters and analysis of posted images."},{"id":2234863140,"microsoftAcademicId":2234863140,"numberInSourceReferences":134,"doi":"10.1007/978-3-319-25554-5_3","title":"An Empathic Robotic Tutor for School Classrooms: Considering Expectation and Satisfaction of Children as End-Users","authors":[{"LN":"Alves-Oliveira","FN":"Patrícia","affil":"University of Lisbon"},{"LN":"Ribeiro","FN":"Tiago","affil":"University of Lisbon"},{"LN":"Petisca","FN":"Sofia","affil":"University of Lisbon"},{"LN":"Tullio","FN":"Eugenio di","affil":"University of Lisbon"},{"LN":"Melo","FN":"Francisco S.","affil":"University of Lisbon"},{"LN":"Paiva","FN":"Ana","affil":"University of Lisbon"}],"year":2015,"journal":"International Conference on Social Robotics","references":[2110506823,2117012826,2137099741,2105025378,2131799829,2120115642,2052556483,2001156279,1846223488,1487719657,2204977270,1673801049,181791009,1969736828,2095838654,1907856289,606745021,2625580680],"citationsCount":13,"citationContext":{"181791009":["Robotic characters are becoming widespread as useful tools in assistive [12], entertainment [17] and tutoring applications [7]."],"606745021":["In fact, sci-fi culture ends up delivering information, most of the times unrealistic information, about a type of technology that is nowadays being created, bringing expectations over robots that are far from being achieved [3]."],"1487719657":["In the field of HRI, Bartneck and Forlizzi (2004), have defined a social robot as “an autonomous or semi-autonomous robot that interacts and communicates with humans by following the behavioral norms expected by the people with whom the robot is intended to interact” [4]."],"1673801049":["Nonetheless, when evaluating an innovative technology such as a robot, it is important to consider that user’s expectations can be coloured by others’ opinion, sci-fi culture, or can be tempered by the user experience [11]."],"1846223488":["One can have a pleasant experience with a product, but still feel dissatisfied if it is below expectation [13–15]."],"1969736828":["The perceptive capabilities of the system includes detecting and tracking the children’s head location, gaze direction, eyebrow movement (AU2 and AU4 [9]), and which child is currently speaking."],"2001156279":["The concepts of expectation and satisfaction have been studied in teacherstudent interactions in different educational settings, such as online education [10], e-Learning [16] and traditional classrooms [1].","The study of these concepts is crucial when developing a robotic tutor, as students’ expectations and satisfaction are important in education [1], being predictors of learning outcomes [10]."],"2052556483":["The concepts of expectation and satisfaction have been studied in teacherstudent interactions in different educational settings, such as online education [10], e-Learning [16] and traditional classrooms [1].","The study of these concepts is crucial when developing a robotic tutor, as students’ expectations and satisfaction are important in education [1], being predictors of learning outcomes [10].","the tutor’s knowledge is transferred to children, how feedback is used to support and facilitate learning, and the level of interaction [10]."],"2095838654":["The majority of robots’ appearance is presented to people through sci-fi culture, showing them robots that are far different from the real developed robots, thus stimulating peoples’ preconceived ideas (expectations) about the functions of robots [21]."],"2105025378":["The concepts of expectation and satisfaction have been studied in teacherstudent interactions in different educational settings, such as online education [10], e-Learning [16] and traditional classrooms [1]."],"2110506823":["So, expectations provide a baseline or reference level for users to form evaluative judgements about the experience with a product in which lower expectations usually influence satisfaction positively, if the previous expectations are confirmed by experience [5].","Thus, experience is what connects expectation and satisfaction [5]."],"2117012826":["749) [8]."],"2120115642":["To evaluate children’s expectations and satisfaction, a TSES was created inspired in the Bhattacherjee and Premkumar (2004) scale [6]."],"2131799829":["The study followed a methodology that merges an autonomous robot with a Wizard-of-Oz (WoZ) [19]."],"2137099741":["Robotic characters are becoming widespread as useful tools in assistive [12], entertainment [17] and tutoring applications [7]."],"2204977270":["Robotic characters are becoming widespread as useful tools in assistive [12], entertainment [17] and tutoring applications [7]."],"2625580680":["Also, Alves-Oliveira and collaborators (2014), have explored the expectations of children towards a robot that can interact with them in their own classroom, concluding that children’s initial expectations can help to identify the usefulness of robots [2]."]},"abstract":"Before interacting with a futuristic technology such as a robot, there is a lot of space for the creation of a whole set of expectations towards that interaction. Once that interaction happens, users can be left with a hand full of satisfaction, dissatisfaction, or even a mix of both. To study the possible role of experience as a mediator between expectation and satisfaction, we developed a scale for HRI that measures expectations and satisfaction of the users. Afterwards, we conducted a study with end-users interacting with a social robot. The robot is being developed to be an empathic robotic tutor to be used in real schools, with input from primary end-users (children). Children’s expectations and subsequent satisfaction after the interaction with the robotic tutor were analysed. The results can be fed back to the system developers on how well it is being designed for such a target population, and what factors regarding their expectation and satisfaction have shifted after the experience of interaction. By delivering on the children’s expectations, we aim to design a robotic tutor that provides enough satisfaction to sustain an enjoyable and natural interaction in the real educational environment."},{"id":2139346079,"microsoftAcademicId":2139346079,"numberInSourceReferences":65,"doi":"10.3389/FPSYG.2015.00576","title":"Is it the real deal? Perception of virtual characters versus humans: an affective cognitive neuroscience perspective","authors":[{"LN":"Borst","FN":"Aline W. ede","affil":"Maastricht University"},{"LN":"Gelder","FN":"Beatrice ede","affil":"Maastricht University"}],"year":2015,"journal":"Frontiers in Psychology","references":[2148764920,2016521818,2127958135,1526915519,2120019338,2042526117,1528045197,2126810579,2019924339,2099386544,2052685640,2172168442,1973787747,2130299716,2126883639,1974413721,2134727133,2132960888,2007997142,2121184995,2064012529,2005817178,2067609776,2149577107,1994259251,2122852424,2007462506,2063306756,2099637955,2035833057,2075274068,2122792792,2022613290,2060618445,2152476676,2098850353,2154930578,2008423926,1862873810,2139742181,2163586899,2104192945,2140189272,2047768646,2054626139,2146163948,2098950991,2116385068,2080595165,2122384877,2104584247,2096183091,207752236,2171361802,2050522639,2108952035,2113921716,2032460447,2153295898,2030918949,2143993797,1994240377,1487022053,1993599378,2104229238,2142882214,2124799425,1995620904,2168883005,2122102303,2057362418,1969900674,2106473212,2034061440,2062694288,2114376276,2120175440,2120227312,2096053194,2801740959,2143889467,2008437975,2159448314,2144601699,3114284325,2050098406,2142077860,2065107107,2036216401,1999115023,2142825656,2030407775,2178448134,2560989044,2115010770,2089950521,1901683522,2113429910,2111376597],"citationsCount":83,"citationContext":{"1487022053":["This phenomenon of perceiving withincategory differences as smaller than between category differences is typically defined as categorical perception and has been shown for different types of stimulus continua (Harnard, 1987; Liberman, 1996; de Gelder et al., 1997; Looser andWheatley, 2010)."],"1999115023":["For example, when offering a gift to a virtual character, participants react much in the sameway as they would with humans when this gift is either accepted or rejected (Zucker et al., 2011)."],"2019924339":["In the decades since its introduction, the notion of a mirror neuron system at the basis of motor perception has been expanded to explain complex behaviors such as imitation, emotion observation, intention, and empathy (Rizzolatti et al., 2001;Wicker et al., 2003; Iacoboni et al., 2005)."],"2089950521":["Slater et al. (2006) showed that during a 3D virtual version of this experiment participants reacted behaviorally and physiologically as if it were real, even though they knew it was not.","The integration across senses and the generation of affective states in the context of the predictive coding model has been discussed more recently in studies on emotion perception, self-representation and multi-sensory integration (Seth, 2013; Apps and Tsakiris, 2014; Ishida et al., 2014; Sel, 2014)."],"2108952035":[", the ZM for happy avatar faces (Weyers et al., 2006; Likowski et al., 2012) and the CS for sad and angry avatar faces (Likowski et al."]},"abstract":"Recent developments in neuroimaging research support the increased use of naturalistic stimulus material such as film, avatars, or androids. These stimuli allow for a better understanding of how the brain processes information in complex situations while maintaining experimental control. While avatars and androids are well suited to study human cognition, they should not be equated to human stimuli. For example, the uncanny valley hypothesis theorizes that artificial agents with high human-likeness may evoke feelings of eeriness in the human observer. Here we review if, when, and how the perception of human-like avatars and androids differs from the perception of humans and consider how this influences their utilization as stimulus material in social and affective neuroimaging studies. First, we discuss how the appearance of virtual characters affects perception. When stimuli are morphed across categories from non-human to human, the most ambiguous stimuli, rather than the most human-like stimuli, show prolonged classification times and increased eeriness. Human-like to human stimuli show a positive linear relationship with familiarity. Secondly, we show that expressions of emotions in human-like avatars can be perceived similarly to human emotions, with corresponding behavioral, physiological and neuronal activations, with exception of physical dissimilarities. Subsequently, we consider if and when one perceives differences in action representation by artificial agents versus humans. Motor resonance and predictive coding models may account for empirical findings, such as an interference effect on action for observed human-like, natural moving characters. However, the expansion of these models to explain more complex behavior, such as empathy, still needs to be investigated in more detail. Finally, we broaden our outlook to social interaction, where virtual reality stimuli can be utilized to imitate complex social situations."},{"id":2611254848,"microsoftAcademicId":2611254848,"numberInSourceReferences":108,"doi":"10.2196/RESPROT.6831","title":"Meeting the Needs of Mothers During the Postpartum Period: Using Co-Creation Workshops to Find Technological Solutions.","authors":[{"LN":"Slomian","FN":"Justine","affil":"University of Liège"},{"LN":"Emonts","FN":"Patrick","affil":"University of Liège"},{"LN":"Vigneron","FN":"Lara","affil":"Wallonia e-health Living Lab, The Labs, Liège, Belgium."},{"LN":"Acconcia","FN":"Alessandro","affil":"Wallonia e-health Living Lab, The Labs, Liège, Belgium."},{"LN":"Reginster","FN":"Jean-Yves","affil":"University of Liège"},{"LN":"Oumourgh","FN":"Mina","affil":"University of Liège"},{"LN":"Bruyère","FN":"Olivier","affil":"University of Liège"}],"year":2017,"journal":"JMIR Research Protocols","references":[1971631504,2112841227,1968539681,2252728976,2135342506,2048314731,2038461958,2083253801,2087956619,1968234907,2078244849,1998037365,2107988486,2161092861,1990604437,1977333740,2067610975,2163854628,1965205994,1970249355,2536176317,1983082216,2108756872,1965811485,2118175139,1995817479,2059779920,2053276940,2114050236,2013758372,2133339382,2076232124],"citationsCount":12,"abstract":"Background: The postnatal period is associated with many new needs for mothers. Objective: The aim of this study was to find technological solutions that meet the needs of mothers during the year following childbirth. Methods: Two co-creation workshops were undertaken with parents and professionals. The aim of the first workshop was to create a list of all the criteria the proposed solution would have to address to meet the needs of mothers after childbirth. The aim of the second workshop was to create solutions in response to the criteria selected during the first workshop. Results: Parents and health professionals want solutions that include empathy (ie, to help fight against the feelings of abnormality and loneliness), that help mothers in daily life, that are personalized and adapted to different situations, that are educational, and that assures some continuity in their contact with health professionals. In practice, we found that parents and professionals think the solution should be accessible to everyone and available at all times. To address these criteria, technology experts proposed different solutions, such as a forum dedicated to the postpartum period that is supervised by professionals, a centralized website, a system of videoconferencing, an online exchange group, a “gift voucher” system, a virtual reality app, or a companion robot. Conclusions: The human component seems to be very important during the postnatal period. Nevertheless, technology could be a great ally in helping mothers during the postpartum period. Technology can help reliably inform parents and may also give them the right tools to find supportive people. However, these technologies should be tested in clinical trials. [JMIR Res Protoc 2017;6(5):e76]"},{"id":3009898121,"microsoftAcademicId":3009898121,"numberInSourceReferences":10,"doi":"10.1145/3319502.3374781","title":"Prompting Prosocial Human Interventions in Response to Robot Mistreatment","authors":[{"LN":"Connolly","FN":"Joe","affil":"Yale University"},{"LN":"Mocz","FN":"Viola","affil":"Yale University"},{"LN":"Salomons","FN":"Nicole","affil":"Yale University"},{"LN":"Valdez","FN":"Joseph","affil":"Yale University"},{"LN":"Tsoi","FN":"Nathan","affil":"Yale University"},{"LN":"Scassellati","FN":"Brian","affil":"Yale University"},{"LN":"Vázquez","FN":"Marynel","affil":"Yale University"}],"year":2020,"journal":"Proceedings of the 2020 ACM/IEEE International Conference on Human-Robot Interaction","references":[2162090451,2125188427,2807126412,1588539311,3100470991,2175873029,2094203099,2407300725,1970239077,2156763515,2505308623,2916823248,2612064344,2067790145,2963427815,2792480533,2073979665,2525736136,2793727073,2279735562,1619839657,2147669981,2792339192,2793710373,2130034757],"citationsCount":8,"abstract":"Inspired by the benefits of human prosocial behavior, we explore whether prosocial behavior can be extended to a Human-Robot Interaction (HRI) context. More specifically, we study whether robots can induce prosocial behavior in humans through a 1x2 between-subjects user study (N=30) in which a confederate abused a robot. Through this study, we investigated whether the emotional reactions of a group of bystander robots could motivate a human to intervene in response to robot abuse. Our results show that participants were more likely to prosocially intervene when the bystander robots expressed sadness in response to the abuse as opposed to when they ignored these events, despite participants reporting similar perception of robot mistreatment and levels of empathy for the abused robot. Our findings demonstrate possible effects of group social influence through emotional cues by robots in human-robot interaction. They reveal a need for further research regarding human prosocial behavior within HRI."},{"id":2958665596,"microsoftAcademicId":2958665596,"numberInSourceReferences":113,"doi":"10.1038/S41598-019-46528-7","title":"Embodiment into a robot increases its acceptability","authors":[{"LN":"Ventre-Dominey","FN":"J.","affil":"French Institute of Health and Medical Research"},{"LN":"Gibert","FN":"G.","affil":"Claude Bernard University Lyon 1"},{"LN":"Gibert","FN":"G.","affil":"French Institute of Health and Medical Research"},{"LN":"Bosse-Platiere","FN":"M.","affil":"Claude Bernard University Lyon 1"},{"LN":"Bosse-Platiere","FN":"M.","affil":"French Institute of Health and Medical Research"},{"LN":"Farnè","FN":"A.","affil":"Claude Bernard University Lyon 1"},{"LN":"Dominey","FN":"P. F.","affil":"Claude Bernard University Lyon 1"},{"LN":"Dominey","FN":"P. F.","affil":"French Institute of Health and Medical Research"},{"LN":"Pavani","FN":"F.","affil":"Integrative Multisensory Perception Action & Cognition Team (ImpAct), Lyon Neuroscience Research Center (CRNL), 69003, Lyon, France."}],"year":2019,"journal":"Scientific Reports","references":[2087484885,1950864686,2998827830,2163290657,2049166294,2170100837,2171627550,2161222115,2044032680,2037141354,1973787747,2014833927,2103551585,2017142040,2148203610,1996077504,2008778470,2089372063,2139426300,1969260350,2038594232,2570888555,1994354386,1963852194,2082302771,2131979409,1580236838,2009991525,2147237207,2138856282,1983247339,1966364624,2014518691,2203553615,2164576237,2523759546,2805464163,1986459945,2522732741,3114284325,3137610584,2038948864,2003301935,2162431167],"citationsCount":9,"abstract":"Recent studies have shown how embodiment induced by multisensory bodily interactions between individuals can positively change social attitudes (closeness, empathy, racial biases). Here we use a simple neuroscience-inspired procedure to beam our human subjects into one of two distinct robots and demonstrate how this can readily increase acceptability and social closeness to that robot. Participants wore a Head Mounted Display tracking their head movements and displaying the 3D visual scene taken from the eyes of a robot which was positioned in front of a mirror and piloted by the subjects’ head movements. As a result, participants saw themselves as a robot. When participant’ and robot’s head movements were correlated, participants felt that they were incorporated into the robot with a sense of agency. Critically, the robot they embodied was judged more likeable and socially closer. Remarkably, we found that the beaming experience with correlated head movements and corresponding sensation of embodiment and social proximity, was independent of robots’ humanoid’s appearance. These findings not only reveal the ease of body-swapping, via visual-motor synchrony, into robots that do not share any clear human resemblance, but they may also pave a new way to make our future robotic helpers socially acceptable."},{"id":2748604175,"microsoftAcademicId":2748604175,"numberInSourceReferences":63,"doi":"10.3389/FPSYG.2017.01393","title":"You Look Human, But Act Like a Machine: Agent Appearance and Behavior Modulate Different Aspects of Human-Robot Interaction","authors":[{"LN":"Abubshait","FN":"Abdulaziz","affil":"George Mason University"},{"LN":"Wiese","FN":"Eva","affil":"George Mason University"}],"year":2017,"journal":"Frontiers in Psychology","references":[2099019320,2136162329,2131227926,2095953405,2150839178,2077324971,2013098353,2117155762,2118586055,2121184995,2161518694,2129928382,2042585112,2122714938,2023238480,2094482505,2050944559,2121435779,2041060465,2063688338,2024010245,1983423649,2123952992,2543205333,2098850353,2098089739,1971224247,2156843313,2160145433,2074869430,2029591514,2101311059,2224792037,2135469644,2095719050,2111981270,2059557633,2171264451,1970278724,2124342905,2145114016,2138415765,2039922776,2082016077,1966027968,2064566830,2003329389,2165315410,2113170322,1990436182,2010165379,2090993493,2033975961,2034061440,2034816455,2160638215,145298465,2089152785,2514907158,2134831554,2087358453,2227489293,1981966235,2025462240,1993581448,1973394280,2301220525,1996629760,1754692440,2407579519,1992137693,1750687029,1491762360,2069753384,2027487134,2115847032,2064808642,2404051208,1965556078,1968851839,2146783471,2398299882,2759928903,2038784784,2758275925,2597371088,1968340857],"citationsCount":53,"abstract":"Gaze following occurs automatically in social interactions, but the degree to which gaze is followed depends on whether an agent is perceived to have a mind, making its behavior socially more relevant for the interaction. Mind perception also modulates the attitudes we have towards others, and deter-mines the degree of empathy, prosociality and morality invested in social interactions. Seeing mind in others is not exclusive to human agents, but mind can also be ascribed to nonhuman agents like robots, as long as their appearance and/or behavior allows them to be perceived as intentional beings. Previous studies have shown that human appearance and reliable behavior induce mind perception to robot agents, and positively affect attitudes and performance in human-robot interaction. What has not been investigated so far is whether different triggers of mind perception have an independent or interactive effect on attitudes and performance in human-robot interaction. We examine this question by manipulating agent appearance (human vs. robot) and behavior (reliable vs. random) within the same paradigm and examine how congruent (human/reliable vs. robot/random) versus incongruent (human/random vs. robot/reliable) combinations of these triggers affect performance (i.e., gaze fol-lowing) and attitudes (i.e., agent ratings) in human-robot interaction. The results show that both ap-pearance and behavior affect human-robot interaction but that the two triggers seem to operate in iso-lation, with appearance more strongly impacting attitudes, and behavior more strongly affecting per-formance. The implications of these findings for human-robot interaction are discussed."},{"id":2912159753,"microsoftAcademicId":2912159753,"numberInSourceReferences":160,"doi":"10.1007/S12369-019-00524-Z","title":"Multimodal Integration of Emotional Signals from Voice, Body, and Context: Effects of (In)Congruence on Emotion Recognition and Attitudes Towards Robots","authors":[{"LN":"Tsiourti","FN":"Christiana","affil":"Vienna University of Technology"},{"LN":"Weiss","FN":"Astrid","affil":"Vienna University of Technology"},{"LN":"Wac","FN":"Katarzyna","affil":"University of Copenhagen"},{"LN":"Vincze","FN":"Markus","affil":"Vienna University of Technology"}],"year":2019,"journal":"International Journal of Social Robotics","references":[2099019320,2032568497,2029334490,2159017231,1966797434,1989104072,2031371003,2149628368,2024221294,1562811233,2159190230,2096284813,2110585328,2168031754,2103153725,2150781787,2095436958,1997060370,2158180452,2064304167,2147449317,2013040441,2040221341,2075274068,2082735203,2115839658,2160287712,2073414813,1882423120,2055973627,2162671057,2413750868,1501380514,1968015255,2115381388,2190260761,2090841896,2013355346,2021631361,1486652013,2110102748,2004382573,2084388911,2110911841,2171068375,2001429584,1969350109,1705970262,2106473212,2055336313,2122376170,2913916027,2151288549,3821698,2023397245,2799638175,168347796,1608468640,2766682046,1974580815,2623483087,2213398625,1966236843,2210204125,20018439,1966779602,2098629945,2794862476,1604371559],"citationsCount":24,"abstract":"Humanoid social robots have an increasingly prominent place in today’s world. Their acceptance in social and emotional human–robot interaction (HRI) scenarios depends on their ability to convey well recognized and believable emotional expressions to their human users. In this article, we incorporate recent findings from psychology, neuroscience, human–computer interaction, and HRI, to examine how people recognize and respond to emotions displayed by the body and voice of humanoid robots, with a particular emphasis on the effects of incongruence. In a social HRI laboratory experiment, we investigated contextual incongruence (i.e., the conflict situation where a robot’s reaction is incongrous with the socio-emotional context of the interaction) and cross-modal incongruence (i.e., the conflict situation where an observer receives incongruous emotional information across the auditory (vocal prosody) and visual (whole-body expressions) modalities). Results showed that both contextual incongruence and cross-modal incongruence confused observers and decreased the likelihood that they accurately recognized the emotional expressions of the robot. This, in turn, gives the impression that the robot is unintelligent or unable to express “empathic” behaviour and leads to profoundly harmful effects on likability and believability. Our findings reinforce the need of proper design of emotional expressions for robots that use several channels to communicate their emotional states in a clear and effective way. We offer recommendations regarding design choices and discuss future research areas in the direction of multimodal HRI."},{"id":2399149746,"microsoftAcademicId":2399149746,"numberInSourceReferences":52,"doi":"10.1007/978-3-319-08338-4_68","title":"An Architecture for Telenoid Robot as Empathic Conversational Android Companion for Elderly People","authors":[{"LN":"Sorbello","FN":"Rosario","affil":"University of Palermo"},{"LN":"Chella","FN":"Antonio","affil":"University of Palermo"},{"LN":"Giardina","FN":"Marcello Emanuele","affil":"University of Palermo"},{"LN":"Nishio","FN":"Shuichi","affil":"ATR Intelligent Robotics and Communication Laboratory"},{"LN":"Ishiguro","FN":"Hiroshi","affil":"Osaka University"}],"year":2016,"journal":"Advances in intelligent systems and computing","references":[2110475150,2044663075,1980083892,1983423649,2111673230,13388985,2114550714,2035128922,2033995044,1562711890,2156257989,2540373225,2070304368,1549951060,1619839657,2126764680,162906327,34509302,1783746456,2408711830,2394784023,1908355812,1968064577,101025192,2055929422],"citationsCount":6,"abstract":"In Human-Humanoid Interaction (HHI), empathy is the crucial key in order to overcome the current limitations of social robots. In facts, a principal defining characteristic of human social behaviour is empathy. The present paper presents a robotic architecture for an android robot as a basis for natural empathic human-android interaction. We start from the hypothesis that the robots, in order to become personal companions need to know how to empathic interact with human beings. To validate our research, we have used the proposed system with the minimalistic humanoid robot Telenoid. We have conducted human-robot interactions test with elderly people with no prior interaction experience with robot. During the experiment, elderly persons engaged a stimulated conversation with the humanoid robot. Our goal is to overcome the state of loneliness of elderly people using this minimalistic humanoid robot capable to exhibit a dialogue similar to what usually happens in real life between human beings. The experimental results have shown a humanoid robotic system capable to exhibit a natural and empathic interaction and conversation with a human user."},{"id":2982009894,"microsoftAcademicId":2982009894,"numberInSourceReferences":142,"doi":"10.1145/3341215.3356982","title":"Ballbit Adventure: A Physical Game for a Collaborative Racing","authors":[{"LN":"Kuang","FN":"Quincy","affil":"Rhode Island School of Design"},{"LN":"Zhang","FN":"Jiaxin","affil":"Rhode Island School of Design"},{"LN":"Druga","FN":"Stefania","affil":"Massachusetts Institute of Technology"}],"year":2019,"journal":"Extended Abstracts of the Annual Symposium on Computer-Human Interaction in Play Companion Extended Abstracts","references":[2726846661,2885844456,1654155932,2064054441,2244563840,2902626067,2132503829],"citationsCount":0,"abstract":"Playtime accounts for one of the most critical learning periods for children, as they learn how to interact and socialize with their playmates. In this paper, we present a new kind of cooperation-based physical game called Ballbit Adventure. Our game provides a collaborative environment for children to communicate, cooperate, and empathize through solving challenges in an interactive maze. Each player must drive a robotic ball and work together to complete different tasks that would ultimately lead them to the finish line. Through the format of a physical racing game, Ballbit Adventure hopes to show the value of face-to-face play experience to counterbalance the disconnected online interactions that children have with video games."},{"id":2887606570,"microsoftAcademicId":2887606570,"numberInSourceReferences":118,"doi":"10.1007/978-3-319-92108-2_11","title":"Towards Metrics of Evaluation of Pepper Robot as a Social Companion for the Elderly","authors":[{"LN":"Bechade","FN":"Lucile","affil":"University of Paris-Sud"},{"LN":"Dubuisson-Duplessis","FN":"Guillaume","affil":"University of Paris-Sud"},{"LN":"Pittaro","FN":"Gabrielle","affil":"University of Paris-Sud"},{"LN":"Garcia","FN":"Mélanie","affil":"University of Paris-Sud"},{"LN":"Devillers","FN":"Laurence","affil":"University of Paris-Sud"},{"LN":"Devillers","FN":"Laurence","affil":"Paris-Sorbonne University"}],"year":2019,"journal":"IWSDS","references":[1841352775,2152193807,2470059143,1989257272,2045053352,2788204336,2143310837,2528661252,2059817661,2586648878,2063652171,2399624915,2121653658],"citationsCount":11,"abstract":"For the design of socially acceptable robots, field studies in Human-Robot Interaction are necessary. Constructing dialogue benchmarks can have a meaning only if researchers take into account the evaluation of robot, human, and their interaction. This paper describes a study aiming at finding an objective evaluation procedure of the dialogue with a social robot. The goal is to build an empathic robot (JOKER project) and it focuses on elderly people, the end-users expected by ROMEO2 project. The authors carried out three experimental sessions. The first time, the robot was NAO, and it was with a Wizard of Oz (emotions were entered manually by experimenters as inputs to the program). The other times, the robot was Pepper, and it was totally autonomous (automatic detection of emotions and decision according to). Each interaction involved various scenarios dealing with emotion recognition, humor, negotiation and cultural quiz. The paper details the system functioning, the scenarios and the evaluation of the experiments."},{"id":2735162671,"microsoftAcademicId":2735162671,"numberInSourceReferences":123,"doi":"10.1109/IJCNN.2017.7966040","title":"Stacked deep convolutional auto-encoders for emotion recognition from facial expressions","authors":[{"LN":"Ruiz-Garcia","FN":"Ariel","affil":"Coventry University"},{"LN":"Elshaw","FN":"Mark","affil":"Coventry University"},{"LN":"Altahhan","FN":"Abdulrahman","affil":"Coventry University"},{"LN":"Palade","FN":"Vasile","affil":"Coventry University"}],"year":2017,"journal":"2017 International Joint Conference on Neural Networks (IJCNN)","references":[2163605009,1677182931,2136922672,2155541015,2163922914,1533861849,2138857742,1921523184,2136655611,2218318129,2045565604,2188183693,2130972944,2178237821,2287334441,2081564928,1631012836,2108358156,2009659028,2004355430,2510427384,2328054901,2086818172,2329469793],"citationsCount":23,"abstract":"Emotion recognition is critical for everyday living and is essential for meaningful interaction. If we are to progress towards human and machine interaction that is engaging the human user, the machine should be able to recognize the emotional state of the user. Deep Convolutional Neural Networks (CNN) have proven to be efficient in emotion recognition problems. The good degree of performance achieved by these classifiers can be attributed to their ability to self-learn a down-sampled feature vector that retains spatial information through filter kernels in convolutional layers. Given the view that random initialization of weights can lead to convergence to non-optimal local minima, in this paper we explore the impact of training the initial weights in an unsupervised manner. We study the effect of pre-training a Deep CNN as a Stacked Convolutional Auto-Encoder (SCAE) in a greedy layer-wise unsupervised fashion for emotion recognition using facial expression images. When trained with randomly initialized weights, our CNN emotion recognition model achieves a performance rate of 91.16% on the Karolinska Directed Emotional Faces (KDEF) dataset. In contrast, when each layer of the model, including the hidden layer, is pre-trained as an Auto-Encoder, the performance increases to 92.52%. Pre-training our CNN as a SCAE also reduces training time marginally. The emotion recognition model developed in this work will form the basis of a real-time empathic robot system."},{"id":3161372380,"microsoftAcademicId":3161372380,"numberInSourceReferences":141,"doi":"10.1145/3411764.3445495","title":"Drone in Love: Emotional Perception of Facial Expressions on Flying Robots","authors":[{"LN":"Herdel","FN":"Viviane","affil":"Ben-Gurion University of the Negev"},{"LN":"Kuzminykh","FN":"Anastasia","affil":"University of Toronto"},{"LN":"Hildebrandt","FN":"Andrea","affil":"University of Oldenburg"},{"LN":"Cauchard","FN":"Jessica R.","affil":"Ben-Gurion University of the Negev"}],"year":2021,"journal":"Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems","references":[2084235337,1588539311,2167557160,1607675442,1989104072,1971687303,2154732345,1993584577,2028238586,2044663075,2021641437,2085436006,1766531844,2160434794,2763083925,1983423649,1974294498,2000982104,2152536828,2803486101,2126398976,2165705569,2114399139,2792617863,2098089739,602907521,1989426632,2990239129,2050731184,1971277746,2974746094,2791904197,2766328169,2172154895,2116411037,2519895671,2003238582,2940509747,2054799742,2097308366,2004382573,2765438554,2765622179,2942227665,2925361472,2773543832,131467608,2006609774,2151288549,2774250617,2010252136,3028775309,2312946905,2156780326,2126558794,2130621102,3010242595,2119679233,2952694408,2135156782,2972469347,2167516569,2161576359,2910648020,3038648473],"citationsCount":0,"abstract":"Drones are rapidly populating human spaces, yet little is known about how these flying robots are perceived and understood by humans. Recent works suggested that their acceptance is predicated upon their sociability. This paper explores the use of facial expressions to represent emotions on social drones. We leveraged design practices from ground robotics and created a set of rendered robotic faces that convey basic emotions. We evaluated individuals’ response to these emotional facial expressions on drones in two empirical studies (N = 98, N = 98). Our results demonstrate that individuals accurately recognize five drone emotional expressions, as well as make sense of intensities within emotion categories. We describe how participants were emotionally affected by the drone, showed empathy towards it, and created narratives to interpret its emotions. As a consequence, we formulate design recommendations for social drones and discuss methodological insights on the use of static versus dynamic stimuli in affective robotics studies."},{"id":2524160855,"microsoftAcademicId":2524160855,"numberInSourceReferences":80,"doi":"10.1007/S11042-016-3797-0","title":"Simulating empathic behavior in a social assistive robot","authors":[{"LN":"Carolis","FN":"Berardina De","affil":"Dipartimento di Informatica, Univerisita' di Bari, Aldo Moro, Bari, Italy 70126#TAB#"},{"LN":"Ferilli","FN":"Stefano","affil":"Dipartimento di Informatica, Univerisita' di Bari, Aldo Moro, Bari, Italy 70126#TAB#"},{"LN":"Palestra","FN":"Giuseppe","affil":"Dipartimento di Informatica, Univerisita' di Bari, Aldo Moro, Bari, Italy 70126#TAB#"}],"year":2017,"journal":"Multimedia Tools and Applications","references":[3097096317,2339343773,1755360231,1841352775,2102512156,1977137834,1645937837,2149628368,2154543439,2111040806,1967769980,2911336608,2023597264,1993925701,1936981920,2109243771,3124386704,2080593835,85091029,587017703,2005210865,2124937956,2133247294,1700127543,2126401725,2102548748,2110102748,2521961213,2136930480,2082135072,2081284900,1722056324,1597748886,1778960568,1968425037,2735835184,64064869,2129760663,118059791,88910451,1866371092],"citationsCount":13,"abstract":"When used as an interface in the context of Ambient Assisted Living (AAL), a social robot should not just provide a task-oriented support. It should also try to establish a social empathic relation with the user. To this aim, it is crucial to endow the robot with the capability of recognizing the user's affective state and reason on it for triggering the most appropriate communicative behavior. In this paper we describe how such an affective reasoning has been implemented in the NAO robot for simulating empathic behaviors in the context of AAL. In particular, the robot is able to recognize the emotion of the user by analyzing communicative signals extracted from speech and facial expressions. The recognized emotion allows triggering the robot's affective state and, consequently, the most appropriate empathic behavior. The robot's empathic behaviors have been evaluated both by experts in communication and through a user study aimed at assessing the perception and interpretation of empathy by elderly users. Results are quite satisfactory and encourage us to further extend the social and affective capabilities of the robot."},{"id":2766145286,"microsoftAcademicId":2766145286,"numberInSourceReferences":76,"doi":"10.3390/FI9040075","title":"Creation and Staging of Android Theatre “Sayonara”towards Developing Highly Human-Like Robots","authors":[{"LN":"Chikaraishi","FN":"Takenobu"},{"LN":"Yoshikawa","FN":"Yuichiro"},{"LN":"Ogawa","FN":"Kohei"},{"LN":"Hirata","FN":"Oriza"},{"LN":"Ishiguro","FN":"Hiroshi"}],"year":2017,"journal":"Future Internet","references":[2142992961,2032568497,2161794541,2066556081,2020924353,2541312590,2529880501,2465534043,2026124789,3140655598,2028811593,3114284325,2053282566,1973394280,2161385114,2046857734,2074130341,3050976981,2517180834,3059942586,2468852952],"citationsCount":8,"abstract":"Even after long-term exposures, androids with a strikingly human-like appearance evoke unnatural feelings. The behavior that would induce human-like feelings after long exposures is difficult to determine, and it often depends on the cultural background of the observers. Therefore, in this study, we generate an acting performance system for the android, in which an android and a human interact in a stage play in the real world. We adopt the theatrical theory called Contemporary Colloquial Theatre Theory to give the android natural behaviors so that audiences can comfortably observe it even after long-minute exposure. A stage play is created and shown in various locations, and the audiences are requested to report their impressions of the stage and their cultural and psychological backgrounds in a self-evaluating questionnaire. Overall analysis indicates that the audience had positive feelings, in terms of attractiveness, towards the android on the stage even after 20 min of exposure. The singularly high acceptance of the android by Japanese audiences seems to be correlated with a high animism tendency, rather than to empathy. We also discuss how the stage play approach is limited and could be extended to contribute to realization of human–robot interaction in the real world."},{"id":2561031991,"microsoftAcademicId":2561031991,"numberInSourceReferences":179,"doi":"10.1515/PJBR-2016-0005","title":"Appearance of a robot affects the impact of its behaviour on perceived trustworthiness and empathy","authors":[{"LN":"Złotowski","FN":"Jakub"},{"LN":"Sumioka","FN":"Hidenobu"},{"LN":"Nishio","FN":"Shuichi"},{"LN":"Glas","FN":"Dylan F."},{"LN":"Bartneck","FN":"Christoph"},{"LN":"Ishiguro","FN":"Hiroshi"}],"year":2016,"journal":"Paladyn: Journal of Behavioral Robotics","references":[2135907989,2101353123,2165332998,2002024051,2129491278,2121533105,2010943014,1985855315,2152536828,2118462055,13388985,2115179573,3125732352,2044664635,2035128922,2019161055,2110102748,2112880115,2111715140,2001429584,2017802568,1619839657,2160403235,2072566914,2326767346,2091241041,1998155667,2121276127],"citationsCount":18},{"id":2784499415,"microsoftAcademicId":2784499415,"numberInSourceReferences":166,"doi":"10.2196/JMIR.7737","title":"Experiences of a Motivational Interview Delivered by a Robot: Qualitative Study","authors":[{"LN":"Silva","FN":"Joana Galvão Gomes da","affil":"University of Plymouth"},{"LN":"Kavanagh","FN":"David J","affil":"Queensland University of Technology"},{"LN":"Belpaeme","FN":"Tony","affil":"University of Plymouth"},{"LN":"Taylor","FN":"Lloyd","affil":"University of Plymouth"},{"LN":"Beeson","FN":"Konna","affil":"University of Plymouth"},{"LN":"Andrade","FN":"Jackie","affil":"University of Plymouth"}],"year":2018,"journal":"Journal of Medical Internet Research","references":[2105335341,2157156054,2107911338,2149129894,2154543439,1997110783,2140527237,2019223138,2068086678,2041192962,2616867613,2171579061,2053112889,2286973183,2739665490,1099455392,2147122465,2127857679,2508145593,2016387931,2568303671,2070865518,2091881664,2147792401,1967756332,2552492149,2103174739,2134804162,2112512141],"citationsCount":17,"abstract":"Background: Motivational interviewing is an effective intervention for supporting behavior change but traditionally depends on face-to-face dialogue with a human counselor. This study addressed a key challenge for the goal of developing social robotic motivational interviewers: creating an interview protocol, within the constraints of current artificial intelligence, which participants will find engaging and helpful. Objective: The aim of this study was to explore participants’ qualitative experiences of a motivational interview delivered by a social robot, including their evaluation of usability of the robot during the interaction and its impact on their motivation. Methods: NAO robots are humanoid, child-sized social robots. We programmed a NAO robot with Choregraphe software to deliver a scripted motivational interview focused on increasing physical activity. The interview was designed to be comprehensible even without an empathetic response from the robot. Robot breathing and face-tracking functions were used to give an impression of attentiveness. A total of 20 participants took part in the robot-delivered motivational interview and evaluated it after 1 week by responding to a series of written open-ended questions. Each participant was left alone to speak aloud with the robot, advancing through a series of questions by tapping the robot’s head sensor. Evaluations were content-analyzed utilizing Boyatzis’ steps: (1) sampling and design, (2) developing themes and codes, and (3) validating and applying the codes. Results: Themes focused on interaction with the robot, motivation, change in physical activity, and overall evaluation of the intervention. Participants found the instructions clear and the navigation easy to use. Most enjoyed the interaction but also found it was restricted by the lack of individualized response from the robot. Many positively appraised the nonjudgmental aspect of the interview and how it gave space to articulate their motivation for change. Some participants felt that the intervention increased their physical activity levels. Conclusions: Social robots can achieve a fundamental objective of motivational interviewing, encouraging participants to articulate their goals and dilemmas aloud. Because they are perceived as nonjudgmental, robots may have advantages over more humanoid avatars for delivering virtual support for behavioral change. [J Med Internet Res 2018;20(5):e116]"},{"id":2962907857,"microsoftAcademicId":2962907857,"numberInSourceReferences":165,"doi":"10.1142/S0219843618500068","title":"Emotional Storytelling Using Virtual and Robotic Agents","authors":[{"LN":"Costa","FN":"Sandra","affil":"University of Minho"},{"LN":"Brunete","FN":"Alberto","affil":"Technical University of Madrid"},{"LN":"Bae","FN":"Byung-Chull","affil":"Hongik University"},{"LN":"Mavridis","FN":"Nikolaos","affil":"Interactive Robots and Media Lab (IRML), Athens, Greece"}],"year":2018,"journal":"International Journal of Humanoid Robotics","references":[2159017231,2167557160,2096476371,2149628368,2165683312,2062865797,2131123711,2150758961,2124399597,1964549822,2089854755,1982517393,2008033734,2027814249,1986930799,215088529,2144127639,1491497103,2106019371,2080049466,1971557336,153615506,1550497380,2500896654,1984867139,2078788134,1971085676],"citationsCount":16,"abstract":"In order to create effective storytelling agents three fundamental questions must be answered: first, is a physically embodied agent preferable to a virtual agent or a voice-only narration? Second,..."},{"id":2912475534,"microsoftAcademicId":2912475534,"numberInSourceReferences":162,"doi":"10.1109/LRA.2019.2896933","title":"A Wearable Skin-Stretching Tactile Interface for Human–Robot and Human–Human Communication","authors":[{"LN":"Haynes","FN":"Alice","affil":"University of Bristol"},{"LN":"Simons","FN":"Melanie F.","affil":"University of Bristol"},{"LN":"Helps","FN":"Tim","affil":"University of Bristol"},{"LN":"Nakamura","FN":"Yuichi","affil":"Kyoto University"},{"LN":"Rossiter","FN":"Jonathan","affil":"University of Bristol"}],"year":2019,"journal":"IEEE Robotics and Automation Letters","references":[2165857685,2612110125,2094938076,2129353167,1996545459,2036617830,201515460,2783748062,2000313766,2110176254,2765184744,2120547502,2345576436,2123712231,2322377956,2166843857,1996210195,2146095903,2024641146,2163898766,967442843,2019371584,2809871420,2546940773,2658158223,2050785069,2807384662],"citationsCount":7,"citationContext":{"201515460":["known [13], [14] and there is evidence to suggest that mediated social touch is processed in a similar way to real physical contact [15]."],"1996210195":["Different methods have been devised to stretch the skin, often using servomotors controlling end-effectors that either move, rotate or rock [16], [20], [21].","Over the past decade, a number of devices have utilised skin stretching methods as alternatives to vibratory stimulation, delivering navigation feedback [16], guided movement [17], [18] and robotic manipulator control [19], [20].","previous devices [16], and the difference in how well participants could detect the two stimuli was negligible."],"1996545459":["Our skin is a multimodal sensor capable of detecting touch, stretch, temperature, texture, vibration, pressure and pain [1], [2]."],"2000313766":["In these cases the wearer should not be distracted and these subtle notifications could reduce risk (studies have shown that even handsfree mobile phone use markedly increased mental workload when driving [28])."],"2019371584":["Haptic wearable devices come in awide range of shapes, sizes and interaction modalities, including insoles that aid navigation for the visually impaired [7], kinesthetic haptic feedback to guide a user’s hand [8] and active pin arrays for laterotactile"],"2024641146":["Despite efforts to ‘anthropomorphise’ vibrotactile stimulation [12], these characteristics make them"],"2036617830":["Our skin is a multimodal sensor capable of detecting touch, stretch, temperature, texture, vibration, pressure and pain [1], [2].","The majority are vibrotactile, where sensory information is relayed to the body by the use of vibrations with differing frequency, amplitude, duration, and/or waveform [2], [10], [11]."],"2050785069":["stimulation [9]."],"2094938076":["The majority are vibrotactile, where sensory information is relayed to the body by the use of vibrations with differing frequency, amplitude, duration, and/or waveform [2], [10], [11]."],"2110176254":["Skin stretch has been shown to be superior to vibratory feedback to convey proprioception information [22] and all of these devices demonstrate the effectiveness of non-vibrational tactile sensations as an information channel."],"2120547502":["Over the past decade, a number of devices have utilised skin stretching methods as alternatives to vibratory stimulation, delivering navigation feedback [16], guided movement [17], [18] and robotic manipulator control [19], [20]."],"2123712231":["The majority are vibrotactile, where sensory information is relayed to the body by the use of vibrations with differing frequency, amplitude, duration, and/or waveform [2], [10], [11]."],"2129353167":["known [13], [14] and there is evidence to suggest that mediated social touch is processed in a similar way to real physical contact [15]."],"2146095903":["SMAs are lightweight, flexible and actuate silently, beneficial properties for use in an unobtrusive wearable device as exploited in tactile pin-arrays [23]–[25] and fingertipmounted skin stretching devices [26]."],"2163898766":["known [13], [14] and there is evidence to suggest that mediated social touch is processed in a similar way to real physical contact [15]."],"2165857685":["These subjective parameters are defined in the circumplex model of affect [27] commonly used in psychology"],"2166843857":["SMAs are lightweight, flexible and actuate silently, beneficial properties for use in an unobtrusive wearable device as exploited in tactile pin-arrays [23]–[25] and fingertipmounted skin stretching devices [26]."],"2345576436":["Different methods have been devised to stretch the skin, often using servomotors controlling end-effectors that either move, rotate or rock [16], [20], [21]."],"2546940773":["body interfaces in the medical field [4], and enhancing military training, augmented reality and immersive experiences in gaming [5]."],"2612110125":["is the trade-off between the cost, comfort, and portability of the device, and its provision of a realistic feeling of touch [6]."],"2658158223":["SMAs are lightweight, flexible and actuate silently, beneficial properties for use in an unobtrusive wearable device as exploited in tactile pin-arrays [23]–[25] and fingertipmounted skin stretching devices [26]."],"2765184744":["Over the past decade, a number of devices have utilised skin stretching methods as alternatives to vibratory stimulation, delivering navigation feedback [16], guided movement [17], [18] and robotic manipulator control [19], [20]."],"2783748062":["body interfaces in the medical field [4], and enhancing military training, augmented reality and immersive experiences in gaming [5]."],"2807384662":["Over the past decade, a number of devices have utilised skin stretching methods as alternatives to vibratory stimulation, delivering navigation feedback [16], guided movement [17], [18] and robotic manipulator control [19], [20]."],"2809871420":["Haptic wearable devices come in awide range of shapes, sizes and interaction modalities, including insoles that aid navigation for the visually impaired [7], kinesthetic haptic feedback to guide a user’s hand [8] and active pin arrays for laterotactile"]},"abstract":"Currently, the majority of wearable robotic haptic feedback devices rely on vibrations for relaying sensory information to the user. While this can be very effective, vibration as a physical stimulation is limited in modality and is uncommon in the natural world. In many cases, for human–robot and human–human interaction, a more natural, affective tactile interaction is needed to provide comfortable and varied stimuli. In this letter, we present the super-cutaneous wearable electrical empathic stimulator (SCWEES), a tactile device that gently stretches and squeezes the surface of the skin. Our hypothesis is that this device can create a pleasant, unobtrusive sensation that can be used to mediate social interactions or to deliver subtle alerts. We describe the design of the SCWEES, a lightweight 3D-printed semi-flexible structure that attaches to the skin at two points and actuates via two shape-memory alloy coil actuators. We evaluate the SCWEES through a range of human interaction experiments: stimulation strength and pleasantness, contraction and extension, and the conveyance of non-disruptive notifications. Quantitative and qualitative results show that the SCWEES generates a pleasant sensation, can convey useful information in human–machine interactions, and delivers affective stimulation that is less disruptive than conventional vibratory tactile stimulation when the user is engaged in a task."},{"id":1896337265,"microsoftAcademicId":1896337265,"numberInSourceReferences":178,"doi":"10.1111/TCT.12410","title":"The PAUL Suit(©) : an experience of ageing.","authors":[{"LN":"Bennett","FN":"Paul","affil":"University of Sydney"},{"LN":"Moore","FN":"Malcolm","affil":"University of Sydney"},{"LN":"Wenham","FN":"John","affil":"University of Sydney"}],"year":2016,"journal":"The Clinical Teacher","references":[1989276949,2170754679,2130252913,1551954081,2184683359,2314123449,1965519294,2338600346],"citationsCount":8,"abstract":"Summary\r\nBackground\r\n\r\nAn ageing population worldwide makes it increasingly important that health students understand issues that elderly people face and can provide empathic care to them.\r\n\r\n\r\n\r\nContext\r\n\r\nThis teaching department in an isolated rural setting developed an interprofessional learning session to assist health students to understand issues of functional loss and social isolation that can affect elderly people.\r\n\r\n\r\n\r\nInnovation\r\n\r\nThe Premature Ageing Unisex Leisure (PAUL) Suit© was developed as part of a 1-day learning session for undergraduate health students – including students of medicine, nursing and allied health – attending clinical placement in far-west New South Wales. The suit was developed locally and can be adjusted to simulate a wide range of functional losses in the wearer. Students undertake a range of daily tasks in the community while wearing the suit in the company of a student ‘carer’. Over the past 4 years, approximately 140 students have participated in the simulation. Post-simulation evaluations report that students gain a greater understanding of some functional issues associated with ageing, and of the social isolation that can be associated with these. The experiential nature of the activity leads to some powerful insights.\r\n\r\n\r\n\r\nThis activity is an innovative, experiential tool to deepen students understanding of issues related to ageing\r\n\r\n\r\n\r\nImplications\r\n\r\nThis activity is an innovative, experiential tool to deepen students understanding of issues relating to ageing. The interprofessional nature of the activity is an important factor in the success of the day, and produces a wide range of shared insights. The activity also enhances the partnerships between the university, the health service and the local community. Our experience supports the value of simulation in providing a deep learning opportunity in the area of ageing and disability."},{"id":2909438955,"microsoftAcademicId":2909438955,"numberInSourceReferences":53,"doi":"10.1109/IROS.2018.8593974","title":"Emotional Bodily Expressions for Culturally Competent Robots through Long Term Human-Robot Interaction","authors":[{"LN":"Tuyen","FN":"Nguyen Tan Viet","affil":"Japan Advanced Institute of Science and Technology"},{"LN":"Jeong","FN":"Sungmoon","affil":"Japan Advanced Institute of Science and Technology"},{"LN":"Chong","FN":"Nak Young","affil":"Japan Advanced Institute of Science and Technology"}],"year":2018,"journal":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","references":[1990517717,1595732857,2110802877,2167557160,2098676269,2052101136,203345490,2010150441,2149628368,2024221294,2130533816,2112796159,2106127067,2165287812,2114138219,2099198275,2036372164,141605101,2023397245,1978967727,1521597329,1529648884,2075437821,2772644739,2014706514,2118568774,1996215131,2742847280,2083698397],"citationsCount":12,"citationContext":{"141605101":["In [6], an android head robot imitates human facial expressions with the main goal to improve the emotion recognition capabilities of","While previous researches focus on mimicry of facial expressions [6][12], this research pays special attention to the robot bodily expressions."],"203345490":["The Covariance Descriptor method [18] is used to encode the sequence of frames Ai into the fixed length descriptor."],"1521597329":["Likewise, the UCLIC Affective Body Posture and Motion Database [7] was utilized to generate emotional expressions for robots in [8]."],"1529648884":["Here, classifying trained neurons into different groups is conducted with Distance matrix based approach [24]."],"1595732857":["Social human robot interaction should be treated in a similar way to the interaction with another person [12].","This approach is similar to the strategy applied in [12], where facial and vocal expressions of","While previous researches focus on mimicry of facial expressions [6][12], this research pays special attention to the robot bodily expressions."],"1978967727":["There are strong influences of speed and amplitude of robot motions on the perceived level of arousal and valence [30]."],"1990517717":["DCS adheres to the Kohonen type learning rule [19]","about the number of clusters, a batch version of SelfOrganizing Map (SOM) [19] was used for the training phase in our previous paper [10]."],"1996215131":["To archive this goal, this research investigates the psychological perspectives about infant social development, where the infant’s interpretation and behavioral responses are highly influenced by their parents through imitative exchanges [9]."],"2010150441":["approach of growing neural network by dynamic allocation the feature map in order to evolve its structure are known as Growing Cell Structure (GCS) [22]."],"2014706514":["In [16], the authors made comparisons between different unsupervised learning algorithms such as Self Organizing Maps (SOM), Fuzzy C Means (FCM), and"],"2023397245":["A similar approach was found in [4] for NAO robot, where emotional expressions with bodily movement and eye color was inspired by the work of Meijer [5] and other psychological researchers."],"2024221294":["affective artifacts [3]."],"2036372164":["To satisfy requirement of incremental learning while ensuring the topological preservation of the grid of trained neurons, this research employs a Dynamic Cell Structure (DCS) neural architecture [20] for the training phase."],"2052101136":["about utilizing external sensors like Leap Motion to estimate the demonstrator’s hand-palm orientation [27] should be conducted in the future."],"2075437821":["parameters [23]."],"2083698397":["shown that the physical expression of emotion is an integral part of social interactions to better convey the communicator’s emotion which affects social outcomes [1]."],"2098676269":["There is a strong psychological evidence known as the chameleon effect [13] which is the tendency to mimic the posture, facial expressions, verbal and nonverbal behaviors of"],"2099198275":["Mohammad [15] used unsupervised learning for association between human gestural commands and robot actions."],"2106127067":["Depending on specific robot platform, the typical kinematic parameters should be defined [26]."],"2110802877":["neurons rather than descriptors directly, significant gains in speed of clustering can be obtained [25]."],"2112796159":["for updating the weight of neural vectors the same as the SOM approach, yet uses the Hebbian learning rule [21] to dynamically update the lateral connection structure (topology of the graph of neurons)."],"2114138219":["The capability of robot arm trajectory learning from human demonstrations was proposed by [17], where the trajectory clustering and approximation modules take human demonstrative trajectories as the input and then classify the trajectories into groups."],"2118568774":["Likewise, the UCLIC Affective Body Posture and Motion Database [7] was utilized to generate emotional expressions for robots in [8].","Summarizing, each individual has their own way to express emotions [7]."],"2130533816":["the similarity of body movements, (2) utilizing the human habitual behavior which could be identified by assessing the frequency of similar behaviors [11] as the references for generating emotional bodily expressions and (3) mapping the predicted human habitual behavior into the robot’s motion space."],"2149628368":["At the second phase of survey, subjects were asked to assign appropriate values of arousal and valence [28] using the Self-Assessment Manikin (SAM) five-point scale [29].","It is widely understood that the first quadrant is the location of Happy, while Sad lies in the third quadrant of model [28].","The distribution of subject’s evaluation along the dimensions of arousal and valence [28] is shown in Fig."],"2165287812":["A similar approach was found in [4] for NAO robot, where emotional expressions with bodily movement and eye color was inspired by the work of Meijer [5] and other psychological researchers."],"2742847280":["about the number of clusters, a batch version of SelfOrganizing Map (SOM) [19] was used for the training phase in our previous paper [10].","described in our previous paper [10], the robot learns to imitate the user’s emotional and behavioral responses to environmental stimuli."],"2772644739":["In [2], the authors investigated the role of culture in representing the robot’s emotions, where bodily expressions were utilized to convey the robot’s emotional state."]},"abstract":"Generating emotional bodily expressions for culturally competent robots has been gaining increased attention to enhance the engagement and empathy between robots and humans in a multi-culture society. In this paper, we propose an incremental learning model for selecting the user's representative or habitual emotional behaviors which place emphasis on individual users' cultural traits identified through long term interaction. Furthermore, a transformation model is proposed to convert the obtained emotional behaviors into a specific robot's motion space. To validate the proposed approach, the models were evaluated by two example scenarios of interaction. The experimental results confirmed that the proposed approach endows a social robot with the capability to learn emotional behaviors from individual users, and to generate its emotional bodily expressions. It was also verified that the imitated robot motions are rated emotionally acceptable by the demonstrator and recognizable by the subjects from the same cultural background with the demonstrator."},{"id":3093870830,"microsoftAcademicId":3093870830,"numberInSourceReferences":37,"doi":"10.1109/RO-MAN47096.2020.9223442","title":"Robot-on-Robot Gossiping to Improve Sense of Human-Robot Conversation","authors":[{"LN":"Mitsuno","FN":"Seiya","affil":"Osaka University"},{"LN":"Yoshikawa","FN":"Yuichiro","affil":"Osaka University"},{"LN":"Ishiguro","FN":"Hiroshi","affil":"Osaka University"}],"year":2020,"journal":"2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)","references":[2154543439,2131227926,2080543434,2123076731,2028029365,2126538391,2130543722,3126007474,2061814640,3011041981,2036472487,2981189122,2103084543,2553918534,2903353619,2140334787,2155759171],"citationsCount":0,"citationContext":{"2028029365":["Communication with robots has attracted increased attention not only in the service industry for the purpose of entertaining users [1], but also welfare fields, such as communication support for the elderly [2] as well as education, such as language learning [3].","In recent years, substantial research has been conducted on social robots relating to communication [1]–[3].","decreases with time [1], [3], [6]–[8]."],"2061814640":["reported that humans tend not to attribute subjective experience (such as feeling pain or anger) to robots [20]."],"2080543434":["Communication with robots has attracted increased attention not only in the service industry for the purpose of entertaining users [1], but also welfare fields, such as communication support for the elderly [2] as well as education, such as language learning [3].","In recent years, substantial research has been conducted on social robots relating to communication [1]–[3].","decreases with time [1], [3], [6]–[8]."],"2103084543":["decreases with time [1], [3], [6]–[8]."],"2123076731":["In summary, dialogue strategies involving remarks based on past conversation experiences can imply the robot’s memory ability [9], [10], sociability [11] and perceived mind [12] to the user.","Moreover, a privacy issue may arise even in a dialogue strategy where the robot talks about past dialogue experiences with the exact user who is currently interacting with it [9], [10].","the robot changes the content of its speech when addressing a user with whom it has previously conversed; for example, “Thank you for coming to see me again!” [9] and “This is the third time you have ordered snickers” [10]."],"2126538391":["In summary, dialogue strategies involving remarks based on past conversation experiences can imply the robot’s memory ability [9], [10], sociability [11] and perceived mind [12] to the user.","Moreover, a privacy issue may arise even in a dialogue strategy where the robot talks about past dialogue experiences with the exact user who is currently interacting with it [9], [10].","the robot changes the content of its speech when addressing a user with whom it has previously conversed; for example, “Thank you for coming to see me again!” [9] and “This is the third time you have ordered snickers” [10]."],"2130543722":["decreases with time [1], [3], [6]–[8]."],"2140334787":["We also evaluated the intention to use [19], which should increase with the improvement in the sense of conversation."],"2154543439":["Communication with robots has attracted increased attention not only in the service industry for the purpose of entertaining users [1], but also welfare fields, such as communication support for the elderly [2] as well as education, such as language learning [3]."],"2155759171":["Humans react negatively to the frequent occurrence of disagreements in conversation [5], [14].","Several researchers have reported that people tend to react negatively when many disagreements occur during dialogue [5], [14]."],"2553918534":["Humans react negatively to the frequent occurrence of disagreements in conversation [5], [14].","Moreover, several researchers have particularly focused on the exchange of subjective opinions in humanrobot conversations [4], [5].","Several researchers have reported that people tend to react negatively when many disagreements occur during dialogue [5], [14]."],"2903353619":["Robot: We used CommU, a small child-like robot, for the experiment [15] (Fig."],"2981189122":["In a dialogue strategy in which a robot refers to a user’s past remarks in conversation with another user, namely gossiping [11], [12], a user whose private information is exposed by the robot may feel discomfort.","In summary, dialogue strategies involving remarks based on past conversation experiences can imply the robot’s memory ability [9], [10], sociability [11] and perceived mind [12] to the user.","jp a dialogue strategy in which the robot refers to a user’s past remark in conversation with another user as gossiping, such as “The previous guy told me he likes red” [11], [12]."],"3011041981":["In a dialogue strategy in which a robot refers to a user’s past remarks in conversation with another user, namely gossiping [11], [12], a user whose private information is exposed by the robot may feel discomfort.","In summary, dialogue strategies involving remarks based on past conversation experiences can imply the robot’s memory ability [9], [10], sociability [11] and perceived mind [12] to the user.","jp a dialogue strategy in which the robot refers to a user’s past remark in conversation with another user as gossiping, such as “The previous guy told me he likes red” [11], [12]."],"3126007474":["However, in the current information society where the awareness of privacy is increasing [13], a strong probability exists that privacy problems will occur when the robot mentions the user information (including preferences, past behaviors, and names), which causes the user to feel uncomfortable."]},"abstract":"In recent years, a substantial amount of research has been aimed at realizing a social robot that can maintain long-term user interest. One approach is using a dialogue strategy in which the robot makes a remark based on previous dialogues with users. However, privacy problems may occur owing to private information of the user being mentioned. We propose a novel dialogue strategy whereby a robot mentions another robot in the form of gossiping. This dialogue strategy can improve the sense of conversation, which results in increased interest while avoiding the privacy issue. We examined our proposal by conducting a conversation experiment evaluated by subject impressions. The results demonstrated that the proposed method could help the robot to obtain higher evaluations. In particular, the perceived mind was improved in the Likert scale evaluation, whereas the robot empathy and intention to use were improved in the binary comparison evaluation. Our dialogue strategy may contribute to understanding the factors regarding the sense of conversation, thereby adding value to the field of human-robot interaction."},{"id":2971447222,"microsoftAcademicId":2971447222,"numberInSourceReferences":158,"doi":"10.3389/FROBT.2019.00088","title":"Android Pretending to Have Similar Traits of Imagination as Humans Evokes Stronger Perceived Capacity to Feel","authors":[{"LN":"Tatsukawa","FN":"Kyohei","affil":"Osaka University"},{"LN":"Takahashi","FN":"Hideyuki","affil":"Osaka University"},{"LN":"Yoshikawa","FN":"Yuichiro","affil":"Osaka University"},{"LN":"Ishiguro","FN":"Hiroshi","affil":"Osaka University"}],"year":2019,"journal":"Frontiers in Robotics and AI","references":[2131227926,2140173604,2021641437,2104999980,2014833927,2095436958,2165113252,2002024051,2129928382,2109682840,2018372581,2074134568,2011525365,2160145433,2102570352,2782247249,2159947028,2066097269,2172086217,2074433766,2894075110,2312852713,2506732915,2073841912],"citationsCount":1,"abstract":"The perception of robots as mindful enriches how humans relate to them. Given that congruence in perceived representations of the world enable humans to experience commonality in mental states (a shared reality), we propose that congruence between humans and robots will encourage humans to attribute humanlike mental capacities to robots. To investigate this, we assessed the mental perceptions of a robot in a visual imagination task using Gray et al.’s mind perception scale, which evaluates experience (capacity to feel) and agency (capacity to plan and do). For each ambiguous picture in the designed task, humans and a robot imagined an animal. The task was performed under six conditions (2 × 3: Lead/Follow for Low/Medium/High). In the Lead condition, the robot records its perceived animal first; in the Follow condition, the robot records after the human participant. The experiment had three different degrees of congruence: Low (0%), Medium (60%), and High (100%). The results showed that perceived experiences were higher in the Lead condition, suggesting that the robot is perceived to be empathetic. It is probable that the Follow condition was perceived as mimicry rather than shared reality. Therefore, the order of response may have played an important role in commonality in mental states. No differences were observed in the perceived agency across all conditions. These results suggest that the order of response affects how humans perceive the minds of robots. Additionally, we assessed a post-task questionnaire to evaluate the interpersonal closeness that the humans felt towards the android. The main effect was observed in the degrees of congruence. This result is in line with those of previous studies that report relationships across sharing of similarities and friendliness."},{"id":2884360773,"microsoftAcademicId":2884360773,"numberInSourceReferences":115,"doi":"10.1016/J.BICA.2018.07.010","title":"A computational model of empathy for interactive agents","authors":[{"LN":"Yalcin","FN":"Ӧzge Nilay","affil":"Simon Fraser University"},{"LN":"DiPaola","FN":"Steve","affil":"Simon Fraser University"}],"year":2018,"journal":"Biologically Inspired Cognitive Architectures","references":[2132339004,1984186949,2117645142,2306941105,1966797434,2159398820,1977137834,2153480757,1985945240,1975000068,2161138645,2041649634,1993584577,1853322427,2150422125,2102639289,2102573486,1973672195,2732246028,2104674963,2165734786,1974219072,1589781773,2010943014,1559672525,103982469,2290844432,2053782908,1825507529,2123952992,2066391335,2754425142,2005563557,2111151330,614048249,2065824068,2035056751,2506641825,1997163218,2064996043,605667585,2080119116,2479661493,2905132164],"citationsCount":15,"abstract":"Abstract  Empathy has been defined in the scientific literature as the capacity to relate another’s emotional state and assigned to a broad spectrum of cognitive and behavioral abilities. Advances in neuroscience, psychology and ethology made it possible to refine the defined functions of empathy to reach a working definition and a model of empathy. Recently, cognitive science and artificial intelligence communities made attempts to model empathy in artificial agents, which can provide means to test these models and hypotheses. A computational model of empathy not only would help to advance the technological artifacts to be more socially compatible, but also understand the empathy mechanisms, test theories, and address the ethics and morality problems the Artificial Intelligence (AI) community is facing today. In this paper, we will review the empathy research from various fields, gather the requirements for empathic capacity and construct a model of empathy that is suitable for interactive conversational agents."},{"id":2769375497,"microsoftAcademicId":2769375497,"numberInSourceReferences":70,"doi":"10.1080/09540091.2017.1350938","title":"Socially grounded game strategy enhances bonding and perceived smartness of a humanoid robot","authors":[{"LN":"Barakova","FN":"E. I.","affil":"Eindhoven University of Technology"},{"LN":"Haas","FN":"M. De","affil":"Eindhoven University of Technology"},{"LN":"Kuijpers","FN":"W.","affil":"Eindhoven University of Technology"},{"LN":"Irigoyen","FN":"N.","affil":"Eindhoven University of Technology"},{"LN":"Betancourt","FN":"A.","affil":"Eindhoven University of Technology"}],"year":2018,"journal":"Connection Science","references":[1509235676,2079476777,2111040806,2120323262,2123076731,2154447040,2592799411,2308362605,2133857047,2125281127,2029374903,2579873239,162743038,2073833219,2120258909,1975839565,627140926,1516665276,336178331,2038417624,118959678,2134369348,2496928381,2172806250,2145229991],"citationsCount":12,"abstract":"In search for better technological solutions for education, we adapted a principle from economic game theory, namely that giving a help will promote collaboration and eventually long-term relations..."},{"id":2346978479,"microsoftAcademicId":2346978479,"numberInSourceReferences":180,"doi":"10.1515/PJBR-2015-0010","title":"Towards the synthetic self: making others perceive me as an other","authors":[{"LN":"Lallee","FN":"Stephane"},{"LN":"Vouloutsi","FN":"Vasiliki"},{"LN":"Munoz","FN":"Maria Blancas"},{"LN":"Grechuta","FN":"Klaudia"},{"LN":"Llobet","FN":"Jordi-Ysard Puigbo"},{"LN":"Sarda","FN":"Marina"},{"LN":"Verschure","FN":"Paul F.M.J."}],"year":2015,"journal":"Paladyn: Journal of Behavioral Robotics","references":[2150839178,2114181363,1982597817,2070862239,2103176783],"citationsCount":16},{"id":2780285914,"microsoftAcademicId":2780285914,"numberInSourceReferences":114,"doi":"10.22452/MJCS.VOL30NO4.1","title":"Emotional Empathy Model For Robot Partners Using Recurrent Spiking Neural Network Model With Hebbian-Lms Learning","authors":[{"LN":"Woo","FN":"Jinseok","affil":"Tokyo Metropolitan University"},{"LN":"Botzheim","FN":"JÃ¡nos","affil":"Tokyo Metropolitan University"},{"LN":"Kubota","FN":"Naoyuki","affil":"Tokyo Metropolitan University"}],"year":2017,"journal":"Malaysian Journal of Computer Science","references":[],"citationsCount":8,"abstract":"This paper discusses the development of an emotion model for robot partner system. In our previous studies, we have focused only on the robot’s emotional state. However, the emotional state of the other party is also an important factor for smooth conversation in human society. Therefore, the robot partner has two emotional structures for human: empathy and robot emotion. First, human empathy uses a perceptual based emotion model to know the human’s emotional state based on the sensory information. Next, we propose a recurrent simple spike response model to improve the robot’s emotional model, and we apply “Hebbian-LMS” learning to modify the weights in the spiking neural network. The robot’s emotional state is calculated by using the human’s emotional information, internal and external information. The robot partner can use the emotional results to control the facial and gesture expression. The utterance style is also changed by the robot’s emotional state. As a result, the robot partner can interact emotionally and naturally with human. First, we explain the related works and the development of the robot partner “iPhonoid-C”. Next, we define the architecture of the emotional model to realize emotional empathy towards human. Then, we discuss the algorithms and the methods for developing the emotional model. Finally, we show experimental results of the proposed method, and discuss the effectiveness of the proposed structure."},{"id":2724805274,"microsoftAcademicId":2724805274,"numberInSourceReferences":102,"doi":"10.3389/FPSYG.2017.00950","title":"Affective and Engagement Issues in the Conception and Assessment of a Robot-Assisted Psychomotor Therapy for Persons with Dementia.","authors":[{"LN":"Rouaix","FN":"Natacha","affil":"Pierre-and-Marie-Curie University"},{"LN":"Retru-Chavastel","FN":"Laure","affil":"Arts et Métiers ParisTechParis, France."},{"LN":"Rigaud","FN":"Anne-Sophie","affil":"Arts et Métiers ParisTech, Paris, France"},{"LN":"Monnet","FN":"Clotilde","affil":"Paris Descartes University"},{"LN":"Monnet","FN":"Clotilde","affil":"Arts et Métiers ParisTech"},{"LN":"Monnet","FN":"Clotilde","affil":"Beaujon Hospital"},{"LN":"Lenoir","FN":"Hermine","affil":"Paris Descartes University"},{"LN":"Lenoir","FN":"Hermine","affil":"Arts et Métiers ParisTech"},{"LN":"Lenoir","FN":"Hermine","affil":"Beaujon Hospital"},{"LN":"Pino","FN":"Maribel","affil":"Paris Descartes University"},{"LN":"Pino","FN":"Maribel","affil":"Arts et Métiers ParisTech"},{"LN":"Pino","FN":"Maribel","affil":"Beaujon Hospital"}],"year":2017,"journal":"Frontiers in Psychology","references":[1847168837,2154543439,2043711811,1987250353,1174325445,2031528526,2167735895,2169984036,2033927268,1575107006,2529469190,2131265993,2088831794,1807810766,2056681550,2119283223,2282380228,2012511508,1966441013,1449228522,1969152782,2087242380,2463024801,2134767980,2123031276,3122437993,2054010997,2413309935,2077020632,2481398984,2069874839,2415005235,2243904590,2287947768],"citationsCount":16,"abstract":"The interest in robot-assisted therapies (RAT) for dementia care has grown steadily in recent years. However, RAT using humanoid robots is still a novel practice for which the adhesion mechanisms, indications and benefits remain unclear. Also, little is known about how the robot’s behavioral and affective style might promote engagement of persons with dementia in RAT. The present study sought to investigate the use of a humanoid robot in a psychomotor therapy for persons with dementia. We examined the robot’s potential to engage participants in the intervention and its effect on their emotional state. A brief psychomotor therapy program involving the robot as the therapist’s assistant was created. For this purpose, a corpus of social and physical behaviors for the robot and a “control software” for customizing the program and operating the robot were also designed. Particular attention was given to components of the RAT that could promote participant’s engagement (robot’s interaction style, personalization of contents). In the pilot assessment of the intervention nine persons with dementia (7 women and 2 men, M age = 86 y/o) hospitalized in a geriatrics unit participated in four individual therapy sessions: one classic therapy (CT) session (patient- therapist) and three RAT sessions (patient-therapist-robot). Outcome criteria for the evaluation of the intervention included: participant’s engagement, emotional state and well-being; satisfaction of the intervention, appreciation of the robot, and empathy-related behaviors in human-robot interaction. Results showed a high constructive engagement in both CT and RAT sessions. More positive emotional responses in participants were observed in RAT compared to CT. RAT sessions were better appreciated than CT sessions. The use of a social robot as a mediating tool appeared to promote the involvement of persons with dementia in the therapeutic intervention increasing their immediate wellbeing and satisfaction."},{"id":2740340001,"microsoftAcademicId":2740340001,"numberInSourceReferences":125,"doi":"10.18653/V1/P17-4021","title":"Zara Returns: Improved Personality Induction and Adaptation by an Empathetic Virtual Agent","authors":[{"LN":"Siddique","FN":"Farhad Bin","affil":"Hong Kong University of Science and Technology"},{"LN":"Kampman","FN":"Onno Pepijn","affil":"Hong Kong University of Science and Technology"},{"LN":"Yang","FN":"Yang","affil":"Hong Kong University of Science and Technology"},{"LN":"Dey","FN":"Anik","affil":"Hong Kong University of Science and Technology"},{"LN":"Fung","FN":"Pascale N.","affil":"Hong Kong University of Science and Technology"}],"year":2017,"journal":"ACL 2017 - 55th Annual Meeting of the Association for Computational Linguistics, Proceedings of System Demonstrations 2017","references":[2964121744,2095705004,2153579005,1832693441,2120615054,2162090451,2085662862,2058818389,2423024114,1966732563,2136009731,2293723806,2480710602,1501506202,1666984270,2573933330,2165312226,2565875961,1989717603,1976692960,2522578306,2263093675,2963382311,2465954854,2167674813,992055789,1994660327,2009504015,2400677246,1554962524,2580937268,1935401112,2295404107],"citationsCount":11,"citationContext":{"992055789":["These scenarios show improvement in user satisfaction and also better collaboration between the user and the VA (Hu et al., 2015; Liew and Tan, 2016).","to non-adaptive VAs when completing a task (Hu et al., 2015)."],"1501506202":["Our classification performance is good when comparing, for instance, to the winner of the 2012 INTERSPEECH Speaker Trait subChallenge on Personality (Ivanov and Chen, 2012; Schuller et al., 2012)."],"1554962524":["To improve, we can seek works in psychology on friendship and relationship compatibility based on personality (Huston and Houts, 1998) to extract and match features"],"1935401112":["Past research has shown the importance of VAs adapting to users in terms of culture (Al-Saleh and Romano, 2015), learning style (Liew and Tan, 2016), and social constructs (Youssef et al., 2015)."],"1966732563":["It is predominantly described with the Big Five model of personality (Goldberg, 1993), which defines five personality traits and rates a person along them."],"1976692960":["The datasets used for the training are taken from the Workshop on Computational Personality Recognition (WCPR) (Kosinski and Stillwell, 2012; Celli et al., 2014)."],"1989717603":["The Tough VA (TVA) and the Gentle VA (GVA) embody the traits of dominance and submissiveness dimension of interpersonal behavior (Kiesler, 1983) respectively."],"2009504015":["In human-human interactions (HHI), personality compatibility is important for relationship satisfaction and interpersonal closeness (Long and Martin, 2000; Berry et al., 2000)."],"2058818389":["In our new version of Zara, we start with recognizing user personality through speech and text based on the Big Five traits of personality (Gosling et al., 2003; Barrick and Mount, 1991)."],"2095705004":["The model is iteratively trained to minimize the Mean Squared Error (MSE) between trait predictions and corresponding training set ground truths, using Adam (Kingma and Ba, 2014) as optimizer and Dropout (Srivastava et al., 2014) in between the two fully connected layers to prevent model overfitting."],"2136009731":["This is in line with human interactions, where empathy in physicians directly improves patient satisfaction and recovery (Derksen et al., 2013)."],"2153579005":["In particular, using pre-trained word embeddings like word2vec (Mikolov et al., 2013) to represent the text has proved to be useful in classifying text from different domains."],"2162090451":["In our new version of Zara, we start with recognizing user personality through speech and text based on the Big Five traits of personality (Gosling et al., 2003; Barrick and Mount, 1991)."],"2165312226":["We propose a method for automatically detecting someone’s personality without the need for complex feature extraction upfront, as in (Mohammadi and Vinciarelli, 2012)."],"2167674813":["In human-human interactions (HHI), personality compatibility is important for relationship satisfaction and interpersonal closeness (Long and Martin, 2000; Berry et al., 2000)."],"2263093675":[", 2001) lexical features to treat as baseline (Verhoeven et al., 2013) in order to compare with our own model."],"2293723806":["al’s research (Miner et al., 2016), various world renowned virtual assistants respond inconsistently and impersonally to affectsensitive topics such as mental health, domestic violence, and emergencies."],"2295404107":["Past research has shown the importance of VAs adapting to users in terms of culture (Al-Saleh and Romano, 2015), learning style (Liew and Tan, 2016), and social constructs (Youssef et al."],"2400677246":["Our classification performance is good when comparing, for instance, to the winner of the 2012 INTERSPEECH Speaker Trait subChallenge on Personality (Ivanov and Chen, 2012; Schuller et al., 2012)."],"2423024114":["extracted lexical features from text, and prosodic features from speech clips (Mairesse et al., 2007)."],"2465954854":["We are building our current work on top of our previous interactive system of Zara the Supergirl (Fung et al., 2015), with an updated user interface (see Figure 1)."],"2480710602":["Big Five personalities can be determined by self-reported assessments, such as the NEO-PI-R (Costa and McCrae, 2008)."],"2522578306":["experimented with a CNN approach on video snapshots and raw audio to train their model (Güçlütürk et al., 2016)."],"2565875961":["utterance, the system gives an empathetic response based on the user’s sentiment and emotion scores (Bertero et al., 2016; Fung et al., 2016)."],"2573933330":["The dataset trained on is the ChaLearn First Impressions dataset (Ponce-López et al., 2016)."],"2580937268":["Past research has shown the importance of VAs adapting to users in terms of culture (Al-Saleh and Romano, 2015), learning style (Liew and Tan, 2016), and social constructs (Youssef et al.","These scenarios show improvement in user satisfaction and also better collaboration between the user and the VA (Hu et al., 2015; Liew and Tan, 2016)."]}},{"id":3040090920,"microsoftAcademicId":3040090920,"numberInSourceReferences":105,"doi":"10.1007/S40860-020-00109-Y","title":"Embodiment matters: toward culture-specific robotized counselling","authors":[{"LN":"Sakurai","FN":"Eriko","affil":"Sanno University"},{"LN":"Kurashige","FN":"Kentarou","affil":"Muroran Institute of Technology"},{"LN":"Tsuruta","FN":"Setsuo","affil":"Tokyo Denki University"},{"LN":"Sakurai","FN":"Yoshitaka","affil":"Meiji University"},{"LN":"Knauf","FN":"Rainer","affil":"Technische Universität Ilmenau"},{"LN":"Damiani","FN":"Ernesto","affil":"Khalifa University"},{"LN":"Damiani","FN":"Ernesto","affil":"University of Milan"},{"LN":"Kutics","FN":"Andrea","affil":"International Christian University"},{"LN":"Frati","FN":"Fulvio","affil":"University of Milan"}],"year":2020,"journal":"Journal of Reliable Intelligent Environments","references":[2752491485,2041889512,2549408802,2043711811,2223237395,2013502477,2028208724,2111579636,1964549822,2996423945,1620054078,2161466446,2251724710,2162236696,2321205984,2992986204,3008573263,2611610300,2209259971,2551337155,2081230344,206138797,3098029746,2757652854,2165403987,2497117392,2322584079,625298245,2796662851,2528143287],"citationsCount":1,"abstract":"In this paper, we propose adding the traditional Japanese nodding behavior to the repertoire of social movements to be used in the context of human–robot interaction. Our approach is motivated by the notion that in many cultures, trust-building can be boosted by small body gestures. We discuss the integration of a robot capable of such movements within CRECA, our context-respectful counseling agent. The frequent nodding called “unazuki” in Japan, often accompanying the “un-un” sound (meaning “I agree”) of Japanese onomatopoeia, underlines empathy and embodies unconditioned approval. We argue that “unazuki” creates more empathy and promotes longer conversation between the robotic counsellor and people. We set up an experiment involving ten subjects to verify these effects. Our quantitative evaluation is based on the classic metrics of utterance, adapted to support the Japanese language. Interactions featuring “unazuki” showed higher value of this metrics. Moreover, subjects assessed the counselling robot’s trustworthiness and kindness as “very high” (Likert scale: 5.5 versus 3—4.5) showing the effect of social gestures in promoting empathetic dialogue to general people including the younger generation. Our findings support the importance of social movements when using robotized agents as a therapeutic tool aimed at improving emotional state and social interactions, with unambiguous evidence that embodiment can have a positive impact that warrants further exploration. The 3D printable design of our robot supports creating culture-specific libraries of social movements, adapting the gestural repertoire to different human cultures."},{"id":2343590725,"microsoftAcademicId":2343590725,"numberInSourceReferences":103,"doi":"10.1109/TPAMI.2015.2496209","title":"Probabilistic Social Behavior Analysis by Exploring Body Motion-Based Patterns","authors":[{"LN":"Roudposhti","FN":"Kamrad Khoshhal","affil":"University of Coimbra"},{"LN":"Nunes","FN":"Urbano","affil":"University of Coimbra"},{"LN":"Dias","FN":"Jorge","affil":"University of Coimbra"}],"year":2016,"journal":"IEEE Transactions on Pattern Analysis and Machine Intelligence","references":[2172156083,1983705368,1973672195,3147976114,2128107557,1689644917,2129421456,2165312226,2136119880,2147272821,2161051436,2041536178,1981509185,2034714139,2008414863,2041910615,2073453224,2301917967,2139764446,197868994,2113145199,1514751585,2049828486,2025275271,20636761,1981936999,2030213573,2087888516],"citationsCount":16,"citationContext":{"1514751585":["Human communication is highly connected to the body language and paralinguistic cues [7], [25]."],"1689644917":["[4] analyzed human emotions by exploring body movements and gesture dynamics based features."],"1981509185":["social behaviors using different nonverbal-based features: prosodic features [18], facial expression [8], facial and verbal expressions [16], and 3D body pose and voice patterns [23]."],"1981936999":["For instance, Alazrai and Lee [2] proposed a two-layered framework to identify human actions in a human-robot"],"1983705368":["analysis, hierarchical approaches have been proposed [1]."],"2034714139":["social behaviors using different nonverbal-based features: prosodic features [18], facial expression [8], facial and verbal expressions [16], and 3D body pose and voice patterns [23]."],"2041536178":["To simplify the modeling process, the “influence model” techniquewas proposed in [6].","[6]."],"2073453224":["4) used to estimate the Effort-Time parameter [11], [12]."],"2113145199":["in [31], used just visual motion energy-based features to analyze mimicry expression in interpersonal interaction."],"2128107557":["sented in [29] an approach for human action modeling by using a number of BNs to recognize the poses of body parts and a DBN to analyze human activities in 2D image data."],"2129421456":["in [10], studied the effect of different audiovisual nonverbal cues for modeling dominance patterns in group conversation scenarios."],"2136119880":["A new dataset and a method for inter-annotator agreements [13], for having more reliable ground truth, will be the subject of our future work.","For the annotation process, we intend to apply a method, such as [13], which is based on judgments of a group of observers to have less effect from observersmistakes."],"2139764446":["example, in 3D body pose-based applications [19]."],"2147272821":["[17] used 3D body parts and speech based features to track a number of emotional trends (activation, valence and dominance)."],"2161051436":["[15] for a robotic application."],"2165312226":["social behaviors using different nonverbal-based features: prosodic features [18], facial expression [8], facial and verbal expressions [16], and 3D body pose and voice patterns [23]."],"2172156083":["[30])."],"3147976114":["Based on the Pentland’s definition about the SRs, the Leading role is described as a combination of attention, interest and great focus in thought and purpose [22].","For analyzing human IBs and SRs, we were inspired by Alex Pentland’s study in “Honest signals” [22], where a number of IBs with their relevant features, and a number of SRs with their corresponding IBs, are defined.","However, the process needs longer data (thirty-seconds [22]), so that the analysis of IBs is effectively feasible.","Several scenarios such as job interview, classroom, and dating, were studied in [22] to illustrate a number of","Space-Head Forward, Backward Space-Hands Front, Back, Up, Down, Right, Left Effort-Time Sudden, Sustained Effort-Space Direct, Indirect Shape-Sagittal Advance, Retreat Shape-Vertical Rise, Sink Shape-Horizontal Spread, Enclose TABLE 4 Effective LMA Components for Each IB [22], [27] (X: Is Not Relevant, and p : Is Relevant)","TABLE 2 Relevant IBs States for the Leading Role [22]","The social terms are defined as Interpersonal Behavior (IB) and Social Role (SR) in [22], [27].","To fill the gap between the nonverbal-based features (vocal and body motion) and SR, the following four IBs were proposed by Alex Pentland [22]:","[22] States [27] LMA comp.","to some of the mentioned IBs [22]."]},"abstract":"Understanding human behavior through nonverbal-based features, is interesting in several applications such as surveillance, ambient assisted living and human-robot interaction. In this article in order to analyze human behaviors in social context, we propose a new approach which explores interrelations between body part motions in scenarios with people doing a conversation. The novelty of this method is that we analyze body motion-based features in frequency domain to estimate different human social patterns: Interpersonal Behaviors (IBs) and a Social Role (SR). To analyze the dynamics and interrelations of people's body motions, a human movement descriptor is used to extract discriminative features, and a multi-layer Dynamic Bayesian Network (DBN) technique is proposed to model the existent dependencies. Laban Movement Analysis (LMA) is a well-known human movement descriptor, which provides efficient mid-level information of human body motions. The mid-level information is useful to extract the complex interdependencies. The DBN technique is tested in different scenarios to model the mentioned complex dependencies. The study is applied for obtaining four IBs ( Interest , Indicator ,  Empathy   and  Emphasis ) to estimate one SR ( Leading ).The obtained results give a good indication of the capabilities of the proposed approach for people interaction analysis with potential applications in human-robot interaction."},{"id":2767103073,"microsoftAcademicId":2767103073,"numberInSourceReferences":112,"doi":"10.4018/IJTHI.2018010103","title":"Fake Empathy and Human-Robot Interaction (HRI): A Preliminary Study","authors":[{"LN":"Vallverdú","FN":"Jordi","affil":"Autonomous University of Barcelona"},{"LN":"Nishida","FN":"Toyoaki","affil":"Kyoto University"},{"LN":"Ohmoto","FN":"Yoshisama","affil":"Kyoto University"},{"LN":"Moran","FN":"Stuart","affil":"University of Nottingham"},{"LN":"Lázare","FN":"Sarah","affil":"Autonomous University of Barcelona"}],"year":2018,"journal":"International Journal of Technology and Human Interaction","references":[3022532439,2096476371,2150422125,2131799829,2127241481,2144872492,2024218186,2144251798,1976156697,13388985,1981509185,41929864,2114550714,2045541992,2073979665,2341720544,2121468850,1980785352,2059868177,1567800216,2059550545,2100274075,2346728334,203969526,2531247636,1978516982,2461378098,39105483,2487754837,2064171501,2013087337,2156916323,2592608098,2091260387],"citationsCount":5,"abstract":"Empathy is a basic emotion trigger for human beings, especially while regulating social relationships and behaviour. The main challenge of this paper is study whether people's empathic reactions towards robots change depending on previous information given to human about the robot before the interaction. The use of false data about robot skills creates different levels of what we call ‘fake empathy'. This study performs an experiment in WOZ environment in which different subjects (n=17) interacting with the same robot while they believe that the robot is a different robot, up to three versions. Each robot scenario provides a different ‘humanoid' description, and out hypothesis is that the more human-like looks the robot, the more empathically can be the human responses. Results were obtained from questionnaires and multi- angle video recordings. Positive results reinforce the strength of our hypothesis, although we recommend a new and bigger and then more robust experiment."},{"id":2747937212,"microsoftAcademicId":2747937212,"numberInSourceReferences":15,"doi":"10.1007/978-3-319-67401-8_15","title":"Do We Need Emotionally Intelligent Artificial Agents? First Results of Human Perceptions of Emotional Intelligence in Humans Compared to Robots","authors":[{"LN":"Fan","FN":"Lisa","affil":"Tufts University"},{"LN":"Scheutz","FN":"Matthias","affil":"Tufts University"},{"LN":"Lohani","FN":"Monika","affil":"University of Utah"},{"LN":"McCoy","FN":"Marissa","affil":"The MITRE Coorporation"},{"LN":"Stokes","FN":"Charlene K.","affil":"The MITRE Coorporation"}],"year":2017,"journal":"International Conference on Intelligent Virtual Agents","references":[2142175015,2110171129,2135907989,2057537779,1985945240,2295535820,2065178260,2135476713,2132049110,1905220224,1701865079,2109239231,2157906992,2094169820,2058688200,1977387036,2085710876,2910057470,1939872869,2342786882,2490472860,255083219,2549255633,116290630,2097489015],"citationsCount":10,"abstract":"Humans are very apt at reading emotional signals in other humans and even artificial agents, which raises the question of whether artificial agents need to be emotionally intelligent to ensure effective social interactions. For artificial agents without emotional intelligence might generate behavior that is misinterpreted, unexpected, and confusing to humans, violating human expectations and possibly causing emotional harm. Surprisingly, there is a dearth of investigations aimed at understanding the extent to which artificial agents need emotional intelligence for successful interactions. Here, we present the first study in the perception of emotional intelligence (EI) in robots vs. humans. The objective was to determine whether people viewed robots as more or less emotionally intelligent when exhibiting similar behaviors as humans, and to investigate which verbal and nonverbal communication methods were most crucial for human observational judgments. Study participants were shown a scene in which either a robot or a human behaved with either high or low empathy, and then they were asked to evaluate the agent’s emotional intelligence and trustworthiness. The results showed that participants could consistently distinguish the high EI condition from the low EI condition regardless of the variations in which communication methods were observed, and that whether the agent was a robot or human had no effect on the perception. We also found that relative to low EI high EI conditions led to greater trust in the agent, which implies that we must design robots to be emotionally intelligent if we wish for users to trust them."},{"id":2527867139,"microsoftAcademicId":2527867139,"numberInSourceReferences":169,"doi":"10.1016/J.SNA.2016.10.006","title":"Touch sensor for social robots and interactive objects affective interaction","authors":[{"LN":"Mazzei","FN":"Daniele","affil":"University of Pisa"},{"LN":"Maria","FN":"Carmelo De","affil":"University of Pisa"},{"LN":"Vozzi","FN":"Giovanni","affil":"University of Pisa"}],"year":2016,"journal":"Sensors and Actuators A-physical","references":[2171130677,1595732857,1989104072,2054854590,2044663075,2001381166,2136426847,2152966241,2075285530,2101916640,2021928942,2160911470,2132973290,2169516620,1001282315,2166649005,2305730232,73339090,2085466338,2052164623,1514450035,1883295944,2040635439,2073808499,1748982522,2066173316,2041609140],"citationsCount":7,"abstract":"Abstract  The recognised importance of physical experience in empathic exchanges has led to the development of touch sensors for human–robot affective interaction. Most of these sensors, implemented as matrix of pressure sensors, are rigid, cannot be fabricated in complex shapes, cannot be subjected to large deformations, and usually allow to capture only the contact event, without any information about the interaction context. This paper presents a tactile flux sensor able to capture the entire context of the interaction including gestures and patterns. The sensor is made of alternate layers of sensitive and insulating silicone: the soft nature of the sensor makes it adaptable to complex and deformable bodies. The main features from electrical signals are extracted with the principal component analysis, and a self-organising neural network is in charge for the classification and spatial identification of the events to acknowledge and measure the gesture. The results open to interesting applications, which span from toy manufacturing, to human-robot interaction, and even to sport and biomedical equipment and applications."},{"id":3092837391,"microsoftAcademicId":3092837391,"numberInSourceReferences":152,"doi":"10.1016/J.CORTEX.2020.09.028","title":"Visual similarity and psychological closeness are neurally dissociable in the brain response to vicarious pain.","authors":[{"LN":"Ionta","FN":"Silvio","affil":"Istituto Italiano di Tecnologia"},{"LN":"Costantini","FN":"Marcello","affil":"University of Chieti-Pescara"},{"LN":"Ferretti","FN":"Antonio","affil":"University of Chieti-Pescara"},{"LN":"Galati","FN":"Gaspare","affil":"Sapienza University of Rome"},{"LN":"Romani","FN":"Gian Luca","affil":"Institute of Advanced Biomedical Technologies, University G. D'Annunzio, Chieti, Italy."},{"LN":"Aglioti","FN":"Salvatore M.","affil":"Istituto Italiano di Tecnologia"}],"year":2020,"journal":"Cortex","references":[2105824687,2058046532,2116146623,2073825915,2153480757,2098425678,2147093380,2059982399,2058713030,1853322427,2157258415,2058521226,2168222508,2138173706,1531938074,2123983635,1973030362,2121004914,2156285102,2055132787,2160683905,2079547149,2127117461,2132578438,2095962545,2085580706,2149325275,2023967132,2115742360,2170938715,2565550142,2533478463,2163336065,2113929747,1132316397,2125566732,2139426300,1972224334,2164998362,2120445824,2080185696,1988938182,2104568401,2019026055,2021636247,2157566701,2150382179,2164482323,2903047954,2763312725,3085014638,1977104693,2156766386,2282380228,2150850591,2110857432,2795498669,3010443243,2145931445,2142859820,1673765788,2012683591,2096618039,2133595197,2963832994,2522276591,2132504838,2160780765,2259091274,2068400048,1964105347,2958151727,2102526097,2972770416,2087327605,2511552942,2312471330,2582879630,2020061795,2117270641,2903921837,1993474157,2567881201,3007859925,2345251901,1966169785,2334844868,3026217146,2417306687,2397594194,2919701584,2999363318,2089807661,2748630952,2885411966,2409666585,2065604437,2894750360,2769331749,2213229394,1963951403,2979884473,2947424230,2911699745,2008849371,2310740709,1847108502,2946659702,2121219893],"citationsCount":4,"abstract":"Personal and vicarious experience of pain activate partially overlapping brain networks. This brain activity is further modulated by low- and high-order factors, e.g., the perceived intensity of the model's pain and the model's similarity with the onlooker, respectively. We investigated which specific aspect of similarity modulates such empathic reactivity, focusing on the potential differentiation between visual similarity and psychological closeness between the onlooker and different types of models. To this aim, we recorded fMRI data in neurotypical participants who observed painful and tactile stimuli delivered to an adult human hand, a baby human hand, a puppy dog paw, and an anthropomorphic robotic hand. The interaction between type of vicarious experience (pain, touch) and nature of model (adult, baby, dog, robot) showed that the right supramarginal gyrus (rSMG) was selectively active for visual similarity (more active during vicarious pain for the adult and baby models), while the anterior cingulate cortex (ACC) was more sensitive to psychological closeness (specifically linked to vicarious pain for the baby model). These findings indicate that visual similarity and psychological closeness between onlooker and model differentially affect the activity of brain regions specifically implied in encoding interindividual sharing of sensorimotor and affective aspects of vicarious pain, respectively."},{"id":2525290694,"microsoftAcademicId":2525290694,"numberInSourceReferences":139,"doi":"10.1145/2974804.2974825","title":"Sharing Emotion Described as Text on the Internet by Changing Self-physiological Perception","authors":[{"LN":"Sakurai","FN":"Sho","affil":"University of Electro-Communications"},{"LN":"Ban","FN":"Yuki","affil":"University of Tokyo"},{"LN":"Katsumura","FN":"Toki","affil":"University of Tokyo"},{"LN":"Narumi","FN":"Takuji","affil":"University of Tokyo"},{"LN":"Tanikawa","FN":"Tomohiro","affil":"University of Tokyo"},{"LN":"Hirose","FN":"Michitaka","affil":"University of Tokyo"}],"year":2016,"journal":"Proceedings of the Fourth International Conference on Human Agent Interaction","references":[2149628368,2137412454,2065704586,2115276604,1975911018,2146355519,2105441954,2162932021,2396107100,1827792418,1983756418,2061131717,2575482212,2138612226,2163551612,2105555732,2051162322,2223446122,2065508755,2260400911,1982726989,2034347979,262974964,1562232511,2149820118,2137975230,1740775935,1999966188,2002746902,2751379301],"citationsCount":0,"abstract":"Agents like human, such as humanoid robots or avatars can be felt as if they have and communicate and communicate due to manipulation of the bodily information. Meanwhile, as in the case of Internet bot, it is still difficult to communiate the emotion described as text, let alone empathizing due to degradation of information online. The current study proposes a method for experiencing emotion on the Internet by reproducing a mechanism of evoking emotion. This method evokes a number of emotions described on the Web, by changing of self-physiological perception with sensory stimuli. To investigate the feasibility of our method, we made a system named \"Communious Mouse.\" This system rewrites the perception of self-skin temperature and pulse in a palm by presenting vibration and thermal stimulation through a mouse device for evoking emotion. The current paper discusses the feasibility of our method based on the obtained feedbacks through an exhibition of the system."},{"id":2928434380,"microsoftAcademicId":2928434380,"numberInSourceReferences":120,"doi":"10.24251/HICSS.2019.509","title":"DYNECOM: Augmenting Empathy in VR with Dyadic Synchrony Neurofeedback","authors":[{"LN":"Järvelä","FN":"Simo","affil":"Aalto University"},{"LN":"Salminen","FN":"Mikko","affil":"Tampere University of Technology"},{"LN":"Ruonala","FN":"Antti","affil":"University of Helsinki"},{"LN":"Timonen","FN":"Janne","affil":"University of Helsinki"},{"LN":"Mannermaa","FN":"Kristiina","affil":"University of Helsinki"},{"LN":"Ravaja","FN":"Niklas","affil":"University of Helsinki"},{"LN":"Jacucci","FN":"Giulio","affil":"Helsinki Institute for Information Technology"}],"year":2019,"journal":"Proceedings of the 52nd Hawaii International Conference on System Sciences","references":[2145585447,2139723353,2300853404,95504984,2006324649,2048813388],"citationsCount":3},{"id":2531188805,"microsoftAcademicId":2531188805,"numberInSourceReferences":78,"doi":"10.1016/J.BICA.2016.09.004","title":"Intelligent emotion and behavior based on topological consciousness and adaptive resonance theory in a companion robot","authors":[{"LN":"Chumkamon","FN":"Sakmongkon","affil":"Kyushu Institute of Technology"},{"LN":"Hayashi","FN":"Eiji","affil":"Kyushu Institute of Technology"},{"LN":"Koike","FN":"Masato","affil":"Kyushu Institute of Technology"}],"year":2016,"journal":"Biologically Inspired Cognitive Architectures","references":[2164598857,2119821739,2125838338,2128728535,2038952578,2152826865,2119859848,2099019320,65738273,2108703536,1977821862,2107544712,2111040806,2131140847,1769974409,2153810084,2163998463,2001381166,3022351671,2094219845,1841887418,1995029578,1966850344,2026575723,187370165,2145867514,1913819253,357604427,2066064791,2177951040,2040870580,2070353225,2001894929,1558214142,2132924377,1979675537,148349441],"citationsCount":11,"abstract":"Abstract  Companion or ‘pet’ robots can be expected to be an important part of a future in which robots contribute to our lives in many ways. An understanding of emotional interactions would be essential to such robots’ behavior. To improve the cognitive and behavior systems of such robots, we propose the use of an artificial topological consciousness that uses a synthetic neurotransmitter and motivation, including a biologically inspired emotion system. A fundamental aspect of a companion robot is a cross-communication system that enables natural interactions between humans and the robot. This paper focuses on three points in the development of our proposed framework: (1) the organization of the behavior including inside-state emotion regarding the phylogenetic consciousness-based architecture; (2) a method whereby the robot can have empathy toward its human user’s expressions of emotion; and (3) a method that enables the robot to select a facial expression in response to the human user, providing instant human-like ‘emotion’ and based on emotional intelligence (EI) that uses a biologically inspired topological online method to express, for example, encouragement or being delighted. We also demonstrate the performance of the artificial consciousness based on the complexity level and a robot’s social expressions that are designed to enhance the users affinity with the robot."},{"id":2477659230,"microsoftAcademicId":2477659230,"numberInSourceReferences":4,"doi":"10.1007/978-3-319-39862-4_28","title":"Empirical Study of Humor Support in Social Human-Robot Interaction","authors":[{"LN":"Bechade","FN":"Lucile","affil":"Université Paris-Saclay"},{"LN":"Duplessis","FN":"Guillaume Dubuisson","affil":"Université Paris-Saclay"},{"LN":"Devillers","FN":"Laurence","affil":"Paris-Sorbonne University"},{"LN":"Devillers","FN":"Laurence","affil":"Université Paris-Saclay"}],"year":2016,"journal":"International Conference on Distributed, Ambient, and Pervasive Interactions","references":[1973933651,2187930957,2082294121,1964926943,2004979235,1995519658,2059380592,2260384878,2082575741,2093183130,1576588741,1994879917,1968765249,1571602109,2089469275,879524043,2028668902],"citationsCount":5,"abstract":"As part of the Joker project which provides a multimodal dialog system with social skills including humor and empathy, this paper explores idea concerning the human verbal responses to a joking robot. Humor support is defined as the conversational strategies used in reaction to humor utterances. This paper aims at exploring the phenomenon of responses to humor interventions from the robot through the examination of a corpus. We assume that using humor in human-robot interaction sets up a positive atmosphere in which participants are willing to contribute. This study relies on 49 human-robot interaction dialogues and 381 adjacency pairs of humorous acts made by the robot and the following human responses. The human humor responses, elicited through canned jokes and conversational humor, were annotated. Three main categories of human responses were found (1) providing no support, (2) recognizing the attempt of humor and (3) contributing with more humor. The findings indicate that, as in human-human interaction, strategies of humor support are strongly dependent of the humorous event’s context."},{"id":2528177402,"microsoftAcademicId":2528177402,"numberInSourceReferences":130,"doi":"10.1007/978-3-319-47437-3_23","title":"Ethically-Guided Emotional Responses for Social Robots: Should I Be Angry?","authors":[{"LN":"Ojha","FN":"Suman","affil":"University of Technology, Sydney"},{"LN":"Williams","FN":"Mary-Anne","affil":"University of Technology, Sydney"}],"year":2016,"journal":"International Conference on Social Robotics","references":[1748703215,2134031328,1984186949,1849553904,1977137834,2143350951,2055894321,2900790966,2104674963,263876593,2146799102,2125906904,1557194112,2130661992,2036445299,1777859530,2077046587,2110241868,1586147214,2025140999,2034327827,2050466241,2125523414,1989864488,238881776,2016893172],"citationsCount":8,"abstract":"Emotions play a critical role in human-robot interaction. Human-robot interaction in social contexts will be more effective if robots can understand human emotions and express (display) emotions accordingly as a means to communicate their own internal state. In this paper we present a novel computational model of robot emotion generation based on appraisal theory and guided by ethical judgement. There have been recent advances in developing emotion for robots. However, despite the extensive research on robot emotion, it is difficult to say if a particular robot is exhibiting appropriate emotions or even showing that it can empathize with humans by exhibiting similar emotions to humans in the same situation. A key question is - to what extent should a robot direct anger toward a young child or an elderly person for an act that it should show anger towards an ordinary adult to signal danger or stupidity? Realizing the need for an ethically guided approach to emotion expressions in social robots as they interact with people, we present a novel Ethical Emotion Generation System (EEGS) for the expression of the most acceptable emotions in social robots."},{"id":3048377509,"microsoftAcademicId":3048377509,"numberInSourceReferences":176,"doi":"10.1109/JIOT.2020.3015381","title":"Multi-modality Sentiment Analysis in Social Internet of Things based on Hierarchical Attentions and CSATTCN with MBM Network","authors":[{"LN":"Xiao","FN":"Guorong"},{"LN":"Tu","FN":"Geng","affil":"Shantou University"},{"LN":"Zheng","FN":"Lin","affil":"Shantou University"},{"LN":"Zhou","FN":"Teng","affil":"Shantou University"},{"LN":"Li","FN":"Xin","affil":"Shantou University"},{"LN":"Ahmed","FN":"Syed Hassan","affil":"[independent researcher, Senior Member, IEEE, USA.]"},{"LN":"Jiang","FN":"Dazhi","affil":"Shantou University"}],"year":2021,"journal":"IEEE Internet of Things Journal","references":[],"citationsCount":7,"abstract":"Multi-modality sentiment analysis in the social internet of things is a developing field, which is basic to empathetic mechanisms, affective computing, and artificial intelligence. Current works in this domain do not explicitly consider the influence of contextual information fusion based on correlation coefficient and memory network with branch structure for sentiment analysis. Unlike present works, this paper presents a Hierarchical Self-attention Fusion (H-SATF) model for capturing contextual information better among utterances, a Contextual Self-attention Temporal Convolutional Network (CSAT-TCN) for the sentiment recognition in social internet of things, and a Multi Branches Memory (MBM) network that stores self-speaker and inter-speaker sentimental states into global memories. For the MOSI datasets, the hybrid H-SATF-CSAT-TCN-MBM model outperforms the state-of-art networks and shows 0.31 9.93% improvement."},{"id":2248429974,"microsoftAcademicId":2248429974,"numberInSourceReferences":147,"doi":"10.1109/WF-IOT.2015.7389088","title":"Affective Internet of Things: Mimicking human-like personality in designing smart-objects","authors":[{"LN":"Pieroni","FN":"Michael","affil":"University of Pisa"},{"LN":"Rizzello","FN":"Lorenzo","affil":"University of Pisa"},{"LN":"Rosini","FN":"Niccolo","affil":"University of Pisa"},{"LN":"Fantoni","FN":"Gualtiero","affil":"University of Pisa"},{"LN":"Rossi","FN":"Danilo De","affil":"University of Pisa"},{"LN":"Mazzei","FN":"Daniele","affil":"University of Pisa"}],"year":2015,"journal":"2015 IEEE 2nd World Forum on Internet of Things (WF-IoT)","references":[2105103777,2026892459,1841352775,3142012400,1989104072,2149628368,24296259,571856966,2244332035,1981288935,2270113935,2074157577,2528881089,2534367192],"citationsCount":6,"citationContext":{"24296259":["However, social and affective communication channels are nowadays extensively exploited in several fields such as marketing [10], networking and also in robotics [3]."],"571856966":["In addition we refer to the Jung’s theory of psychological types [5] and to the Myers-Briggs Type Indicator (MBTI) [9], which"],"1981288935":["Internet of Things [1], [8])."],"1989104072":["However, social and affective communication channels are nowadays extensively exploited in several fields such as marketing [10], networking and also in robotics [3].","In order to represent the in-silico emotional state of the object, we use the Emotional Cartesian Space (ECS) [13] theory that is widely used to represent emotional state in social robotics [3], [7]."],"2026892459":["in 2012 [2]."],"2074157577":["In addition, an ’affective objects can be designed taking into consideration its intrinsic capability to induce attachment [15] that together with a proper functional and aesthetic design [16] [11] could bring to the production of a new generation of emotional and lovable objects."],"2105103777":["Internet of Things [1], [8]).","smart home/office [1]) in which an heterogeneous collection of objects becomes able to collect, exchange, analyze and convey information."],"2149628368":["In order to represent the in-silico emotional state of the object, we use the Emotional Cartesian Space (ECS) [13] theory that is widely used to represent emotional state in social robotics [3], [7]."],"2244332035":["VIPER is an IoT developing suite aimed at programming embedded devices with support for multi-threaded processes and Internet/Cloud connections [6]."],"2270113935":["In order to represent the in-silico emotional state of the object, we use the Emotional Cartesian Space (ECS) [13] theory that is widely used to represent emotional state in social robotics [3], [7]."],"2534367192":["In addition, an ’affective objects can be designed taking into consideration its intrinsic capability to induce attachment [15] that together with a proper functional and aesthetic design [16] [11] could bring to the production of a new generation of emotional and lovable objects."],"3142012400":["Several theories of personality were proposed ([14]) but a com-"]},"abstract":"The paper wants to introduce the concept of Affective Internet of Things (AIoT) where smart objects are empowered with affective capability in terms of abstraction of their emotional state. Moreover each smart object can be associated with a specific ‘personality’. This approach, already used in the field of social robotics, mainly exploits robots' appearance (i.e. anthropomorphism or zoomorphism). The research aims at extending such a paradigm to everyday-life objects in order to ‘warm-up’ the empathic connections that humans generally establish with ‘cold’ gadgets and devices. A new framework for the Affective IoT has been developed: EMPATI (EMPATI Mimics Personalities on Affective Things on Internet). It provides models and functions to simulate different personality for affective objects living in both virtual and real world. Finally, a set of experiments has been conceived to assess the key aspects of the framework in terms of capability to simulate emotional responses depending on the object interaction with the environment and the affective stimuli."},{"id":2959293999,"microsoftAcademicId":2959293999,"numberInSourceReferences":89,"doi":"10.2196/13667","title":"The Effect of Robot Attentional Behaviors on User Perceptions and Behaviors in a Simulated Health Care Interaction: Randomized Controlled Trial","authors":[{"LN":"Johanson","FN":"Deborah L","affil":"University of Auckland"},{"LN":"Ahn","FN":"Ho Seok","affil":"University of Auckland"},{"LN":"MacDonald","FN":"Bruce A","affil":"University of Auckland"},{"LN":"Ahn","FN":"Byeong Kyu","affil":"University of Auckland"},{"LN":"Lim","FN":"JongYoon","affil":"University of Auckland"},{"LN":"Hwang","FN":"Euijun","affil":"University of Auckland"},{"LN":"Sutherland","FN":"Craig J","affil":"University of Auckland"},{"LN":"Broadbent","FN":"Elizabeth","affil":"University of Auckland"}],"year":2019,"journal":"Journal of Medical Internet Research","references":[2087484885,2164935233,2044663075,1926036632,2145978062,2133071520,2124150942,2078841988,2002024051,2042585112,2104635544,3331483,2144576651,2011652428,407492923,2128308912,1792483523,1981509185,1980980822,2464112344,2001951511,2103566745,2080184306,1985527932,1494266680,2171771417,2039163542,2000537164,2171121512,2887919024,2010682154,1981418373,2150701708,2140321956,2903061909,2032702416,2288053165,2016314018,2155864818,1979406688,1999120901,2099847706,2163049580,2175823724,1846663101,1963840460,2049754931,2041528244,2002905670,2027674754,2156612662,2793566021,2067973874,2279784851,1992672921,2772745709,1993507917,2171648021,2079065534],"citationsCount":7,"abstract":"Background: For robots to be effectively used in health applications, they need to display appropriate social behaviors. A fundamental requirement in all social interactions is the ability to engage, maintain, and demonstrate attention. Attentional behaviors include leaning forward, self-disclosure, and changes in voice pitch. Objective: This study aimed to examine the effect of robot attentional behaviors on user perceptions and behaviors in a simulated health care interaction. Methods: A parallel randomized controlled trial with a 1:1:1:1 allocation ratio was conducted. We randomized participants to 1 of 4 experimental conditions before engaging in a scripted face-to-face interaction with a fully automated medical receptionist robot. Experimental conditions included a self-disclosure condition, voice pitch change condition, forward lean condition, and neutral condition. Participants completed paper-based postinteraction measures relating to engagement, perceived robot attention, and perceived robot empathy. We video recorded interactions and coded for participant attentional behaviors. Results: A total of 181 participants were recruited from the University of Auckland. Participants who interacted with the robot in the forward lean and self-disclosure conditions found the robot to be significantly more stimulating than those who interacted with the robot in the voice pitch or neutral conditions (P=.03). Participants in the forward lean, self-disclosure, and neutral conditions found the robot to be significantly more interesting than those in the voice pitch condition (PPP=.01), whereas significantly more participants in the forward lean condition leant toward the robot during the interaction (P<.001). Conclusions: The use of self-disclosure and forward lean by a health care robot can increase human engagement and attentional behaviors. Voice pitch changes did not increase attention or engagement. The small effects with regard to participant perceptions are potentially because of the limitations in self-report measures or a lack of comparison for most participants who had never interacted with a robot before. Further research could explore the use of self-disclosure and forward lean using a within-subjects design and in real health care settings."},{"id":2900111961,"microsoftAcademicId":2900111961,"numberInSourceReferences":38,"doi":"10.1109/ROMAN.2018.8525777","title":"Studying Effects of Incorporating Automated Affect Perception with Spoken Dialog in Social Robots","authors":[{"LN":"Mollahosseini","FN":"Ali","affil":"University of Denver"},{"LN":"Abdollahi","FN":"Hojjat","affil":"University of Denver"},{"LN":"Mahoor","FN":"Mohammad H","affil":"University of Denver"}],"year":2018,"journal":"2018 27th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)","references":[2194775991,1998294030,2526050071,1595732857,2041616772,3122081138,2153480757,1853322427,2124773960,1980083892,2043181832,2064304167,103982469,2053782908,2754425142,1990266238,13388985,1981509185,2124937956,2391561377,258663217,3098002373,2095508859,2135115712,1966378118,2110102748,2048596731,2089917229,2126554711,2016131089,2068036243,2802929279,3101049705,2171552611,2963322563,605667585,1970223698,2899802400],"citationsCount":5,"citationContext":{"13388985":["The EMOTE project’s questionnaire is inspired by some of the dimensions of Interpersonal Reactivity Index (IRI) [17] where it is also claimed to be a proper resource for measuring empathy in socially assistive robots [41]."],"103982469":["Sixteen subjects from the 3rd grade played a total of five chess exercises with the iCat robot over five consecutive weeks [12].","show that empathic agents are perceived as more positive [7] and friendly [8]; lead to more trust [9], less stress [10], helpfulness towards the agent [11]; and can build and sustain long-term socio-emotional relationships with human partners [12]."],"258663217":["agents [21]."],"605667585":["fective empathy (also called emotional empathy, or primitive empathy), where empathy can cause strong vicarious emotion in others, (b) cognitive empathy, where empathy is the awareness, cognitive understanding, or knowing of another’s emotional state, and (c) combination of both affective and cognitive components [18]."],"1595732857":["[5] modeled Kismets emotional system based on Mehrabians"],"1853322427":["It promotes stronger relationships and collaboration and often is one of the elements of morality [3], [4]."],"1966378118":["Social robotics is a rapidly emerging field, which aims to develop robots capable of communicating and interacting with human users in a socio-emotional way [1]."],"1970223698":["show that empathic agents are perceived as more positive [7] and friendly [8]; lead to more trust [9], less stress [10], helpfulness towards the agent [11]; and can build and sustain long-term socio-emotional relationships with human partners [12]."],"1980083892":["Davis (1983) [17] divided these definitions into two categories: (1) Empathy as a process and (2) Empathy as an","The EMOTE project’s questionnaire is inspired by some of the dimensions of Interpersonal Reactivity Index (IRI) [17] where it is also claimed to be a proper resource for measuring empathy in socially assistive robots [41]."],"1981509185":["show that empathic agents are perceived as more positive [7] and friendly [8]; lead to more trust [9], less stress [10], helpfulness towards the agent [11]; and can build and sustain long-term socio-emotional relationships with human partners [12]."],"1990266238":["and social abilities such as mimicking the behaviors of others [14], [15].","investigated how imitation by a robot can affect people’s perceptions of their conversation with it [14]."],"1998294030":["The OpenCV face recognition library was used to detect faces in the images, and 66 landmark points were found using a face alignment algorithm via regression local binary features [38], [39]."],"2016131089":["Several works in the field of social robotics have studied the effect of evoking empathy in users [22], [23], [24]."],"2043181832":["com/ This research is partially supported by grant CNS-1427872 from the National Science Foundation PAD emotion representation [6] (a continuous space of affect model describing the emotional states in three dimensions of Pleasure, Arousal, and Dominance)."],"2048596731":["Several works in the field of social robotics have studied the effect of evoking empathy in users [22], [23], [24]."],"2053782908":["Developing agents with empathic behavior has received considerable attention, especially with virtual agents [10], [19].","show that empathic agents are perceived as more positive [7] and friendly [8]; lead to more trust [9], less stress [10], helpfulness towards the agent [11]; and can build and sustain long-term socio-emotional relationships with human partners [12]."],"2064304167":["Scale has been extensively used in robotic literature to evaluate likability of a robot in different scenarios [43], [44]."],"2068036243":["Several works in the field of social robotics have studied the effect of evoking empathy in users [22], [23], [24]."],"2089917229":["Likability of the robot: Questions 20-23 estimate the likability of the robot, borrowed from Reysen Likability Scale [42]."],"2095508859":["show that empathic agents are perceived as more positive [7] and friendly [8]; lead to more trust [9], less stress [10], helpfulness towards the agent [11]; and can build and sustain long-term socio-emotional relationships with human partners [12]."],"2110102748":["show that empathic agents are perceived as more positive [7] and friendly [8]; lead to more trust [9], less stress [10], helpfulness towards the agent [11]; and can build and sustain long-term socio-emotional relationships with human partners [12].","titudes towards robots [9]."],"2124773960":["Numerous computer vision and machine learning algorithms have been proposed in the literature for automated Facial Expression Recognition (FER) [31]."],"2124937956":["These agents are used for helping children to deal with bullying [20] and intercultural training with culturally configurable"],"2126554711":["They mostly have a limited user-dependent affect perception module which is trained on a specific population of the study in a lab-controlled setting [16]."],"2135115712":["and social abilities such as mimicking the behaviors of others [14], [15]."],"2153480757":["According to [4] empathy reaction can be a function of three factors:","It promotes stronger relationships and collaboration and often is one of the elements of morality [3], [4].","Preston and De Waal [4] unified different perspectives of empathy by defining it as a “Perception-Action Model”","They defined empathy as the capacity to (a) share another’s emotional state or be affected by it, (b) assess the reasons of emotional state, and (c) identify and adopt other perspectives [4]."],"2171552611":["Scale has been extensively used in robotic literature to evaluate likability of a robot in different scenarios [43], [44]."],"2194775991":["Adding the shortcut connection eases the training of deeper networks (more than 100 layers) and avoid degradation problem (the phenomenon that accuracy gets saturated and then degrades rapidly [35]).","Since we only study four facial expressions in this work, we trained a 50-layers Residual Network (ResNet) [35] on five classes of neutral, happy, surprise, sad, and disgust of affectNet database.","the-art performance in several computer vision applications such as visual object detection [35], audio classification [36], and facial expression recognition [37]."],"2391561377":["Recently, databases of facial expression and affect in the wild received much attention [32], [33]."],"2526050071":["the-art performance in several computer vision applications such as visual object detection [35], audio classification [36], and facial expression recognition [37]."],"2754425142":["Developing agents with empathic behavior has received considerable attention, especially with virtual agents [10], [19].","Studies on empathy in social agents can be divided into two approaches [19].","The EMOTE questionnaire has been used in short and long-term studies in the EMOTE project, both for individual interactions (one person and one robot) and multiuser interactions [19]."],"2802929279":["Ryan has been used in several studies [28], [29], [30]."],"2899802400":["Ryan has been used in several studies [28], [29], [30]."],"2963322563":["the-art performance in several computer vision applications such as visual object detection [35], audio classification [36], and facial expression recognition [37]."],"3098002373":["Ryan has been used in several studies [28], [29], [30].","[30] for more details on the hardware and software of the robot."],"3101049705":["Using actuators to build a natural face capable of showing visual speech and more importantly different emotions is difficult and expensive [27]."],"3122081138":["In this work, we used a newly released database of facial Affect from the InterNet (called AffectNet) which is publicly available to the research community [34]."]},"abstract":"Social robots are becoming an integrated part of our daily lives with the goal of understanding humans' social intentions and feelings, a capability which is often referred to as empathy. Despite significant progress towards the development of empathic social agents, current social robots have yet to reach the full emotional and social capabilities. This paper presents our recent effort on incorporating an automated Facial Expression Recognition (FER) system based on deep neural networks into the spoken dialog of a social robot (Ryan) to extend and enrich its capabilities beyond spoken dialog and integrate the user's affect state into the robot's responses. In order to evaluate whether this incorporation can improve social capabilities of Ryan, we conducted a series of Human-Robot-Interaction (HRI) experiments. In these experiments the subjects watched some videos and Ryan engaged them in a conversation driven by user's facial expressions perceived by the robot. We measured the accuracy of the automated FER system on the robot when interacting with different human subjects as well as three social/interactive aspects, namely task engagement, empathy, and likability of the robot. The results of our HRI study indicate that the subjects rated empathy and likability of the affect-aware Ryan significantly higher than non-empathic (the control condition) Ryan. Interestingly, we found that the accuracy of the FER system is not a limiting factor, as subjects rated the affect-aware agent equipped with a low accuracy FER system as empathic and likable as when facial expression was recognized by a human observer."},{"id":2564450351,"microsoftAcademicId":2564450351,"numberInSourceReferences":44,"doi":"10.1109/LARS-SBR.2016.13","title":"An Emotion-Based Interaction Strategy to Improve Human-Robot Interaction","authors":[{"LN":"Ranieri","FN":"Caetano Mazzoni","affil":"University of São Paulo"},{"LN":"Romero","FN":"Roseli A.F.","affil":"University of São Paulo"}],"year":2016,"journal":"2016 XIII Latin American Robotics Symposium and IV Brazilian Robotics Symposium (LARS/SBR)","references":[2168405694,2103943262,2076017598,2150283722,2131799829,2000120390,1489323111,1985855315,2003350835,3121604639,2022964551,2132143241,2077711093,1999540005,2565387335],"citationsCount":5,"abstract":"Emotion and empathy are key subjects on human-robot interaction, especially regarding social robots. Several studies have investigated emotional reactions of humans toward robots, while others deal with development of actual systems to analyze how affective feedback may influence this kind of interaction. This paper presents an emotion-aware interaction strategy applied to an embodied virtual agent, implemented as an Android application. The system assigns two distinct paradigms to the virtual character, according to the user's emotion, inferred through facial expressions analysis. Within subject user experiments have been performed, in order to evaluate if the proposed strategy improves empathy and pleasantness."},{"id":3093260290,"microsoftAcademicId":3093260290,"numberInSourceReferences":83,"doi":"10.1038/S41598-020-74438-6","title":"Ordered interpersonal synchronisation in ASD children via robots.","authors":[{"LN":"Giannopulu","FN":"Irini","affil":"Bond University"},{"LN":"Etournaud","FN":"Aude","affil":"Interdisciplinary Centre for the Artificial Mind"},{"LN":"Terada","FN":"Kazunori","affil":"Gifu University"},{"LN":"Velonaki","FN":"Mari","affil":"University of New South Wales"},{"LN":"Watanabe","FN":"Tomio","affil":"Okayama University"}],"year":2020,"journal":"Scientific Reports","references":[2087484885,2073825915,2050363967,2323648988,2044954931,1769013118,2136179834,1984675555,2088779903,1976472487,1979877610,2000181935,2155034734,2148745188,2026473386,2790350641,2042831497,2154855355,165886685,2188793210,2077597530,1981658442,2019242682,2076787172,2088321173,2514439528,2202330825,2606876479,2565407566,2279878284,2758432130,2083645227,2782996557,2787391982,1658121921,13022595,2253955724,2552781363,1968711994,2396366411,2897433896,2885486127],"citationsCount":1,"abstract":"Children with autistic spectrum disorders (ASD) experience persistent disrupted coordination in interpersonal synchronisation that is thought to be associated with deficits in neural connectivity. Robotic interventions have been explored for use with ASD children worldwide revealing that robots encourage one-to-one social and emotional interactions. However, associations between interpersonal synchronisation and emotional empathy have not yet been directly explored in French and Japanese ASD children when they interact with a human or a robot under analogous experimental conditions. Using the paradigm of actor-perceiver, where the child was the actor and the robot or the human the perceiver, we recorded the autonomic heart rate activation and reported emotional feelings of ASD children in both countries. Japanese and French ASD children showed different interpersonal synchronisation when they interacted with the human perceiver, even though the human was the same in both countries. However, they exhibited similar interpersonal synchronisation when the perceiver was the robot. The findings suggest that the mechanism combining interpersonal synchronisation and emotional empathy might be weakened but not absent in ASD children and that both French and Japanese ASD children do spontaneously and unconsciously discern non verbal actions of non human partners through a direct matching process that occurs via automatic mapping."},{"id":2791907682,"microsoftAcademicId":2791907682,"numberInSourceReferences":30,"doi":"10.1007/978-3-319-76270-8_6","title":"Development and Evaluation of an Interactive Therapy Robot","authors":[{"LN":"Kohori","FN":"Tomoko","affil":"Dai Nippon Printing"},{"LN":"Hirayama","FN":"Shiho","affil":"Dai Nippon Printing"},{"LN":"Hara","FN":"Takenori","affil":"Dai Nippon Printing"},{"LN":"Muramatsu","FN":"Michiko","affil":"Dai Nippon Printing"},{"LN":"Naganuma","FN":"Hiroyuki","affil":"Dai Nippon Printing"},{"LN":"Yamano","FN":"Masayuki","affil":"Joshibi University of Art and Design"},{"LN":"Ichikawa","FN":"Kazuko","affil":"Joshibi University of Art and Design"},{"LN":"Matsumoto","FN":"Hiroko","affil":"Joshibi University of Art and Design"},{"LN":"Uchiyama","FN":"Hiroko","affil":"Joshibi University of Art and Design"}],"year":2017,"journal":"Advances in Computer Entertainment Technology: 14th International Conference, ACE 2017 London, UK, December 14–16, 2017 Proceedings, 2018, ISBN 978-3-319-76270-8, págs. 66-83","references":[2162148733,2124135442,1991797265,1974224195,2320301523,1995905345,640458273,2334149947,2076059783,2513443863],"citationsCount":4,"abstract":"Interactions with animals can enhance emotions and improve mood by engendering feelings of healing, relaxation, comfort, and reduced stress. Un-fortunately, many people cannot live with animals because of allergies, infection risk, or risk of damage to rental housing. To address these problems, some research groups have investigated robot-based psychotherapy. However, the important healing elements for therapy robots were not identified. Therefore, we conducted an Internet survey to determine the design elements of such a robot that might engender a healing mood and the functions that should be implemented. We assumed that a healing mood could be induced based on the interactive functions and appearance. To verify this hypothesis, we developed and evaluated a new interactive therapy robot. Next, we conducted interviews with individuals who interacted with a prototype therapy robot. The interviews revealed that the appearance of the robot was critical to engendering feelings of healing, comfort, and empathy. In addition, the size, softness, and comfort of the interactive therapy robot contributed to people feeling affection towards it. We also confirmed the importance of the robot appearing to listen to those who interacted with it. Our results should be useful for designing companion robots for therapy purposes."},{"id":3135908719,"microsoftAcademicId":3135908719,"numberInSourceReferences":98,"doi":"10.1002/MAR.21475","title":"Attachment styles moderate customer responses to frontline service robots: Evidence from affective, attitudinal, and behavioral measures","authors":[{"LN":"Pozharliev","FN":"Rumen","affil":"Libera Università Internazionale degli Studi Sociali Guido Carli"},{"LN":"Angelis","FN":"Matteo De","affil":"Libera Università Internazionale degli Studi Sociali Guido Carli"},{"LN":"Rossi","FN":"Dario","affil":"Libera Università Internazionale degli Studi Sociali Guido Carli"},{"LN":"Romani","FN":"Simona","affil":"Libera Università Internazionale degli Studi Sociali Guido Carli"},{"LN":"Verbeke","FN":"Willem","affil":"Erasmus University Rotterdam"},{"LN":"Cherubino","FN":"Patrizia","affil":"BrainSigns s.r.l. Rome Italy"}],"year":2021,"journal":"Psychology & Marketing","references":[2084235337,2135907989,2078831478,1991015565,2894528744,2558287871,1767586509,2038041609,2064304167,2010943014,1681671561,2941128398,2094482505,2036974889,3121353240,2035833057,1967097368,1984842352,2751092903,2315596314,2747740261,2754425142,2767537599,2075132641,2011193616,2805041655,2060764895,2147270755,2594438400,2018664421,2080595165,2103093506,3003753339,2034951011,2012192938,2555162129,2110102748,2940226795,2344453698,2997061806,2605905685,2966519442,2963341140,2952763485,2911718048,2064899380,57640832,2051226867,2037689509,2893146054,2125486193,2896893551,2112883678,3002137173,3047873411,2565522027,1978227907,3001471445,2765399109,3147106146,2758264485,2067404519],"citationsCount":0},{"id":2059546446,"microsoftAcademicId":2059546446,"numberInSourceReferences":69,"doi":"10.1016/J.ACTAASTRO.2015.01.010","title":"Humanly space objects—Perception and connection with the observer","authors":[{"LN":"Balint","FN":"Tibor S.","affil":"Royal College of Art"},{"LN":"Hall","FN":"Ashley","affil":"Royal College of Art"}],"year":2015,"journal":"Acta Astronautica","references":[2613049552,1528027857,1933657216,2101215876,1515263424,1606540910,2157904933,1580568969,2465836925,3128638433,2128666650,1527571715,2261500961,1409979095,8545369],"citationsCount":5,"citationContext":{"1515263424":["Over the past century technological innovation drove societal advancements, especially in first world countries [4]."],"1580568969":["In connection with our paper's topic, we will briefly discuss foundational concepts, including Polanyi's tacit knowledge [28], Piaget's schema [27,29], Wiener's cybernetics [32], and Gibson's affordances [6,7].","In this paper we will first introduce a range of foundational concepts, such as tacit knowledge by Michael Polanyi [28], cognitive development by Jean Piaget [27,29], cybernetics and its circularity by Norbert Wiener [32], variety by William Ross Ashby [3], and affordances by James Gibson [6,7] and Don Norman [22,23].","The term “affordance” was introduced by the American psychologist James Gibson in his article, titled “The Theory of Affordances” [7]."],"1606540910":["In connection with our paper's topic, we will briefly discuss foundational concepts, including Polanyi's tacit knowledge [28], Piaget's schema [27,29], Wiener's cybernetics [32], and Gibson's affordances [6,7].","In this paper we will first introduce a range of foundational concepts, such as tacit knowledge by Michael Polanyi [28], cognitive development by Jean Piaget [27,29], cybernetics and its circularity by Norbert Wiener [32], variety by William Ross Ashby [3], and affordances by James Gibson [6,7] and Don Norman [22,23]."],"2157904933":["A schema organizes categories of information and the relationships within Piaget [27].","In connection with our paper's topic, we will briefly discuss foundational concepts, including Polanyi's tacit knowledge [28], Piaget's schema [27,29], Wiener's cybernetics [32], and Gibson's affordances [6,7].","In his book, titled “Tacit dimensions”, Michael Polanyi introduced the term “tacit knowledge” as opposed to explicit knowledge [27].","In this paper we will first introduce a range of foundational concepts, such as tacit knowledge by Michael Polanyi [28], cognitive development by Jean Piaget [27,29], cybernetics and its circularity by Norbert Wiener [32], variety by William Ross Ashby [3], and affordances by James Gibson [6,7] and Don Norman [22,23].","The theory of cognition and human intelligence development was first constructed by Jean Piaget, a Swiss developmental psychologist [27,29]."],"2261500961":["These findings are also reported in [26].","[26] and Ortiz [25]."],"3128638433":["(A more exhaustive list of stressors is provided in [15].","In addition, NASA and other national space agencies studied the psychological and medical impact of the space environment on the crew throughout the course of longduration spaceflight [15]."]},"abstract":"Expanding humanity into space is an inevitable step in our quest to explore our world. Yet space exploration is costly, and the awaiting environment challenges us with extreme cold, heat, vacuum and radiation, unlike anything encountered on Earth. Thus, the few pioneers who experience it needed to be well protected throughout their spaceflight. The resulting isolation heightens the senses and increases the desire to make humanly connections with any other perceived manifestation of life. Such connections may occur via sensory inputs, namely vision, touch, sound, smell, and taste. This then follows the process of sensing, interpreting, and recognizing familiar patterns, or learning from new experiences. The desire to connect could even transfer to observed objects, if their movements and characteristics trigger the appropriate desires from the observer. When ordered in a familiar way, for example visual stimuli from lights and movements of an object, it may create a perceived real bond with an observer, and evoke the feeling of surprise when the expected behavior changes to something no longer predictable or recognizable. These behavior patterns can be designed into an object and performed autonomously in front of an observer, in our case an astronaut. The experience may introduce multiple responses, including communication, connection, empathy, order, and disorder. While emotions are clearly evoked in the observer and may seem one sided, in effect the object itself provides a decoupled bond, connectivity and communication between the observer and the artist-designer of the object. In this paper we will discuss examples from the field of arts and other domains, including robotics, where human perception through object interaction was explored, and investigate the starting point for new innovative design concepts and future prototype designs, that extend these experiences beyond the boundaries of Earth, while taking advantage of remoteness and the zero gravity environment. Through a form of emotional connection and design, these concepts will focus on the connection and brief emotional bond between a humanly animate object in space and a co-located observer in spaceflight. We conclude that beyond providing creative expressions for humanly contacts, these experiences may also provide further insights into human perception in spaceflight, and could be tested on the International Space Station, and serve as a stepping-stone towards use on long-duration spaceflight to Mars."},{"id":2962480806,"microsoftAcademicId":2962480806,"numberInSourceReferences":34,"doi":"10.1109/FG.2019.8756517","title":"Performance Analysis of Unimodal and Multimodal Models in Valence-Based Empathy Recognition","authors":[{"LN":"Mallol-Ragolta","FN":"Adria","affil":"University of Augsburg"},{"LN":"Schmitt","FN":"Maximilian","affil":"University of Augsburg"},{"LN":"Baird","FN":"Alice","affil":"University of Augsburg"},{"LN":"Cummins","FN":"Nicholas","affil":"University of Augsburg"},{"LN":"Schuller","FN":"Bjorn","affil":"University of Augsburg"}],"year":2019,"journal":"2019 14th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2019)","references":[2557283755,2325939864,2085662862,2395639500,2513140567,2239141610,2399733683,2081074144,2278113816,2164186291,2005418748,2751214333,2163928333,2027508313,2787459734,2053907973,567437002,2897444637,2765860599,2149940198,2341536223,1976235033,2040246121,2963382311,2513404240,2332701278,2620769954,2047305901,2750249781,2747610795,2897782875,2515456817,2901737425,2784591842,2577914755,2783878045],"citationsCount":2,"citationContext":{"567437002":["Results computed on the development set for generalised and personalised unimodal and multimodal models with LSTM units ∈ [30, 40, 50] are presented in Table II.","The architecture of the neural network we propose employs a BLSTM layer with M ∈ [30, 40, 50] LSTM units, which is optimised on the development set, followed by a dense layer with a single unit, which outputs the valence predictions at every time step.","of speech [30]."],"1976235033":["In future work, we plan to explore the benefits of realigning the audio-visual and self-assessed information using annotation delay compensation [17]."],"2005418748":["visual information should be more effective than the utilisation of one modality alone, as suggested in previous emotion detection research [37], [25]."],"2027508313":["In the field of medicine, empathy towards the patient is a vital part of the care-giving process [16]."],"2040246121":["the ability to empathise is often overlooked by medical professionals in favour of technological advancements in diagnosis and treatment [36]."],"2047305901":["In psychotherapy, for instance, treatments from therapists with high empathy towards their patients have shown to be more effective compared to those from therapists with low empathy [18], [21]."],"2053907973":["Due to the relevance of facial information when judging behavioural cues [1], we hypothesise that listeners’ faces might contain information related to their level of empathy."],"2081074144":["Speaker diarisation [4] could also be a conducive next step, concerning the indi-"],"2085662862":["OPENSMILE software [12].","We employ OPENSMILE [12] to extract the extended Geneva minimalistic acoustic parameter set (eGeMAPS) [11] from the speech signals, since it has been very successful in similar affective computing tasks,"],"2149940198":["visual information should be more effective than the utilisation of one modality alone, as suggested in previous emotion detection research [37], [25]."],"2163928333":["As visual features, we extract facial action units (FAUs) using OPENFACE [3], [6], because FAUs have been shown to be an effective method for recognising a variety of emotional states [9], [28], [27]."],"2164186291":["successfully used on valence prediction problems [22], this is a surprising result, which allows us to argue about the suitability of the visual feature set employed in this work for the automatic recognition of empathy."],"2239141610":["The acoustic features that we extract from the segmented audio frames correspond to the 88 features defined by the eGeMAPS feature set [11].","We employ OPENSMILE [12] to extract the extended Geneva minimalistic acoustic parameter set (eGeMAPS) [11] from the speech signals, since it has been very successful in similar affective computing tasks,"],"2278113816":["Previous works concerning the automatic prediction of affective information from multimodal data employed different solutions to overcome the disparity in sampling rates [31], [24].","literature [31]."],"2332701278":["approaches analysing empathy from audio- and video-based features have resulted in promising findings [39] due to the richness in emotional content of both modalities."],"2341536223":["With this in mind, research efforts towards empathy prediction have explored the use of audio alone [38], in part, due to the success achieved in the field of computational paralinguistics"],"2395639500":["As visual features, we extract facial action units (FAUs) using OPENFACE [3], [6], because FAUs have been shown to be an effective method for recognising a variety of emotional states [9], [28], [27]."],"2399733683":["As outlined, eGeMAPS is a feature set widely used in the affective computing literature [34], [26], [29], [10], and it is extracted using the open-source"],"2513140567":["As visual features, we extract facial action units (FAUs) using OPENFACE [3], [6], because FAUs have been shown to be an effective method for recognising a variety of emotional states [9], [28], [27].","from the listener’s face using the OPENFACE software [3],"],"2513404240":["As outlined, eGeMAPS is a feature set widely used in the affective computing literature [34], [26], [29], [10], and it is extracted using the open-source"],"2515456817":["such as depression recognition [32], [35]."],"2557283755":["long short-term memory networks (BLSTM) [14] to capture valence-based empathy time-dependencies."],"2577914755":["Previous works concerning the automatic prediction of affective information from multimodal data employed different solutions to overcome the disparity in sampling rates [31], [24]."],"2620769954":["As outlined, eGeMAPS is a feature set widely used in the affective computing literature [34], [26], [29], [10], and it is extracted using the open-source"],"2747610795":["Nevertheless, as it has been shown that the modelling of facial information is vital in the context of empathy [33], in recent years, contributions towards empathy prediction have also come from the domain of computer vision [15], [7]."],"2750249781":["such as depression recognition [32], [35]."],"2751214333":["As visual features, we extract facial action units (FAUs) using OPENFACE [3], [6], because FAUs have been shown to be an effective method for recognising a variety of emotional states [9], [28], [27]."],"2765860599":["As outlined, eGeMAPS is a feature set widely used in the affective computing literature [34], [26], [29], [10], and it is extracted using the open-source"],"2784591842":["Researchers in the field of human-robot interaction (HRI) have envisioned the use of automatic empathy prediction systems to improve HRI [13], [5]."],"2787459734":["trait in improving interpersonal relationships [19]."],"2897444637":["As visual features, we extract facial action units (FAUs) using OPENFACE [3], [6], because FAUs have been shown to be an effective method for recognising a variety of emotional states [9], [28], [27]."],"2897782875":["Nevertheless, as it has been shown that the modelling of facial information is vital in the context of empathy [33], in recent years, contributions towards empathy prediction have also come from the domain of computer vision [15], [7]."],"2901737425":["alised models might not be able to capture intra-subject dependencies on the perception of empathy [20], we also train personalised models, which are trained exclusively with interactions corresponding to a particular listener, to solve the same task."],"2963382311":["Researchers in the field of human-robot interaction (HRI) have envisioned the use of automatic empathy prediction systems to improve HRI [13], [5]."]},"abstract":"The human ability to empathise is a core aspect of successful interpersonal relationships. In this regard, human-robot interaction can be improved through the automatic perception of empathy, among other human attributes, allowing robots to affectively adapt their actions to interactants’ feelings in any given situation. This paper presents our contribution to the generalised track of the One-Minute Gradual (OMG) Empathy Prediction Challenge by describing our approach to predict a listener’s valence during semi-scripted actor-listener interactions. We extract visual and acoustic features from the interactions and feed them into a bidirectional long short-term memory network to capture the time-dependencies of the valence-based empathy during the interactions. Generalised and personalised unimodal and multimodal valence-based empathy models are then trained to assess the impact of each modality on the system performance. Furthermore, we analyse if intra-subject dependencies on empathy perception affect the system performance. We assess the models by computing the concordance correlation coefficient (CCC) between the predicted and self-annotated valence scores. The results support the suitability of employing multimodal data to recognise participants’ valence-based empathy during the interactions, and highlight the subject-dependency of empathy. In particular, we obtained our best result with a personalised multimodal model, which achieved a CCC of 0.11 on the test set."},{"id":2981376758,"microsoftAcademicId":2981376758,"numberInSourceReferences":73,"doi":"10.1007/S12369-019-00599-8","title":"Perceived Mistreatment and Emotional Capability Following Aggressive Treatment of Robots and Computers","authors":[{"LN":"Carlson","FN":"Zachary","affil":"University of Nevada, Reno"},{"LN":"Lemmon","FN":"Louise","affil":"University of Nevada, Reno"},{"LN":"Higgins","FN":"MacCallister","affil":"University of Nevada, Reno"},{"LN":"Frank","FN":"David","affil":"University of Nevada, Reno"},{"LN":"Shahrezaie","FN":"Roya Salek","affil":"University of Nevada, Reno"},{"LN":"Feil-Seifer","FN":"David","affil":"University of Nevada, Reno"}],"year":2019,"journal":"International Journal of Social Robotics","references":[1841352775,2108703536,2137099741,1539282749,2138130910,2062845262,2132049110,2103530214,2105569914,2108626787,1979628236,2157105625,2148142846,2003350835,2082490605,2152536828,3142599250,1589998457,2035874886,2127135847,2497161031,2792480533,1979597072,2282380228,3143158348,2148999704,2094223914,1977176245,2143910019,2552988712,2244429805],"citationsCount":7,"abstract":"Robots (and computers) are increasingly being used in scenarios where they interact socially with people. How people react to these agents is telling about the perceived empathy of such agents. Mistreatment of robots (or computers) by co-workers might provoke such telling reactions. This study examines perceived mistreatment directed towards a robot in comparison to a computer. This will provide some understanding of how people feel about robots in collaborative social settings. We conducted a two by two between-subjects study with 80 participants. Participants worked cooperatively with either a robot or a computer agent. An experiment confederate would either act aggressively or neutrally towards the agent. We hypothesized that people would not perceive aggressive speech as mistreatment when an agent was capable of emotional feelings and similar to themselves; that participants would perceive the robot as more similar in appearance and emotionally capable to themselves than a computer; and so would observe more mistreatment with a robot. The final results supported our hypotheses; the participants observed greater mistreatment for the robot, but not the computer. Also participants felt significantly more sympathetic towards the robot and believed that it was much more emotionally capable."},{"id":3157600114,"microsoftAcademicId":3157600114,"numberInSourceReferences":174,"doi":"10.1007/S00146-021-01214-Z","title":"Can communication with social robots influence how children develop empathy? Best-evidence synthesis","authors":[{"LN":"Pashevich","FN":"Ekaterina","affil":"University of Oslo"}],"year":2021,"journal":"Ai & Society","references":[2079283960,1841352775,1595732857,2111040806,2912483755,2001862949,2325035506,2886956600,2261178860,2095899461,2037289888,2108615342,2136132767,13388985,2142190957,3080186923,2595857079,2616358185,2960118633,2948638627,2808512042,2736270522,2745000941,2112351484,2592689179,2899506378,2060349841,2479984892,3008789378,2012032379,2799638175,2981700299,2559431840,2922133732,1529017456,134062113,2163805431,2919701584,2002775149,2808836675,2913503318,2333303458,3018414480,2971413511,2726890133,2950549049,2029356420,2592980642,2934483775,3048638976],"citationsCount":1,"abstract":"Social robots are gradually entering children’s lives in a period when children learn about social relationships and exercise prosocial behaviors with parents, peers, and teachers. Designed for long-term emotional engagement and to take the roles of friends, teachers, and babysitters, such robots have the potential to influence how children develop empathy. This article presents a review of the literature (2010–2020) in the fields of human–robot interaction (HRI), psychology, neuropsychology, and roboethics, discussing the potential impact of communication with social robots on children’s social and emotional development. The critical analysis of evidence behind these discussions shows that, although robots theoretically have high chances of influencing the development of empathy in children, depending on their design, intensity, and context of use, there is no certainty about the kind of effect they might have. Most of the analyzed studies, which showed the ability of robots to improve empathy levels in children, were not longitudinal, while the studies observing and arguing for the negative effect of robots on children’s empathy were either purely theoretical or dependent on the specific design of the robot and the situation. Therefore, there is a need for studies investigating the effects on children’s social and emotional development of long-term regular and consistent communication with robots of various designs and in different situations."},{"id":3158476941,"microsoftAcademicId":3158476941,"numberInSourceReferences":172,"doi":"10.1038/S41598-021-88622-9","title":"Promises and trust in human-robot interaction.","authors":[{"LN":"Cominelli","FN":"Lorenzo","affil":"University of Pisa"},{"LN":"Feri","FN":"Francesco","affil":"Royal Holloway, University of London"},{"LN":"Garofalo","FN":"Roberto","affil":"University of Pisa"},{"LN":"Giannetti","FN":"Caterina","affil":"University of Pisa"},{"LN":"Meléndez-Jiménez","FN":"Miguel A.","affil":"University of Málaga"},{"LN":"Greco","FN":"Alberto","affil":"University of Pisa"},{"LN":"Nardelli","FN":"Mimma","affil":"University of Pisa"},{"LN":"Scilingo","FN":"Enzo Pasquale","affil":"University of Pisa"},{"LN":"Kirchkamp","FN":"Oliver","affil":"University of Jena"}],"year":2021,"journal":"Scientific Reports","references":[2339343773,2026699230,2029334490,2045633722,2135907989,2105938655,2149628368,2799347062,1991015565,2095953405,3123273141,1594303435,1239497932,1575023497,2794740256,3125705609,1492443485,3122331528,1969004079,1520861770,2074134568,2568738663,2604175534,2146163914,2329439512,2530352241,2109923073,2025637627,2060349841,3019325610,3013778231,2917979491,2166442339,2103632254,2497635027,1495438814,2616194516,2792400400,2944157668,1608468640,2791452829,2270113935,2132810577,2914385037,1986300201,2899095794,2985326275,2951801228,116290630,1000719765,2997593515,3150639393,2014048615,2960208674,2478564110,2107910420,3126009810,2051400058,3124220107,1583880184],"citationsCount":0,"abstract":"Understanding human trust in machine partners has become imperative due to the widespread use of intelligent machines in a variety of applications and contexts. The aim of this paper is to investigate whether human-beings trust a social robot-i.e. a human-like robot that embodies emotional states, empathy, and non-verbal communication-differently than other types of agents. To do so, we adapt the well-known economic trust-game proposed by Charness and Dufwenberg (2006) to assess whether receiving a promise from a robot increases human-trust in it. We find that receiving a promise from the robot increases the trust of the human in it, but only for individuals who perceive the robot very similar to a human-being. Importantly, we observe a similar pattern in choices when we replace the humanoid counterpart with a real human but not when it is replaced by a computer-box. Additionally, we investigate participants' psychophysiological reaction in terms of cardiovascular and electrodermal activity. Our results highlight an increased psychophysiological arousal when the game is played with the social robot compared to the computer-box. Taken all together, these results strongly support the development of technologies enhancing the humanity of robots."},{"id":2929048143,"microsoftAcademicId":2929048143,"numberInSourceReferences":47,"doi":"10.1109/HRI.2019.8673326","title":"Good robot design or machiavellian?: an in-the-wild robot leveraging minimal knowledge of passersby's culture","authors":[{"LN":"Sanoubari","FN":"Elaheh","affil":"University of Manitoba"},{"LN":"Seo","FN":"Stela H.","affil":"University of Manitoba"},{"LN":"Garcha","FN":"Diljot","affil":"University of Manitoba"},{"LN":"Young","FN":"James E.","affil":"University of Manitoba"},{"LN":"Loureiro-Rodriguez","FN":"Veronica","affil":"University of Manitoba"}],"year":2019,"journal":"Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction","references":[2081228936,2140918013,2099019320,2164391693,2032160998,2131799829,2901391803,1982887619,1976660112,2065336436,2064304167,2097404405,1834533132,2805647790,2112065758,2060981264,2002417021,2123952992,2084715379,2345071775,2122310820,2157582551,2090196591,2134818908,2591951582,2050747765,2139730678,2282380228,2310536335,2003276682,1996210527,2550668593,46530081,2137789046,2123808109,2794001962,2745199108,1613462424,2771151535,2009096656,2104300506,1533447652,2049044678,2068458270,2765177538,2741617675,2898173725],"citationsCount":5,"citationContext":{"1533447652":["Further, as noted in Section 2, some robots are explicitly designed to be persuasive, for goals that the user may have subscribed to (such as saving energy [26])."],"1613462424":[", [8,27,42–44,46]), and uncovering culture-specific expectations and design preferences [35,36,45]."],"1976660112":["[58]"],"1982887619":["Similarly, people may develop inauthentic relationships with social care robots, which do not actually reciprocate caring and are only programmed to do so [56,65]."],"1996210527":[", [8,27,42–44,46]), and uncovering culture-specific expectations and design preferences [35,36,45]."],"2002417021":["It underlies various aspects of our social behaviors, affects our reasoning style [32], and shapes what we deem appropriate in our interactions with others."],"2009096656":["Robots as team members can purposefully shape and influence social dynamics in a group, and between human team members, to improve team effectiveness [33,53]."],"2032160998":["Similarly, people may develop inauthentic relationships with social care robots, which do not actually reciprocate caring and are only programmed to do so [56,65]."],"2049044678":["Culture varies wildly across the world and between social groups, and is recognized as a key variable in understanding social behavior [10,39]."],"2060981264":["While the majority of social HRI research has been carried out in a traditional lab setting [9], the merits of conducting experiments in ecologically valid, in-the-wild contexts are being increasingly recognized [34,52]."],"2064304167":[", [11,50,67,68]), for example, a person’s culture can determine how a robot’s communication style impacts credibility [49], or how they rate a robot’s social acceptance, likeability and trustworthiness [3,42]."],"2068458270":[", [11,50,67,68]), for example, a person’s culture can determine how a robot’s communication style impacts credibility [49], or how they rate a robot’s social acceptance, likeability and trustworthiness [3,42].",", [8,27,42–44,46]), and uncovering culture-specific expectations and design preferences [35,36,45]."],"2084715379":[", [11,50,67,68]), for example, a person’s culture can determine how a robot’s communication style impacts credibility [49], or how they rate a robot’s social acceptance, likeability and trustworthiness [3,42]."],"2090196591":["Cultural background tends to influence communication style preferences [23], which correlate well with individualism: collectivist cultures tend to prefer implicit and indirect communication, and to avoid conflict, whereas individualist cultures prefer explicit and direct communication."],"2097404405":["These social robots often have anthropomorphic or zoomorphic designs and use human-like gestures, speech, and behaviors to interact with people [70]."],"2099019320":["This can be quite positive, as social interaction is fundamental for user comfort [19] and, if done well, can help a robot be easy to understand [28]."],"2104300506":[", [11,50,67,68]), for example, a person’s culture can determine how a robot’s communication style impacts credibility [49], or how they rate a robot’s social acceptance, likeability and trustworthiness [3,42]."],"2112065758":["This can be quite positive, as social interaction is fundamental for user comfort [19] and, if done well, can help a robot be easy to understand [28]."],"2122310820":[", [11,50,67,68]), for example, a person’s culture can determine how a robot’s communication style impacts credibility [49], or how they rate a robot’s social acceptance, likeability and trustworthiness [3,42].","Furthermore, similar to people, the robot itself can act differently in different cultures, to impact how it is perceived [27,35] and how much people trust its opinion [3,67]."],"2123808109":[", see [14,25,30,60])."],"2123952992":["For example, some designs aim to use social interaction techniques to modify human behavior for an ostensibly desirable goal, such as to encourage energy savings [41] or to support health management [37,38]."],"2131799829":["Sam is remotely operated using the Wizard of Oz method [51], although people are led to believe it is autonomous."],"2137789046":["We use a backstory to help encourage engagement and believability [59]."],"2139730678":["For example, some designs aim to use social interaction techniques to modify human behavior for an ostensibly desirable goal, such as to encourage energy savings [41] or to support health management [37,38]."],"2157582551":["This follows an established inquiry of “dark” interactions [22], where usability principles can be leveraged for the detriment of a user (and perhaps the benefit of a company or stakeholder)."],"2164391693":["In human-human interaction, it is common for individuals to change how they communicate with others according to age, social rank, mood or the context of the interaction [4]."],"2282380228":["Research has shown how people experience strong negative physiological reactions when seeing a robot hurt or abused [48,61]."],"2310536335":["Some robots are purposefully designed in both appearance and behavior to exert power over people, for example being placed in a position of authority [5], or by giving commands [21].","Such robots can employ manipulative or coercive social behavior to pressure people to comply to requests that may be uncomfortable or not in their best interests [5,21].","There are also potentially negative cases, such as social robots designed to make a person feel bad [55], or persuasive robots that leverage social interaction to pressure people to do things they may rather not do or may not be in their best interest [5,12,21,47]."],"2345071775":["While the majority of social HRI research has been carried out in a traditional lab setting [9], the merits of conducting experiments in ecologically valid, in-the-wild contexts are being increasingly recognized [34,52]."],"2591951582":["Recent studies have investigated interaction in pubic spaces including rural areas [16], shopping malls [1], cafés [2],"],"2741617675":[", [11,50,67,68]), for example, a person’s culture can determine how a robot’s communication style impacts credibility [49], or how they rate a robot’s social acceptance, likeability and trustworthiness [3,42]."],"2745199108":["For example, some designs aim to use social interaction techniques to modify human behavior for an ostensibly desirable goal, such as to encourage energy savings [41] or to support health management [37,38]."],"2765177538":["and classrooms [66,69]."],"2771151535":[", [8,27,42–44,46]), and uncovering culture-specific expectations and design preferences [35,36,45]."],"2794001962":["In robotics, negative uses of social robots has been coined “psychological attacks” [15,47], relating socially manipulative behavior to literature in security.","There are also potentially negative cases, such as social robots designed to make a person feel bad [55], or persuasive robots that leverage social interaction to pressure people to do things they may rather not do or may not be in their best interest [5,12,21,47]."],"2805647790":["While the majority of social HRI research has been carried out in a traditional lab setting [9], the merits of conducting experiments in ecologically valid, in-the-wild contexts are being increasingly recognized [34,52]."],"2898173725":["Eventually, perhaps this will mirror what has happened with marketing and advertising, where regulations have been established regarding deceptive or unfair practices, for example, many locales have rules about subliminal priming [17] or targeting children [13]."]},"abstract":"Social robots are being designed to use human-like communication techniques, including body language, social signals, and empathy, to work effectively with people. Just as between people, some robots learn about people and adapt to them. In this paper we present one such robot design: we developed Sam, a robot that learns minimal information about a person's background, and adapts to this background. Our in-the-wild study found that people helped Sam for significantly longer when it adapted to match their background. While initially we saw this as a success, in re-considering our study we started seeing a different angle. Our robot effectively deceived people (changed its story and text), based on some knowledge of their background, to get more work from them. There was little direct benefit to the person from this adaptation, yet the robot stood to gain free labor. We would like to pose the question to the community: is this simply good robot design, or, is our robot being manipulative? Where does the ethical line lay between a robot leveraging social techniques to improve interaction, and the more negative framing of a robot or algorithm taking advantage of people? How can we decide what is good here, and what is less desirable?"},{"id":2735800117,"microsoftAcademicId":2735800117,"numberInSourceReferences":48,"doi":"10.1007/978-3-319-54978-1_89","title":"Approaches for Generating Empathy: A Systematic Mapping","authors":[{"LN":"Santos","FN":"Breno Santana","affil":"Universidade Federal de Sergipe"},{"LN":"Júnior","FN":"Methanias Colaço","affil":"Universidade Federal de Sergipe"},{"LN":"Nunes","FN":"Maria Augusta S. N.","affil":"Universidade Federal de Sergipe"}],"year":2018,"references":[4214443,2168894761,2084582948,614546888,2405501373,2034951011,2011210641,2032270138,2042077402,2074264048,2236198384,1994950970,3147592868,2103926245,2115861042,2295550511,2021368646,2181973141],"citationsCount":5,"abstract":"Empathy plays an important role in social interactions, such an effective teaching-learning process in a teacher-student relationship, and company-client or employee-customer relationship to retain potential clients and provide them with greater satisfaction. Increasingly, people are using technology to support their interactions, especially when the interlocutors are geographically distant from one another. This has a negative impact on the empathic capacity of individuals. In the Computer Science, there are different approaches, techniques and mechanisms to promote empathy in social or human-computer interactions. Therefore, this article presents a systematic mapping to identify and systematize the approaches, techniques and mechanisms used in computing to promote empathy. As a result, we have identified existing approaches (e.g. collaborative learning environment, virtual and robotics agents, and collaborative/affective games) to promote empathy, the main areas involved (e.g. human-computer interaction, artificial intelligence, robotics, and collaborative systems), the top researchers and their affiliations who are potential contributors to future research and, finally, the growth status of this line of research."},{"id":3131453300,"microsoftAcademicId":3131453300,"numberInSourceReferences":100,"doi":"10.3389/FROBT.2021.603510","title":"“Hit the Robot on the Head With This Mallet” – Making a Case for Including More Open Questions in HRI Research","authors":[{"LN":"Riddoch","FN":"Katie A.","affil":"University of Glasgow"},{"LN":"Cross","FN":"Emily S.","affil":"University of Glasgow"},{"LN":"Cross","FN":"Emily S.","affil":"Macquarie University"}],"year":2021,"journal":"Frontiers in Robotics and AI","references":[1979290264,2763766763,2570760970,2032160998,2014833927,2100827418,2118568307,2521535695,1566447278,2039376592,2121533105,1968395121,2763083925,2996617870,2023805937,2940411187,2921568661,2321730798,2073270049,1972176018,1997215703,1976741746,2786123477,2933190547,1969900674,2767753181,2099714955,1986155022,2574735131,2799638175,2801888051,2227489293,3023711522,2143910019,1973423536,2919701584,2945573609,2784222617,2765176894,3162718200],"citationsCount":0,"abstract":"Researchers continue to devise creative ways to explore the extent to which people perceive robots as social agents, as opposed to objects. One such approach involves asking participants to inflict 'harm' on a robot. Researchers are interested in the length of time between the experimenter issuing the instruction and the participant complying, and propose that relatively long periods of hesitation might reflect empathy for the robot, and perhaps even attribution of human-like qualities, such as agency and sentience. In a recent experiment, we adapted the so-called 'hesitance to hit' paradigm, in which participants were instructed to hit a humanoid robot on the head with a mallet. After standing up to do so (signaling intent to hit the robot), participants were stopped, and then took part in a semi-structured interview to probe their thoughts and feelings during the period of hesitation. Thematic analysis of the responses indicate that hesitation not only reflects perceived socialness, but also other factors including (but not limited to) concerns about cost, mallet disbelief, processing of the task instruction, and the influence of authority. The open-ended, free responses participants provided also offer rich insights into individual differences with regards to anthropomorphism, perceived power imbalances, and feelings of connection toward the robot. In addition to aiding understanding of this measurement technique and related topics regarding socialness attribution to robots, we argue that greater use of open questions can lead to exciting new research questions and interdisciplinary collaborations in the domain of social robotics."},{"id":2969264810,"microsoftAcademicId":2969264810,"numberInSourceReferences":159,"doi":"10.3390/APP9173442","title":"Development of an Effective Information Media Using Two Android Robots","authors":[{"LN":"Nishio","FN":"Toshiaki"},{"LN":"Yoshikawa","FN":"Yuichiro"},{"LN":"Ogawa","FN":"Kohei"},{"LN":"Ishiguro","FN":"Hiroshi"}],"year":2019,"journal":"Applied Sciences","references":[2101234009,2130607791,2132049110,2161794541,2061309618,2084275282,2132446817,1976587136,2071468852,2145685195,2541312590,2313382721,2785520163,2114109477,2096801730,2765093611],"citationsCount":1},{"id":90393491,"microsoftAcademicId":90393491,"numberInSourceReferences":1,"doi":"10.1007/978-3-319-08108-3_20","title":"Ethical and Technical Aspects of Emotions to Create Empathy in Medical Machines","authors":[{"LN":"Vallverdú","FN":"Jordi","affil":"Autonomous University of Barcelona"},{"LN":"Casacuberta","FN":"David","affil":"Autonomous University of Barcelona"}],"year":2015,"journal":"Intelligent Systems, Control and Automation: Science and Engineering","references":[1535803599,2159024459,2339343773,2110631042,3145637508,1994682066,2096476371,1581645554,2150422125,2131799829,2113876961,2144168775,2149731110,2144872492,2151645026,1601400566,1482630046,2008115067,1975911018,2165542916,2003350835,2162897682,2139766084,2798993695,2152536828,2131911005,13388985,3125119928,2003238582,58212258,2443950378,1567800216,85402463,2006887428,2099081440,2059550545,2100274075,2167434815,1972011061,1557191966,1619839657,2110048782,1772516903,3030292250,1978516982,2105754886,116290630,2138683923,2272894197,35931001,2061582917,3160979476,2100737263,2592608098,1027697028,2951608939,579723986,2530042152,2478080977,1529232921],"citationsCount":5,"citationContext":{"13388985":["[54] and Tapus and Mataric [55] also use empathy as a way to improve HRI, making possible “friendships” between machines and robots."],"35931001":["The first level is already amendable by means of pattern recognition algorithms which would allow an autonomous agent to recognize the patient emotional state by means of a camera and/or microphone [43, 44]."],"58212258":["[5], either you obtain erroneous research results or unnecessary privacy protections (because there is too much complexity)."],"116290630":["[61] we know that there are specific arousal patterns for each of the 6 basic emotions he described and considered universal across all cultures (anger, disgust, fear, happiness, sadness and surprise), and with the proper sensors, you can distinguish between them in a reliable form [62–64]."],"1027697028":["According to Waal and Thompson [42], we can view empathy as an emotion that includes the ability to connect with others and to become affected by their emotions.","Again, following Waal and Thompson [42], emotional expressions and gestures are visibly imitated by observers and this imitation is accompanied by self-reports of associated emotional states.","Following Waal and Thompson [42] we can distinguish four levels of empathy: motor contagion, imaginative transposition, perspective taking and moral empathy."],"1482630046":["Also, the fact that there are a lot less doctors and nurses there: around two doctors per 10,000 people in Africa [38]."],"1529232921":["Following Casacuberta and Senges [14], we only need to consider some inarticulate elements which are either missing or overdeveloped."],"1557191966":["The first level is already amendable by means of pattern recognition algorithms which would allow an autonomous agent to recognize the patient emotional state by means of a camera and/or microphone [43, 44]."],"1581645554":["However, there is no need to create new rights as some people like Barlow [11], Hayles [12] or Gray [13] argue."],"1975911018":["[61] we know that there are specific arousal patterns for each of the 6 basic emotions he described and considered universal across all cultures (anger, disgust, fear, happiness, sadness and surprise), and with the proper sensors, you can distinguish between them in a reliable form [62–64]."],"1994682066":["Following the Big Data approach, the journal Science published a paper [36] describing how in tracking cell phone data from 15 million people in Kenya, they were able to discover relevant facts on how traveling patterns contribute to the spreading of malaria."],"2003238582":["According to Ekman and Friesen [65] and Ekman [66], they present transcultural patterns, so they are a reliable guide to decide which emotion a person is feeling independently of his or her cultural background."],"2003350835":["In the first fMRI study on how humans express empathy for a robot (a tortured and vexed Pleo-toy dinosaur-robot), Rosenthal-von der Pütten [57]15 conclude: “One limitation of the study is that we used videos in our experiment and that participants saw"],"2059550545":["Recent studies have correlated the apparently neural basis of learning by mirror neurons [22, 23] and functional magnetic resonance imaging (fMRI) has been intensively employed to investigate the functional anatomy of empathy (for reviews, see [24–26, 27])."],"2061582917":["For example, Zborowski [19] demonstrated that pain symptomatic of the same illness was processed very differently by patients according their belonging to a cultural community within the USA (italo-American, old-American, Irish-American, Jews)."],"2096476371":["Recent studies have correlated the apparently neural basis of learning by mirror neurons [22, 23] and functional magnetic resonance imaging (fMRI) has been intensively employed to investigate the functional anatomy of empathy (for reviews, see [24–26, 27])."],"2099081440":["The third level adds to the system folk psychology [45] so it can reinterpret patient psychological states by means of an expert system that is able to reason within the context of beliefs, desires, plans and motivations [46]."],"2100274075":["There is one last question to ask: cultural attitudes towards robots also change [29, 30], although the conceptual design is the same for eastern and Western robotic experts [31]."],"2100737263":[", [53]) provide a richer framework from which to approach anthropomorphic robots."],"2105754886":["[60] have also started to work in this area with different emotions (fear, pain, sadness, surprise) and also emotion behaviors like laughter."],"2113876961":["European projects like e-Sense or Sensei [69] are trying to develop systems and protocols for remote emoting, facilitating emotional communication in such environments.","Following Sadri [69], we can view ambient intelligence as follows: Ambient Intelligence is the vision of a future in which environments support the people inhabiting them."],"2131799829":["More on WOZ in Riek [59]."],"2131911005":["This has been experimentally tested by artist and scientist Naoko Tosa in her projects Neurobaby [48] and Unconscious Flow [49]."],"2139766084":["[20] found significant results with racial/ethnic differences in physician distrust in the United States."],"2144168775":["For example, in a recent paper, [8] showed that when using a portal to help people with diabetes, having access to computer was not the main relevant element, but mental barriers on how to use computers for empowerment."],"2144872492":["Recent studies have correlated the apparently neural basis of learning by mirror neurons [22, 23] and functional magnetic resonance imaging (fMRI) has been intensively employed to investigate the functional anatomy of empathy (for reviews, see [24–26, 27])."],"2149731110":["Even if the subject tries to hide an emotion, or pretend to have a different one, based on what [67] calls micro expressions, it is always possible to check the real emotion a person is feeling."],"2150422125":["Recent studies have correlated the apparently neural basis of learning by mirror neurons [22, 23] and functional magnetic resonance imaging (fMRI) has been intensively employed to investigate the functional anatomy of empathy (for reviews, see [24–26, 27])."],"2151645026":["AIBO deserved respect, had rights, or could be held morally accountable for action) [50].","[50] found that owners projected towards AIBO robots conceptions of technological essences (75 %), life-like essences (49 %), mental states (60 %), and social rapport (59 %)."],"2152536828":["Nevertheless, humans look more comfortable with anthropomorphic objects (if not under Uncanny valley effect) [51]."],"2167434815":["Later, Sweeney was able to demonstrate [4] that up to 87 % of Americans can be associated with their hospital records even if name, address, and other identifying information are removed from the registry, but date of birth, zip code, and census are not."],"2339343773":["This was the beginning of the affective computing research program [17]."],"2592608098":["There is one last question to ask: cultural attitudes towards robots also change [29, 30], although the conceptual design is the same for eastern and Western robotic experts [31]."],"2798993695":["So, contrary to Brin [16], the main reason we believe that privacy is obsolete is not technology per se, but that the traditional conditions no longer hold."],"2951608939":["One of the key issues measured is emotion [71]."],"3030292250":["Carpenter [56] interviewed multiple soldiers based in the Explosive Ordenance Disposal Unit about their work operating robots that diffuse or locate bombs."],"3125119928":["According to Lerman [9], “Billions of people worldwide remain on big data’s periphery."],"3145637508":["Thus, we have intelligent houses, buildings or even streets and whole cities [68]."]},"abstract":"This chapter analyzes the ethical challenges in healthcare when introducing medical machines able to understand and mimic human emotions. Artificial emotions is still an emergent field in artificial intelligence, so we devote some space in this paper in order to explain what they are and how we can have an machine able to recognize and mimic basic emotions. We argue that empathy is the key emotion in healthcare contexts. We discuss what empathy is and how it can be modeled to include it in a medical machine. We consider types of medical machines (telemedicine, care robots and mobile apps), and describe the main machines that are in use and offer some predictions about what the near future may bring. The main ethical problems we consider in machine medical ethics are: privacy violations (due to online patient databases), how to deal with error and responsibility concerning machine decisions and actions, social inequality (as a result of people being removed from an e-healthcare system), and how to build trust between machines, patients, and medical professionals."},{"id":2974326827,"microsoftAcademicId":2974326827,"numberInSourceReferences":85,"doi":"10.23919/JCC.2019.08.011","title":"Amygdala-inspired affective computing: To realize personalized intracranial emotions with accurately observed external emotions","authors":[{"LN":"Gong","FN":"Chao","affil":"University of Science and Technology Beijing"},{"LN":"Lin","FN":"Fuhong","affil":"University of Science and Technology Beijing"},{"LN":"Zhou","FN":"Xianwei","affil":"University of Science and Technology Beijing"},{"LN":"Lu","FN":"Xing","affil":"University of Science and Technology Beijing"}],"year":2019,"journal":"China Communications","references":[2913218058,2894936288,2885016967,2811329793,2883743124,2799806067,2799893612,1411018550,2913905656],"citationsCount":3,"citationContext":{"1411018550":["Traditional approaches of affective computing find out the apparent mapping relationship between the external appearance data and emotions by analyzing the instantaneous emotional signals, such as speech signals [57], facial expressions [8-10] and physiological signals [11, 12]."],"2799806067":["Second, emergency emotions are quick and low-precision responses generated on a small amount of key information pre-transmitted to amygdala from hypothalamus for avoiding sudden dangers [15, 16]."],"2799893612":["Second, emergency emotions are quick and low-precision responses generated on a small amount of key information pre-transmitted to amygdala from hypothalamus for avoiding sudden dangers [15, 16]."],"2811329793":["During [ , 1] t t + , the emergency emotion Ot i","For instance in automatic driving vehicles, affective computing will help the vehicle system to give out a warning or take over the driving according to the driver’s emotional state to avoid dangerous driving [1].","This is because they can perfectly work together that the lightweight CNN quickly responses the input and help fill the output gap in the initial stage of each [ , 1] t t + ."],"2883743124":["In the field of natural language processing, affective computing can recognize the rich emotional information hidden in the language [2] and achieve more realistic effects in speech synthesis [3]."],"2885016967":["Lévêque Y [12] performed classification of four musical emotions (positive/high arousal, negative/high arousal, negative/low arousal, and positive/low arousal) by using an extended linear discriminant analysis .","Traditional approaches of affective computing find out the apparent mapping relationship between the external appearance data and emotions by analyzing the instantaneous emotional signals, such as speech signals [57], facial expressions [8-10] and physiological signals [11, 12]."],"2894936288":["Manfredonia J [10] presented their approach in instantaneously detecting the emotions of video viewers’ emotions from facial","Traditional approaches of affective computing find out the apparent mapping relationship between the external appearance data and emotions by analyzing the instantaneous emotional signals, such as speech signals [57], facial expressions [8-10] and physiological signals [11, 12]."],"2913218058":["Finally, human emotions are personalized [17] and the emotions of different human individuals excited by the same stimulus can be different, while that of emotional machines simulated by traditional affective computing will always be the same."]},"abstract":"Artificial intelligence technology has revolutionized every industry and trade in recent years. However, its own development is encountering bottlenecks that it is unable to implement empathy with human emotions. So affective computing is getting more attention from researchers. In this paper, we propose an amygdala-inspired affective computing framework to realize the recognition of all kinds of human personalized emotions. Similar to the amygdala, the instantaneous emergency emotion is first computed more quickly in a low-redundancy convolutional neural network compressed by pruning and weight sharing with hashing trick. Then, the real-time process emotion is identified more accurately by the memory level neural networks, which is good at handling time-related signals. Finally, the intracranial emotion is recognized in personalized hidden Markov models. We demonstrate on Facial Expression of Emotion Dataset and the recognition accuracy of external emotions (including the emergency emotion and the process emotion) reached 85.72%. And the experimental results proved that the personalized affective model can generate desired intracranial emotions as expected."},{"id":2293470506,"microsoftAcademicId":2293470506,"numberInSourceReferences":18,"doi":"10.1007/978-3-319-25554-5_22","title":"Influence of Upper Body Pose Mirroring in Human-Robot Interaction","authors":[{"LN":"Fuente","FN":"Luis A.","affil":"Oxford Brookes University"},{"LN":"Ierardi","FN":"Hannah","affil":"Oxford Brookes University"},{"LN":"Pilling","FN":"Michael","affil":"Oxford Brookes University"},{"LN":"Crook","FN":"Nigel T.","affil":"Oxford Brookes University"}],"year":2015,"journal":"International Conference on Social Robotics","references":[2098676269,2118363134,1889777305,2039376592,1990266238,1697749196,2131060534,2002404540,2135115712,2147095743,2021457470,1490321332,2334778303,1975348090],"citationsCount":7,"abstract":"This paper explores the effect of upper body pose mirroring in human-robot interaction. A group of participants is used to evaluate how imitation by a robot affects people’s perception of their conversation with it. A set of twelve questions about the participants’ university experience serves as a backbone for the dialogue structure. In our experimental evaluation, the robot reacts in one of three ways to the human upper body pose: ignoring it, displaying its own upper body pose, and mirroring it. The manner in which the robot behaviour influences human appraisal is analysed using the standard Godspeed questionnaire. Our results show that robot body mirroring/non-mirroring influences the perceived humanness of the robot. The results also indicate that body pose mirroring is an important factor in facilitating rapport and empathy in human social interactions with robots."},{"id":3100362702,"microsoftAcademicId":3100362702,"numberInSourceReferences":67,"doi":"10.1145/3341198","title":"An Autonomous Cognitive Empathy Model Responsive to Users’ Facial Emotion Expressions","authors":[{"LN":"Bagheri","FN":"Elahe","affil":"Vrije Universiteit Brussel"},{"LN":"Esteban","FN":"Pablo G.","affil":"Vrije Universiteit Brussel"},{"LN":"Cao","FN":"Hoang-Long","affil":"Vrije Universiteit Brussel"},{"LN":"Beir","FN":"Albert De","affil":"Vrije Universiteit Brussel"},{"LN":"Lefeber","FN":"Dirk","affil":"Vrije Universiteit Brussel"},{"LN":"Vanderborght","FN":"Bram","affil":"Vrije Universiteit Brussel"}],"year":2020,"journal":"ACM Transactions on Interactive Intelligent Systems (TiiS)","references":[2807126412,1992745256,2803193013,2042962409,1980331490,2168222508,2150422125,1980083892,1973378890,2871950322,2045883408,2001564357,2129928382,2042585112,2611618328,2010943014,2099270617,103982469,2014153923,2169473752,2054832392,2053782908,2093040750,2117373981,1980870891,2109543807,1976156697,2165287812,1981509185,2080184306,2045053352,2156766386,2921653928,2082285558,2075446845,1564084013,2111715140,2028414772,2096004790,2016314018,1975831188,2727839256,1968729274,2578725037,1619839657,2146939392,2727108092,1994660327,2105508921,2127023913,1985605850,2154655824,2006681446,2785012264,2045620158,2072122946,2131954654,2792765841,2169389179,1746490978,2989721801,2055311115,2007147685,2249189521],"citationsCount":1,"abstract":"Successful social robot services depend on how robots can interact with users. The effective service can be obtained through smooth, engaged, and humanoid interactions in which robots react properly to a user’s affective state. This article proposes a novel Automatic Cognitive Empathy Model, ACEM, for humanoid robots to achieve longer and more engaged human-robot interactions (HRI) by considering humans’ emotions and replying to them appropriately. The proposed model continuously detects the affective states of a user based on facial expressions and generates desired, either parallel or reactive, empathic behaviors that are already adapted to the user’s personality. Users’ affective states are detected using a stacked autoencoder network that is trained and tested on the RAVDESS dataset.The overall proposed empathic model is verified throughout an experiment, where different emotions are triggered in participants and then empathic behaviors are applied based on proposed hypothesis. The results confirm the effectiveness of the proposed model in terms of related social and friendship concepts that participants perceived during interaction with the robot."},{"id":2909989773,"microsoftAcademicId":2909989773,"numberInSourceReferences":58,"doi":"10.1109/IROS.2018.8594324","title":"Band of Brothers and Bolts: Caring About Your Robot Teammate","authors":[{"LN":"Wen","FN":"James","affil":"United States Air Force Academy"},{"LN":"Stewart","FN":"Amanda","affil":"United States Air Force Academy"},{"LN":"Billinghurst","FN":"Mark","affil":"University of South Australia"},{"LN":"Tossell","FN":"Chad","affil":"United States Air Force Academy"}],"year":2018,"journal":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","references":[2111040806,1976660112,2165542916,2152536828,3121604639,2912641090,343869564],"citationsCount":1,"citationContext":{"343869564":["performance impeded by concerns over the well-being of their robot teammates [2]."],"1976660112":["of loneliness [7]."],"2111040806":["The creation of non-trivial bonds between a human and a robot companion may require a substantial length of time, as a survey of 24 social robotics studies seems to suggest [9]."],"2152536828":["than robots that are have a more mechanical appearance by design [5]."],"2165542916":["In fact, the contriving of a robot’s visual appearance, voice, gestures, or other aspects can be so effective in manipulating human reaction that there are concerns that an excessive degree of it can cause expectation levels to be set at an inappropriate level [8]."],"2912641090":["edu anthropomorphism lest a close bonding with their human teammates hinders effective execution of operations [3]."],"3121604639":["[10] by using a narrative on a relatively non-anthropomorphized robot but rather than a singular action of destroying the robot, we used a system that would degrade the robot over time based upon the participants actions.","found that empathy can be detected from people for robots even when anthropomorphism is minimal [10]."]},"abstract":"It has been observed that a robot shown as suffering is enough to cause an empathic response from a person. Whether the response is a fleeting reaction with no consequences or a meaningful perspective change with associated behavior modifications is not clear. Existing work has been limited to measurements made at the end of empathy inducing experimental trials rather measurements made over time to capture consequential behavioral pattern. We report on preliminary results collected from a study that attempts to measure how the actions of a participant may be altered by empathy for a robot companion. Our findings suggest that induced empathy can in fact have a significant impact on a person's behavior to the extent that the ability to fulfill a mission may be affected."},{"id":2911875319,"microsoftAcademicId":2911875319,"numberInSourceReferences":36,"doi":"10.1007/978-3-030-04672-9_7","title":"Degrees of Empathy: Humans’ Empathy Toward Humans, Animals, Robots and Objects","authors":[{"LN":"Mattiassi","FN":"Alan D. A.","affil":"University of Modena and Reggio Emilia"},{"LN":"Sarrica","FN":"Mauro","affil":"Sapienza University of Rome"},{"LN":"Cavallo","FN":"Filippo","affil":"Sant'Anna School of Advanced Studies"},{"LN":"Fortunati","FN":"Leopoldina","affil":"University of Udine"}],"year":2017,"journal":"Italian Forum of Ambient Assisted Living","references":[1992745256,1989477529,2096476371,2078479875,2131227926,2085612867,3162503594,1971079539,2101870300,2160604906,2152536828,2260848322,2035128922,2030149967,2012511508,2087996665,3114284325,2189103754,32014909,1986038870,2145482038,1978431764,2779045286,2071572667,2001771035,2137819965,1629118197],"citationsCount":1,"abstract":"The aim of this paper is to present an experiment in which we compare the degree of empathy that a convenience sample of students expressed with humans, animals, robots and objects. The present study broadens the spectrum of the elements eliciting empathy that previous research has so far explored separately. Our research questions are: does the continuum represented by this set of elements elicit empathy? Is it possible to observe a linear decrease of empathy according to different features of the selected elements? More broadly, does empathy, as a construct, resist in front of the diversification of the element eliciting it? Results show that participants expressed empathy differently when exposed to three clusters of social actors being mistreated: they felt more sad, sorry, aroused and out of control for animals than for humans, but showed little to no empathy for objects. Interestingly, robots that looked more human-like evoked emotions similar to those evoked by humans, while robots that looked more animal-like evoked emotions half-way between those evoked by humans and objects. Implications are discussed."},{"id":3093482637,"microsoftAcademicId":3093482637,"numberInSourceReferences":40,"doi":"10.1109/RO-MAN47096.2020.9223598","title":"Robot Mirroring: Promoting Empathy with an Artificial Agent by Reflecting the User’s Physiological Affective States","authors":[{"LN":"Perusquia-Hemandez","FN":"Monica","affil":"NTT Communication Science Laboratories,Japan"},{"LN":"Balda","FN":"Marisabel Cuberos","affil":"Independent researcher based in Switzerland"},{"LN":"Jauregui","FN":"David Antonio Gomez","affil":"University of Bordeaux"},{"LN":"Paez-Granados","FN":"Diego","affil":"École Polytechnique Fédérale de Lausanne"},{"LN":"Dollack","FN":"Felix","affil":"NTT Communication Science Laboratories,Japan"},{"LN":"Salazar","FN":"Jose Victorio","affil":"Tohoku University"}],"year":2020,"journal":"2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)","references":[2339343773,2029334490,1969977883,2149628368,3124537122,1607171655,2144756000,1986119901,2054560711,2165542916,2403010436,2132785383,1990266238,3105679954,2811336223,2126890968,2027814249,1526704262,2610658142,2114909357,1991134918,1982188321,2540373225,2127381594,2160254180,2088178795,109314002,2800314559,1999191904,2101129795,2950298184],"citationsCount":0,"citationContext":{"109314002":["It is also possible to foster empathy with virtual agents and robots [6], [7]."],"1526704262":["People might only trust information that confirms or suits their beliefs [1]."],"1607171655":["Following an automatized HTML page, they answered the State-Trait Anxiety Inventory (STAI-Y2) questionnaire [40], and were informed by the system that a pet was automatically selected based on their answers."],"1969977883":["In turn, this might lead to self-compassion, and trigger healthier behaviors [3]."],"1982188321":["Facial expressions are a good predictor of the valence [17]."],"1986119901":["It is also possible to foster empathy with virtual agents and robots [6], [7]."],"1990266238":["Similarly, animallike robots mimic the facial expressions and head movements of users [11], or are able to convey emotions from respiration patterns [12]."],"1991134918":["Mimicking the behaviors of others, and physiological synchronization is related to higher empathetic accuracy [23], [24]."],"1999191904":["The HR of each user was mapped to a vibration pattern simulating the heart sounds S1 and S2 which correspond to the QRS complex and T-wave of the heart’s electrical activity [31]."],"2027814249":["Mimicking the behaviors of others, and physiological synchronization is related to higher empathetic accuracy [23], [24]."],"2029334490":["Other autonomic physiological changes are often associated with arousal [18] and with emotional responses to an event or to an interaction with another agent [19]."],"2054560711":["2) Procedure: The pet representations were presented in random order, together with the question “How is the character in the image feeling?” answered with the Affect Grid (AG) [29]."],"2088178795":["as an effective strategy leading to pro-social behavior and increased liking of the mimicking person in economic [4], and courtship [5] contexts."],"2114909357":["Other autonomic physiological changes are often associated with arousal [18] and with emotional responses to an event or to an interaction with another agent [19]."],"2126890968":["3) Stimuli: Emotion-eliciting film scenes from two publicly available databases were chosen: the FilmStim database [36] and the Emotional Movie Database (EMDB) [37].","N-V, N-A Blue FilmStim [36] N-V, L-A 6000 EMDB [37] L-V, N-A E."],"2127381594":["Furthermore, high interoceptive awareness has been linked to a greater automatic imitation [26]."],"2132785383":["as an effective strategy leading to pro-social behavior and increased liking of the mimicking person in economic [4], and courtship [5] contexts."],"2144756000":["3) Stimuli: Emotion-eliciting film scenes from two publicly available databases were chosen: the FilmStim database [36] and the Emotional Movie Database (EMDB) [37].","FilmStim [36] H-V, N-A The Dead Poets society FilmStim [36] L-V, L-A Life is beautiful (La vita e bella) FilmStim [36] N-V, H-A Wiserhood [38] H-V, H-A Baby laughing hysterically at ripping paper [39] L-V, H-A Schindler’s list FilmStim [36] L-V, L-A City of angels FilmStim [36]","FilmStim [36] H-V, N-A The Dead Poets society FilmStim [36] L-V, L-A Life is beautiful (La vita e bella) FilmStim [36] N-V, H-A Wiserhood [38] H-V, H-A Baby laughing hysterically at ripping paper [39] L-V, H-A Schindler’s list FilmStim [36] L-V, L-A City of angels FilmStim [36]","FilmStim [36] H-V, N-A The Dead Poets society FilmStim [36] L-V, L-A Life is beautiful (La vita e bella) FilmStim [36] N-V, H-A Wiserhood [38] H-V, H-A Baby laughing hysterically at ripping paper [39] L-V, H-A Schindler’s list FilmStim [36] L-V, L-A City of angels FilmStim [36]","FilmStim [36] H-V, N-A The Dead Poets society FilmStim [36] L-V, L-A Life is beautiful (La vita e bella) FilmStim [36] N-V, H-A Wiserhood [38] H-V, H-A Baby laughing hysterically at ripping paper [39] L-V, H-A Schindler’s list FilmStim [36] L-V, L-A City of angels FilmStim [36]","FilmStim [36] H-V, N-A The Dead Poets society FilmStim [36] L-V, L-A Life is beautiful (La vita e bella) FilmStim [36] N-V, H-A Wiserhood [38] H-V, H-A Baby laughing hysterically at ripping paper [39] L-V, H-A Schindler’s list FilmStim [36] L-V, L-A City of angels FilmStim [36]","N-V, N-A Blue FilmStim [36] N-V, L-A 6000 EMDB [37] L-V, N-A E."],"2149628368":["These requirements were translated into our concept as follows: To represent and mirror affective states, we chose visuo-haptic representations based on a mapping of posture, facial expressions, and HR to the circumplex model of affect [14]."],"2160254180":["This seems in line with previous research that suggested that even neutral faces are appraised positively or negatively according to the context or the personality of the rater [30]."],"2165542916":["This ”social activation model” contributes to foster empathy [28]."],"2339343773":["To show caring and empathetic behavior, an agent must be attuned to the affective state of the user [9]."],"2403010436":["Visual representations of both valence and arousal are usually successful [13]; and wearable devices can be used to produce vibration patterns that can be recognized by the user in ecological settings [15]."],"2540373225":["Anthropomorphic robots successfully show happiness, fear and neutral facial expressions as responses to the affective speech signal of users [10]."],"2610658142":["In addition, small pet robots can also convey arousal levels through breathing patterns [12].","Similarly, animallike robots mimic the facial expressions and head movements of users [11], or are able to convey emotions from respiration patterns [12]."],"2800314559":["The visual representation aims to maintain adherence over long periods of time, when compared to text-based agents [13].","Visual representations of both valence and arousal are usually successful [13]; and wearable devices can be used to produce vibration patterns that can be recognized by the user in ecological settings [15]."],"2811336223":["Interoception is defined as the ability to correctly perceive bodily sensations [22]."],"3105679954":["both a sensor and as biofeedback device to convey emotions by breathing patterns [21]."],"3124537122":["The users also customized their pet to create appreciation by spending some time modifying it [27]."]},"abstract":"Self-tracking aims to increase awareness, decrease undesired behaviors, and ultimately lead towards a healthier lifestyle. However, inappropriate communication of self- tracking results might cause the opposite effect. Subtle self- tracking feedback is an alternative that can be provided with the aid of an artificial agent representing the self. Hence, we propose a wearable pet that reflects the user’s affective states through visual and haptic feedback. By eliciting empathy and fostering helping behaviors towards it, users would indirectly help themselves. A wearable prototype was built, and three user studies performed to evaluate the appropriateness of the proposed affective representations. Visual representations using facial and body cues were clear for valence and less clear for arousal. Haptic interoceptive patterns emulating heart-rate levels matched the desired feedback urgency levels with a saturation frequency. The integrated visuo-haptic representations matched to participants own affective experience. From the results, we derived three design guidelines for future robot mirroring wearable systems: physical embodiment, interoceptive feedback, and customization."},{"id":3156501751,"microsoftAcademicId":3156501751,"numberInSourceReferences":173,"doi":"10.1007/S00146-021-01211-2","title":"Empathic responses and moral status for social robots: an argument in favor of robot patienthood based on K. E. Løgstrup","authors":[{"LN":"Balle","FN":"Simon N.","affil":"Aarhus University"}],"year":2021,"journal":"Ai & Society","references":[1892502720,2115040353,2111040806,1980083892,2102453924,2936213937,2251410821,350888970,2522193641,2012676995,1834533132,2003350835,2152536828,1973983602,2766105414,2330152664,3042067146,2035128922,2282380228,2012511508,2049480502,2111006467,2080902143,2615413400,2791549227,2951503759,2933190547,2143323141,1957757161,2022297454,2164980806,2034426409,3026367857,2915909934,2992873857,2934055000,1605339395,648498687,2888628879,2145482038,3007230393,2917908282,2801939058,2035844185,2072710811,2883815653,1565661599,3017585837,2552464633,2479655640,2134165085,1991912320,2001771035,2337433655,2024228489,2924079068,3000266002,3093739782,2477989693,2765335669,2069874839,2946462556,2619729895,3041999555,2727344993,2905515412,2742759440,3044322115,2974206664,2763990171,2905118400,2742335986,2999954507],"citationsCount":1,"abstract":"Empirical research on human–robot interaction (HRI) has demonstrated how humans tend to react to social robots with empathic responses and moral behavior. How should we ethically evaluate such responses to robots? Are people wrong to treat non-sentient artefacts as moral patients since this rests on anthropomorphism and ‘over-identification’ (Bryson and Kime, Proc Twenty-Second Int Jt Conf Artif Intell Barc Catalonia Spain 16–22:1641–1646, 2011)—or correct since spontaneous moral intuition and behavior toward nonhumans is indicative for moral patienthood, such that social robots become our ‘Others’ (Gunkel, Robot rights, MIT Press, London, 2018; Coeckelbergh, Kairos J Philos Sci 20:141–158, 2018)?. In this research paper, I weave extant HRI studies that demonstrate empathic responses toward robots with the recent debate on moral status for robots, on which the ethical evaluation of moral behavior toward them is dependent. Patienthood for robots has standardly been thought to obtain on some intrinsic ground, such as being sentient, conscious, or having interest. But since these attempts neglect moral experience and are curbed by epistemic difficulties, I take inspiration from Coeckelbergh and Gunkel’s ‘relational approach’ to explore an alternative way of accounting for robot patienthood based on extrinsic premises. Based on the ethics of Danish theologian K. E. Logstrup (1905–1981) I argue that empathic responses can be interpreted as sovereign expressions of life and that these expressions benefit human subjects—even if they emerge from social interaction afforded by robots we have anthropomorphized. I ultimately develop an argument in defense of treating robots as moral patients."},{"id":2900150883,"microsoftAcademicId":2900150883,"numberInSourceReferences":31,"doi":"10.1109/ROMAN.2018.8525515","title":"Effect of Explicit Emotional Adaptation on Prosocial Behavior of Humans towards Robots depends on Prior Robot Experience","authors":[{"LN":"Kuhnlenz","FN":"Barbara","affil":"Coburg University of Applied Sciences"},{"LN":"Kuhnlenz","FN":"Kolja","affil":"Coburg University of Applied Sciences"},{"LN":"Busse","FN":"Fabian","affil":"Coburg University of Applied Sciences"},{"LN":"Fortsch","FN":"Pascal","affil":"Coburg University of Applied Sciences"},{"LN":"Wolf","FN":"Maximilian","affil":"Coburg University of Applied Sciences"}],"year":2018,"journal":"2018 27th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)","references":[2167557160,2031371003,2078671978,2132049110,2150781787,2169749310,2123076731,2145467192,2013040441,2111438996,2124717700,2064149108,1552206080,2165287812,1697749196,1567947883,2133425506,2101348300,2145802841,2056713258,2052837583,2135115712,2085025315,2016131089,2020299617,53247728,2126293656,2144275181,2047366844,2488222674,2468378424,2014589203,2068607682,1985308259,2017634848,1970223698,2182117442,2167483262],"citationsCount":2,"citationContext":{"53247728":["The importance of such “mutual beliefs” in natural language communication is instantiated in the phenomenon of “grounding” [12], meaning that the interpretation of communicated contents has to be at least “approximately correct” in order to achieve successful communication acts, based on a common underlying field of knowledge and/or required actions [45].","in related work it could be observed that attitudes towards robots changed during interaction [45]."],"1552206080":["Thereby, it has to be considered that this may not match the actual mood but only the mood, the user is willing to communicate because of social conventions and rituals during small-talk [44]."],"1567947883":["Hence, in human-robot interaction (HRI), emotion recognition, expression, and emotionally enriched communication and closed-loop behavior control have gained strong attention during the last two decades [27,32,36,38,40]."],"1697749196":["The first part consists of the “Godspeed” questionnaires [2] to evaluate “five key concepts of HRI” on semantic differential scales: anthropomorphism, animacy, likeability, perceived intelligence, and perceived safety."],"1970223698":["In this context, prosocial behavior in terms of helpful behavior towards a robot could successfully be induced in task-related HRI [21,22,30].","Thereby, emotional facial and verbal expressions were still employed, but not adapted to the human user in order to still induce a high extent of situational empathy, since this is regarded as a premise for the approach to work [21]."],"1985308259":["the robot head EDDIE, with 23 degrees of freedom, mixing anthropomorphic (human-shaped) and zoomorphic (animal-shaped) features [41], and the head of the IURO platform [8]."],"2013040441":["by body language [4,13] or changing eye-colors that are matched to different emotional states [23]."],"2014589203":["Existing HRI-applications using implicit (non-verbal) communication channels are based on a communicative mechanism in human-human interaction, called “alignment” [37], that leads to adaptive processes between interlocutors which are essential for human-human interactions [17,28]."],"2016131089":["In this context, prosocial behavior in terms of helpful behavior towards a robot could successfully be induced in task-related HRI [21,22,30].","In this scenario, users’ prosocial behavior is measured objectively in terms of a behavioral measure for helpfulness towards a robot in form of an object labelling task (with open number of objects) according to previous work [30].","The instrumentalized form of small-talk used in the presented approach is referred to as “social subdialog” in the following, since triggering helpfulness by means of similarity is regarded to be a social sub-task in cases where helpfulness is necessary to fulfill the overall task [30].","explicit emotional adaption on helpfulness, as well as on the HRI-key concepts anthropomorphism and animacy [30].","of both, implicit and explicit, components of emotional adaptation at the same time, even the single component of explicit emotional adaptation in form of a social sub-dialog already led to a statistically significant increase of prosocial behavior towards the robot [30]."],"2017634848":["According to Mehrabian [33] this includes “all facets of nonverbal communication, including body positions and movements, facial expressions, voice quality and intonation during speech, volume and speed of speech, subtle variations in wording of sentences that reveal hidden meanings in what is said, as well as combinations of messages from different sources, e.","explicit communication channels like direct verbal or written utterances, but also “silent messages” [33] as implicit communication channels of emotions and attitudes."],"2020299617":["In this context, prosocial behavior in terms of helpful behavior towards a robot could successfully be induced in task-related HRI [21,22,30]."],"2031371003":["This ability is already developed in infants [5] and dysfunctions in feeling empathy might lead to social deficits, as observed in autism [15]."],"2047366844":["by body language [4,13] or changing eye-colors that are matched to different emotional states [23]."],"2052837583":["Existing HRI-applications using implicit (non-verbal) communication channels are based on a communicative mechanism in human-human interaction, called “alignment” [37], that leads to adaptive processes between interlocutors which are essential for human-human interactions [17,28]."],"2056713258":["In linguistic pragmatics, a distinction is made between explicitly communicated content which is directly said or written, and ”implicatures” [9], that enrich and manipulate the pragmatic interpretation of explicitly communicated content."],"2068607682":["social psychological studies investigating inter-human empathy, the experimentally induced extent of empathy has successfully been manipulated via similarity of personal attitudes between the subjects [3,29]."],"2078671978":["Hence, in human-robot interaction (HRI), emotion recognition, expression, and emotionally enriched communication and closed-loop behavior control have gained strong attention during the last two decades [27,32,36,38,40]."],"2085025315":["tions, where a robot has to handle dynamically changing environmental impacts to fulfill its task, a robot may depend on asking humans for missing task knowledge [1,8,34,35]."],"2101348300":["Existing HRI-applications using implicit (non-verbal) communication channels are based on a communicative mechanism in human-human interaction, called “alignment” [37], that leads to adaptive processes between interlocutors which are essential for human-human interactions [17,28]."],"2111438996":["Hence, in human-robot interaction (HRI), emotion recognition, expression, and emotionally enriched communication and closed-loop behavior control have gained strong attention during the last two decades [27,32,36,38,40]."],"2123076731":["In applications where (robotic) systems provide human users with information [25,26,39], it is self-evident that the user keeps up the interaction as long as all requested informa-tion is provided."],"2124717700":["from the toolkit for measuring user acceptance of social robots [24]: social presence, trust, intention to use, and perceived sociability."],"2126293656":["social psychological studies investigating inter-human empathy, the experimentally induced extent of empathy has successfully been manipulated via similarity of personal attitudes between the subjects [3,29]."],"2132049110":["In applications where (robotic) systems provide human users with information [25,26,39], it is self-evident that the user keeps up the interaction as long as all requested informa-tion is provided."],"2133425506":["tions, where a robot has to handle dynamically changing environmental impacts to fulfill its task, a robot may depend on asking humans for missing task knowledge [1,8,34,35]."],"2135115712":["Additionally, these five constructs are enhanced by two more constructs, which were developed in previous works [20], measuring the induced scope of (situational) empathy towards a robot, and the subjective system-performance perceived by the user.","Along these lines, prior studies have shown that adaptive implicit emotional robotic facial expressions have a significantly positive impact on empathy towards a robot, as well as on other dimensions of user experience in HRI [20]."],"2144275181":["One example is the “Interactive Urban Robot (IURO)” project [8], a social robot capable of proactively acquiring directional information from humans in order to achieve its objective to navigate to goal locations in urban environments, e.","the robot head EDDIE, with 23 degrees of freedom, mixing anthropomorphic (human-shaped) and zoomorphic (animal-shaped) features [41], and the head of the IURO platform [8].","tions, where a robot has to handle dynamically changing environmental impacts to fulfill its task, a robot may depend on asking humans for missing task knowledge [1,8,34,35]."],"2145467192":["” This holds equally true for HRI, where beliefs about the other’s mind are also resulting from interpretation of the other’s behavior that becomes a “sign” of their own minds, by means of implicit and explicit ways of communication [10]."],"2145802841":["This ability is already developed in infants [5] and dysfunctions in feeling empathy might lead to social deficits, as observed in autism [15]."],"2165287812":["by body language [4,13] or changing eye-colors that are matched to different emotional states [23]."],"2167483262":["Dispositional empathy is measured based on the Toronto Empathy Questionnaire (TEQ) [43] and current mood on pleasure-, arousal- and dominance scales based on the Self-assessment Manikin (SAM) [7], a quick graphical questionnaire with high reliability."],"2169749310":["The importance of such “mutual beliefs” in natural language communication is instantiated in the phenomenon of “grounding” [12], meaning that the interpretation of communicated contents has to be at least “approximately correct” in order to achieve successful communication acts, based on a common underlying field of knowledge and/or required actions [45]."],"2182117442":["In applications where (robotic) systems provide human users with information [25,26,39], it is self-evident that the user keeps up the interaction as long as all requested informa-tion is provided."],"2468378424":["A more recent study introduced pluggable eyebrows that turned out to significantly increase the recognition rate of emotions, depicted by NAO in a user study, employing psychology students without any prior robot experience as human test subjects [16]."]},"abstract":"Emotional adaptation increases pro-social behavior of humans towards robotic interaction partners. Social cues are an important factor in this context. This work investigates, if emotional adaptation still works under absence of human-like facial Action Units. A human-robot dialog scenario is chosen using NAO pretending to work for a supermarket and involving humans providing object names to the robot for training purposes. In a user study, two conditions are implemented with or without explicit emotional adaptation of NAO to the human user in a between-subjects design. Evaluations of user experience and acceptance are conducted based on evaluated measures of human-robot interaction (HRI). The results of the user study reveal a significant increase of helpfulness (number of named objects), anthropomorphism, and empathy in the explicit emotional adaptation condition even without social cues of facial Action Units, but only in case of prior robot contact of the test persons. Otherwise, an opposite effect is found. These findings suggest, that reduction of these social cues can be overcome by robot experience prior to the interaction task, e.g. realizable by an additional bonding phase, confirming the importance of such from previous work. Additionally, an interaction with academic background of the participants is found."},{"id":2935842681,"microsoftAcademicId":2935842681,"numberInSourceReferences":46,"doi":"10.1145/3313991.3314018","title":"Development of a Cloud-based Computational Framework for an Empathetic Robot","authors":[{"LN":"Salaken","FN":"Syed Moshfeq","affil":"Deakin University"},{"LN":"Nahavandi","FN":"Saeid","affil":"Deakin University"},{"LN":"McGinn","FN":"Conor","affil":"Trinity College, Dublin"},{"LN":"Hossny","FN":"Mohammed","affil":"Deakin University"},{"LN":"Kelly","FN":"Kevin","affil":"Trinity College, Dublin"},{"LN":"Abobakr","FN":"Ahmed","affil":"Deakin University"},{"LN":"Nahavandi","FN":"Darius","affil":"Deakin University"},{"LN":"Iskander","FN":"Julie","affil":"Deakin University"}],"year":2019,"journal":"Proceedings of the 2019 11th International Conference on Computer and Automation Engineering","references":[1522301498,2336416123,1999156278,2297172695,2085302753,2141175342,1497336187,142020493,1979295420,164400977,1541288193,2008731318,2141112954,2472568752,1985762823,2126613127,2730698429,2588812027,2889241684,2097853126,2748823972,2108248639,1516001954],"citationsCount":0,"abstract":"This article presents the development and preliminary evaluation of an empathy controlled robot. Such a robot is one step forward towards industry 5.0, as it provides a theoretical framework to enable the performance of the robot to be customized to suit the needs of both the task as well as the operator. An inventive step is taken through the separation of computational resources based on whether the algorithms are addressing functional or experiential needs. The paper therefore addresses the requirement for new approaches that can be employed in the design of mobile robots to reduce cost, power consumption, and computational burden of the system. We propose that tasks requiring real-time and safety critical control are processed using dedicated on-board computers, whereas functionality dedicated to system optimization, machine learning and customization are handled through use a cloud-based platform. In this paper, key components of the architecture are defined, and the development and preliminary evaluation of an exemplar robot capable of changing its behavior in accordance with the perceived emotional state of an operator's voice is presented."},{"id":2529322580,"microsoftAcademicId":2529322580,"numberInSourceReferences":124,"doi":"10.1007/978-3-319-42975-5_24","title":"Robot as Tutee","authors":[{"LN":"Pareto","FN":"Lena","affil":"University College West"}],"year":2017,"journal":"7th International Conference on Robotics in Education, RiE 2016; Vienna; Austria; 14 April 2016 through 15 April 2016","references":[2135943618,2105335341,2120040514,2098555131,2110437072,2105507325,2053976921,1965551836,2017954468,2301621274,1968621787,2135179426,1846888044,2013364658,2049323762,2197461842,188510888,2126304672,202359184,2046532794],"citationsCount":3,"abstract":"This paper explores the possible advantages of substituting teachable agents in a learning environment, with a humanoid robot as the non-human tutee. Teachable agents are used as an extension to educational games in order to leverage engagement, reflection and learning. The learning environment is engaging and shown to be effective for learning and promote self-efficacy in experimental studies in authentic classroom settings. Features beneficial for learning which are further enhanced by a robot compared to an agent are identified. These include embodiment of the robot; a social, empathic behaviour, better conversational abilities which together provide a better role model of an ideal learner for the student to identify with."},{"id":2570599691,"microsoftAcademicId":2570599691,"numberInSourceReferences":33,"doi":"10.1109/COGINFOCOM.2016.7804526","title":"Compassion, empathy and sympathy expression features in affective robotics","authors":[{"LN":"Lewandowska-Tomaszczyk","FN":"Barbara","affil":"Department of Translation, State University of Applied Sciences in Konin, Konin, Poland"},{"LN":"Wilson","FN":"Paul A.","affil":"University of Łódź"}],"year":2016,"journal":"2016 7th IEEE International Conference on Cognitive Infocommunications (CogInfoCom)","references":[2185859000,2102998034,3129857645,1880373995,2140054227,1630385374,3041448632,1486431372,1480715631,2005094357,2119460884,2078841787],"citationsCount":7,"citationContext":{"1480715631":["In Polish, sympatia is defined as uczucie polegające na tym, że się kogoś lub coś lubi; życzliwy, przyjazny, przychylny stosunek do kogoś, czegoś; pociąg do kogoś [13], i."],"1880373995":["This methodology offers an in depth analysis as it enables emotion conceptualisations to be compared across cultures on all six of the emotion categories identified by emotion scholars [4, 5, 6]."],"2005094357":["The methodology has also been employed to analyse the conceptual structure of pleasure [11]."],"2078841787":["In [2] the cross-cultural results indicate that the profiles of the properties need to be tuned in robots to make them emotionally competent in different cultures.","The argument presented by us in [2] for the profile of expression features to be tuned in robots to make them emotionally competent in different cultures, has been demonstrated to be valid here as presented in the converging evidence from the three methodologies used."],"2102998034":["This methodology offers an in depth analysis as it enables emotion conceptualisations to be compared across cultures on all six of the emotion categories identified by emotion scholars [4, 5, 6]."],"2119460884":["used this methodology to compare the conceptual structure of Dutch and Indonesian emotions [10]."],"2140054227":["The GRID instrument was completed by participants online [7]."],"2185859000":["Our study can contribute towards engineering applications in which a robotic system is enabled to infocommunicate with a human cognitive system via a sensor-bridging type of information transfer [14]."],"3041448632":["GRID The GRID methodology [3] uses a system of dimensions"]},"abstract":"The present paper identifies differences in the expression features of compassion, sympathy and empathy in British English and Polish that need to be tuned accordingly in socially interactive robots to enable them to operate successfully in these cultures. The results showed that English compassion is characterised by more positive valence and more of a desire to act than Polish wspolczucie. Polish empatia is also characterised by a more negative valence than English empathy, which has a wider range of application. When used in positive contexts, English sympathy corresponds to Polish sympatia; however, it also acquires elements of negative valence in English. The results further showed that although the processes of emotion recognition and expression in robotics must be tuned to culture-specific emotion models, the more explicit patterns of responsiveness (British English for the compassion model in our case) is also recommended for the transfer to make the cognitive and sensory infocommunication more readily interpretable by the interacting agents."},{"id":2990152296,"microsoftAcademicId":2990152296,"numberInSourceReferences":177,"doi":"10.5840/TECHNE20191126106","title":"Motions with Emotions? : A Phenomenological Approach to Understanding the Simulated Aliveness of a Robot Body","authors":[{"LN":"Parviainen","FN":"Jaana"},{"LN":"Aerschot","FN":"Lina Van"},{"LN":"Särkikoski","FN":"Tuomo"},{"LN":"Pekkarinen","FN":"Satu"},{"LN":"Melkas","FN":"Helinä"},{"LN":"Hennala","FN":"Lea"}],"year":2019,"journal":"Techné: Research in Philosophy and Technology","references":[],"citationsCount":6},{"id":3089272957,"microsoftAcademicId":3089272957,"numberInSourceReferences":68,"doi":"10.1007/S12369-020-00683-4","title":"A Reinforcement Learning Based Cognitive Empathy Framework for Social Robots","authors":[{"LN":"Bagheri","FN":"Elahe","affil":"Vrije Universiteit Brussel"},{"LN":"Roesler","FN":"Oliver","affil":"Vrije Universiteit Brussel"},{"LN":"Cao","FN":"Hoang-Long","affil":"Vrije Universiteit Brussel"},{"LN":"Vanderborght","FN":"Bram","affil":"Vrije Universiteit Brussel"}],"year":2020,"journal":"International Journal of Social Robotics","references":[1570448133,2121863487,2042962409,2111040806,2137577228,2082494017,1980083892,2871950322,2000476293,2010943014,1975804500,2153454886,2011955465,2054832392,2053782908,1981509185,2080184306,2804718622,2950591343,2075446845,2096210009,2060427350,1966237985,2078948548],"citationsCount":1,"abstract":"Robots that express human’s social norms, like empathy, are perceived as more friendly, understanding, and caring. However, appropriate human-like empathic behaviors cannot be defined in advance, instead, they must be learned through daily interaction with humans in different situations. Additionally, to learn and apply the correct behaviors, robots must be able to perceive and understand the affective states of humans. This study presents a framework to enable cognitive empathy in social robots, which uses facial emotion recognition to perceive and understand the affective states of human users. The perceived affective state is then provided to a reinforcement learning model to enable a robot to learn the most appropriate empathic behaviors for different states. The proposed framework has been evaluated through an experiment between 28 individual humans and the humanoid robot Pepper. The results show that by applying empathic behaviors selected by the employed learning model, the robot is able to provide participants comfort and confidence and help them enjoy and feel better."},{"id":2956276402,"microsoftAcademicId":2956276402,"numberInSourceReferences":61,"doi":"10.1007/978-3-030-22335-9_24","title":"User Experience Study: The Service Expectation of Hotel Guests to the Utilization of AI-Based Service Robot in Full-Service Hotels","authors":[{"LN":"Zhang","FN":"Yaozhi","affil":"Institute for Tourism Studies, Macao"},{"LN":"Qi","FN":"Shanshan","affil":"Institute for Tourism Studies, Macao"}],"year":2019,"journal":"International Conference on Human-Computer Interaction","references":[1791587663,1605991455,17805075,2160957486,2736784785,2091517176,2946456350,1554282264,2265243895,2013456722,1545918959,2967743314,2482187764],"citationsCount":3,"abstract":"With the dramatic development of AI technology, the concept of robotic hotel is entering the public’s awareness. Although AI application brings in high efficiency, low labor cost and novelty, practical operation of robotic hotels still faces with challenges. This quantitative research aims at understanding the current user expectation level of AI robotic hotel and robot appliance. Based on that, it tries to make the user classification by demographic, behavioral and attitude factors. By using the refined SERVQUAL model, it gathers the expectation from five dimensions involving tangibles, reliability, responsiveness, assurance and empathy."},{"id":3107110953,"microsoftAcademicId":3107110953,"numberInSourceReferences":75,"doi":"10.1016/J.NEUROPSYCHOLOGIA.2020.107695","title":"EEG based functional connectivity analysis of human pain empathy towards humans and robots.","authors":[{"LN":"Chang","FN":"Wenwen","affil":"Lanzhou Jiaotong University"},{"LN":"Wang","FN":"Hong","affil":"Northeastern University (China)"},{"LN":"Yan","FN":"Guanghui","affil":"Lanzhou Jiaotong University"},{"LN":"Lu","FN":"Zhiguo","affil":"Northeastern University (China)"},{"LN":"Liu","FN":"Chong","affil":"Northeastern University (China)"},{"LN":"Hua","FN":"Chengcheng","affil":"Nanjing University of Information Science and Technology"}],"year":2021,"journal":"Neuropsychologia","references":[1999653836,2167822639,1992745256,2096476371,2118323338,1853322427,2071869259,2058521226,1987387046,1980083892,2139749000,2015218205,2144872492,2056874158,1965886953,2055344521,2005727185,2095962545,2085580706,2023967132,1999174202,2163150496,2083505311,2003350835,2161917637,2041684398,2111385371,2003689160,2090114941,1977851920,2117228279,1981509185,2140189272,1847847467,2028424602,2092206153,2292373711,2116888785,2282380228,2012511508,2250311903,2036359025,2162007873,2035347516,2232793048,2267653472,2419209887,2012164305,2061312820,2017711639,1556810297,2052730064,2551488158,2140837820,2228032610,2417306687,2518505580,2040966750],"citationsCount":1,"abstract":"Abstract  Humans can show emotional reactions toward humanoid robots, such as empathy. Previous neuroimaging studies have indicated that neural responses of empathy for others' pain are modulated by an early automatic emotional sharing and a late controlled cognitive evaluation process. Recent studies about pain empathy for robots found humans present similar empathy process towards humanoid robots under painful stimuli as well as to humans. However, the whole-brain functional connectivity and the spatial dynamics of neural activities underlying empathic processes are still unknown. In the present study, the functional connectivity was investigated for ERPs recorded from 18 healthy adults who were presented with pictures of human hand and robot hand under painful and non-painful situations. Functional brain networks for both early and late empathy responses were constructed and a new parameter, empathy index (EI), was proposed to represent the empathy ability of humans quantitatively. We found that the mutual dependences between early ERP components was significantly decreased, but for the late components, there were no significant changes. The mutual dependences for human hand stimuli were larger than to robot hand stimuli for early components, but not for late components. The connectivity weights for early components were larger than late components. EI value shows significant difference between painful and non-painful stimuli, indicating it is a good indicator to represent the empathy of humans. This study enriches our understanding of the neurological mechanisms implicated in human empathy, and provides evidence of functional connectivity for both early and late responses of pain empathy towards humans and robots."},{"id":3134300827,"microsoftAcademicId":3134300827,"numberInSourceReferences":55,"doi":"10.1145/3434074.3447171","title":"Building a Collaborative Relationship between Human and Robot through Verbal and Non-Verbal Interaction","authors":[{"LN":"Urakami","FN":"Jacqueline","affil":"Tokyo Institute of Technology"},{"LN":"Sutthithatip","FN":"Sujitra","affil":"Tokyo Institute of Technology"}],"year":2021,"journal":"Companion of the 2021 ACM/IEEE International Conference on Human-Robot Interaction","references":[2096476371,2325035506,2011525365,2005210865,2405357702,2955727043,2978201853,3014767846],"citationsCount":0,"abstract":"Interpersonal communication and relationship building promote successful collaborations. This study investigated the effect of conversational nonverbal and verbal interactions of a robot on bonding and relationship building with a human partner. Participants interacted with two robots that differed in their nonverbal and verbal expressiveness. The interactive robot actively engaged the participant in a conversation before, during and after a collaborative task whereas the non-interactive robot remained passive. The robots' nonverbal and verbal interactions increased participants' perception of the robot as a social actor and strengthened bonding and relationship building between human and robot. The results of our study indicate that the evaluation of the collaboration improves when the robot maintains eye contact, the robot is attributed a certain personality, and the robot is perceived as being alive. Our study could not show that an interactive robot receives more help by the collaboration partner. Future research should investigate additional factors that facilitate helpful behavior among humans, such as similarity, attributional judgement and empathy."},{"id":2781760502,"microsoftAcademicId":2781760502,"numberInSourceReferences":181,"doi":"10.1590/0034-761220170333","title":"Profissionalismo público em uma era de transformações radicais: seu significado, desafios e treinamento","authors":[{"LN":"Stillman","FN":"Richard","affil":"University of Colorado Boulder"}],"year":2017,"journal":"Revista de Administração Pública","references":[],"citationsCount":0,"abstract":"Public professionalism lives within a schizophrenic world today. On one hand more people aspire to be professionals than ever before. As the late Everett Hughes, perhaps the most eminent scholar of this topic, once wrote, “professions are more numerous than ever before. Professional people are a larger portion of the labor force. The professional attitude or mood is likewise more widespread; professional status more sought after.” Everywhere in advanced and developing nations public pro­fessionals and professionalism are triumphant in contributing to the GDP growth to shaping and implementing most if not all areas of public policy."},{"id":2886814486,"microsoftAcademicId":2886814486,"numberInSourceReferences":163,"doi":"10.1017/DSJ.2018.11","title":"Application of autoencoders in cyber-empathic design","authors":[{"LN":"Ghosh","FN":"Dipanjan","affil":"University at Buffalo"},{"LN":"Olewnik","FN":"Andrew","affil":"University at Buffalo"},{"LN":"Lewis","FN":"Kemper","affil":"University at Buffalo"}],"year":2018,"journal":"Design Science","references":[2099471712,2168231600,2025768430,2130325614,1545460766,1988050849,2396976214,2122912498,2090552208,2107789863,2409589671,1971014294,2163544552,3125983421,1541296021,2165720259,2102409316,2221538265,2739810712,2560849594],"citationsCount":2,"abstract":"A critical task in product design is mapping information from the consumer space to the design space. This process is largely dependent on the designer to identify and relate psychological and consumer level factors to engineered product attributes. In this way, current methodologies lack provision to test a designer’s cognitive reasoning and may introduce bias through the mapping process. Prior work on Cyber-Empathic Design (CED) supports this mapping by relating user–product interaction data from embedded sensors to psychological constructs. To understand consumer perceptions, a network of psychological constructs is developed using Structural Equation Modeling for parameter estimation and hypothesis testing, making the framework falsifiable in nature. The focus of this technical brief is toward automating CED through unsupervised deep learning to extract features from raw data. Additionally, Partial Least Square Structural Equation Modeling is used with extracted sensor features as inputs. To demonstrate the effectiveness of the approach a case study involving sensor-integrated shoes compares three models – a survey-only model (no sensor data), the existing CED approach with manually extracted sensor features, and the proposed deep learning based CED approach. The deep learning based approach results in improved model fit."},{"id":3084602142,"microsoftAcademicId":3084602142,"numberInSourceReferences":88,"doi":"10.1007/S12369-020-00691-4","title":"Empathetic Speech Synthesis and Testing for Healthcare Robots","authors":[{"LN":"James","FN":"Jesin","affil":"University of Auckland"},{"LN":"Balamurali","FN":"B. T.","affil":"Singapore University of Technology and Design"},{"LN":"Watson","FN":"Catherine I.","affil":"University of Auckland"},{"LN":"MacDonald","FN":"Bruce","affil":"University of Auckland"}],"year":2020,"journal":"International Journal of Social Robotics","references":[273955616,1573234480,2112076978,1581387623,1966797434,1570629387,1976869056,2137577228,1967769980,2021641437,2110585328,1628789162,34686378,2095436958,2107831318,1523135049,2325035506,2094856020,2267107524,2150791533,2082173922,2572190574,2027693172,267812377,1849169576,103982469,2075132641,2136132767,2016671493,2080184306,2401838240,1990114866,2095508859,2077132576,2119553435,2185573909,2190260761,2131916103,2317990020,2935088330,2055958787,3103080153,1978952352,2126969072,2889112744,2938833595,2089750886,2899785433,1497831663,1195136826,3024566288,2575731963,2146541144,823000640,2982743323],"citationsCount":0,"abstract":"One of the major factors that affect the acceptance of robots in Human-Robot Interaction applications is the type of voice with which they interact with humans. The robot’s voice can be used to express empathy, which is an affective response of the robot to the human user. In this study, the aim is to find out if social robots with empathetic voice are acceptable for users in healthcare applications. A pilot study using an empathetic voice spoken by a voice actor was conducted. Only prosody in speech is used to express empathy here, without any visual cues. Also, the emotions needed for an empathetic voice are identified. It was found that the emotions needed are not only the stronger primary emotions, but also the nuanced secondary emotions. These emotions are then synthesised using prosody modelling. A second study, replicating the pilot test is conducted using the synthesised voices to investigate if empathy is perceived from the synthetic voice as well. This paper reports the modelling and synthesises of an empathetic voice, and experimentally shows that people prefer empathetic voice for healthcare robots. The results can be further used to develop empathetic social robots, that can improve people’s acceptance of social robots."},{"id":2978170619,"microsoftAcademicId":2978170619,"numberInSourceReferences":39,"doi":"10.1145/3349537.3351891","title":"The Role of Animism Tendencies and Empathy in Adult Evaluations of Robot","authors":[{"LN":"Okanda","FN":"Mako","affil":"Otemon Gakuin University"},{"LN":"Taniguchi","FN":"Kosuke","affil":"Doshisha University"},{"LN":"Itakura","FN":"Shoji","affil":"Doshisha University"}],"year":2019,"journal":"Proceedings of the 7th International Conference on Human-Agent Interaction","references":[2115040353,2062845262,2165113252,2064304167,1979628236,2039426506,1986558703,2107517885,2240797251,2019066729,2050747765,2765305414,2165098668,1963822947,1976201425,2120120384,2299921089,1498862882,2600836787,1975625163,3050976981,2517180834,1988031538,2080035105,2331225052],"citationsCount":5,"abstract":"We investigated whether Japanese adults' beliefs about friendship and morality toward robots differing in appearance (i.e., humanoid, dog-like, and egg-shaped) related to their animism tendencies and empathy. University students responded to questionnaires regarding three animism tendencies (i.e., general animism or a tendency to believe souls or gods in nonliving things, aliveness animism or a tendency to consider nonliving things as live entities, and agentic animisms or a tendency to attribute biological, artifactual, psychological, perceptual, and naming properties) and empathy. We found that friendship and morality were related to slightly different animism tendencies and empathy even though they shared some major factors. Aliveness animism, as well as a tendency to attribute perceptual and name properties toward robots, might be necessary for an individual to believe that robots could be social agents. Participants who responded that robots could be their friends showed a tendency to feel a soul in manmade objects and a strong self-oriented emotional reactivity, whereas participants who answered that robots were moral beings showed a tendency to exhibit strong emotional susceptibility. We discuss implications of these results and reasons why people feel that robots have a mind or consciousness."},{"id":3087295506,"microsoftAcademicId":3087295506,"numberInSourceReferences":91,"doi":"10.1002/IJOP.12715","title":"Seeing the mind of robots: Harm augments mind perception but benevolent intentions reduce dehumanisation of artificial entities in visual vignettes.","authors":[{"LN":"Küster","FN":"Dennis","affil":"University of Bremen"},{"LN":"Swiderska","FN":"Aleksandra","affil":"University of Warsaw"}],"year":2020,"journal":"International Journal of Psychology","references":[2087484885,1950970151,2124809053,2115040353,2141973273,1991015565,2131227926,2021641437,2140342020,2108616861,2100827418,2102453924,2165113252,2164318031,2132578438,2049520464,2165542916,2003350835,2616027181,2157289326,2151363564,2156311465,2152536828,2154556191,1994617002,2140244761,2260848322,2133256113,2009805354,1964426979,2020766402,2171264451,2117084973,2027337585,2282380228,2139203871,2131619946,1970278724,2153175354,1964520967,2208253782,2944379815,2117200408,2027104670,2738048415,2569790743,2490347410,2127206185,2803838348,1973808738,2095933975,1692480425,2899905311,3038189261,2969624079,3083574057,1968340857],"citationsCount":2,"abstract":"According to moral typecasting theory, good- and evil-doers (agents) interact with the recipients of their actions (patients) in a moral dyad. When this dyad is completed, mind attribution towards intentionally harmed liminal minds is enhanced. However, from a dehumanisation view, malevolent actions may instead result in a denial of humanness. To contrast both accounts, a visual vignette experiment (N = 253) depicted either malevolent or benevolent intentions towards robotic or human avatars. Additionally, we examined the role of harm-salience by showing patients as either harmed, or still unharmed. The results revealed significantly increased mind attribution towards visibly harmed patients, mediated by perceived pain and expressed empathy. Benevolent and malevolent intentions were evaluated respectively as morally right or wrong, but their impact on the patient was diminished for the robotic avatar. Contrary to dehumanisation predictions, our manipulation of intentions failed to affect mind perception. Nonetheless, benevolent intentions reduced dehumanisation of the patients. Moreover, when pain and empathy were statistically controlled, the effect of intentions on mind perception was mediated by dehumanisation. These findings suggest that perceived intentions might only be indirectly tied to mind perception, and that their role may be better understood when additionally accounting for empathy and dehumanisation."},{"id":2587739913,"microsoftAcademicId":2587739913,"numberInSourceReferences":13,"doi":"10.1109/SII.2016.7844051","title":"A laughing-driven pupil response system for inducing empathy","authors":[{"LN":"Egawa","FN":"Shoichi","affil":"Okayama Prefectural University"},{"LN":"Sejima","FN":"Yoshihiro","affil":"Okayama Prefectural University"},{"LN":"Sato","FN":"Yoichiro","affil":"Okayama Prefectural University"},{"LN":"Watanabe","FN":"Tomio","affil":"Okayama Prefectural University"}],"year":2016,"journal":"2016 IEEE/SICE International Symposium on System Integration (SII)","references":[2067169334,165886685,2094536313,2143998471,2075745499,2474020747,2106938390,1979675537,2621804265,2992557520,2326133752,2055864799,2977272052,2067793962],"citationsCount":3,"citationContext":{"1979675537":["In addition, the human pupil dilates in enhancing own affects, and it has effects on the favorable impressions [9]-[11]."],"2067169334":["Interaction Model As the estimated model of laughing response, we introduced a nodding response model based on speech input [12]."],"2075745499":["In previous researches, when the robot motions with laughing voice were synchronized, humans felt like laughing as affective expressions [17]."],"2094536313":["9 shows the calculated results of the evaluation provided in Table III and based on the BradleyTerry model given in Equations (4) and (5) [15], [16]."],"2106938390":["The exaggeration of pupil response was effective for supporting human interaction and communication [5]."],"2326133752":["the adult men gave a favorite impression in the previous research [14]."],"2474020747":["In particular, we developed an embodied communication system with a pupil response which is closely related to the human emotion such as degree of interest and stress, and demonstrated the effects of favorable impression [4]."]},"abstract":"Laughing response plays an important role in supporting human interaction and communication, and enhances empathy by sharing laughter each other. Therefore, in order to develop communication systems which enhance empathy, it is desired to design the media representation using the pupil response which is related to affective response such as pleasure-unpleasure. In this paper, we aim to enhance empathy during human and robot interaction and communication, and develop a pupil response system for inducing empathy by laughing response using hemispherical display. In addition, we evaluate the pupil response with the laughing response by using the developed system. The results demonstrate that the dilated pupil response with laughing response is effective for enhancing empathy."},{"id":2528708407,"microsoftAcademicId":2528708407,"numberInSourceReferences":150,"doi":"10.1007/978-3-319-47437-3_26","title":"MuDERI: Multimodal Database for Emotion Recognition Among Intellectually Disabled Individuals","authors":[{"LN":"Shukla","FN":"Jainendra","affil":"Rovira i Virgili University"},{"LN":"Barreda-Ángeles","FN":"Miguel","affil":"Eurecat, Technology Centre of Catalonia"},{"LN":"Oliver","FN":"Joan","affil":"Instituto de Robótica para la Dependencia"},{"LN":"Puig","FN":"Domènec","affil":"Rovira i Virgili University"}],"year":2016,"journal":"8th International Conference on Social Robotics, ICSR 2016","references":[2128495200,2002055708,2122098299,1966797434,2132555391,2001097956,2120339204,1556578540,2128561111,2007041320,1965696296,2019771222,2119915586,2291663305,2019871559,2009806533,2076904349,2216822410,2293312268],"citationsCount":5,"abstract":"Social robots with empathic interaction is a crucial requirement towards deliverance of an effective cognitive stimulation among individuals with Intellectual Disability (ID) and has been challenged by absence of any particular database. Project REHABIBOTICS presents a first ever multimodal database of individuals with ID, recorded in a nearly real world settings for analysis of human affective states. MuDERI is an annotated multimodal database of audiovisual recordings, RGB-D videos and physiological signals of 12 participants in actual settings, which were recorded as participants were elicited using personalized real world objects and/or activities. The database is publicly available."},{"id":2764000623,"microsoftAcademicId":2764000623,"numberInSourceReferences":168,"doi":"10.1109/CC.2017.8068769","title":"Empathizing with emotional robot based on cognition reappraisal","authors":[{"LN":"Liu","FN":"Xin","affil":"University of Science and Technology Beijing"},{"LN":"Xie","FN":"Lun","affil":"University of Science and Technology Beijing"},{"LN":"Wang","FN":"Zhiliang","affil":"University of Science and Technology Beijing"}],"year":2017,"journal":"China Communications","references":[1849553904,1975000068,2490787411,1994405094,2055695864,2127536482,2003653478,2540108576,1998814105,1791377278,2110583230,2086055136,2052730064,2005882909,2078833921,364462331,2227884176],"citationsCount":5,"citationContext":{"364462331":[") in the interaction[21]."],"1791377278":["The rest of this paper is organized as follows: Section 2 discusses the mechanimented with innate personality and acquired learning[9]."],"1975000068":["According to Hoffman, empathy is “an affective response more appropriate to another’s situation than to one’s own[20]."],"1994405094":["In order to show the emotional properties, the classical 3D emotional space, pleasant/unpleasant, excitement/depression, tension/relaxation was used by Wundt and a large number of emotional dimensional theories have been proposed over the years[10-14]."],"1998814105":["” D’Amvrosio pointed out that we can feel empathy from two parts: cognitive empathy and affective empathy [23]."],"2003653478":["One of the most accepted theories is Pleasure-Arousal-Dominance (PAD) space[15]."],"2005882909":["In order to show the emotional properties, the classical 3D emotional space, pleasant/unpleasant, excitement/depression, tension/relaxation was used by Wundt and a large number of emotional dimensional theories have been proposed over the years[10-14]."],"2052730064":["Robot will evolve into intelligent agents with anthropomorphic and diversified emotions, and even will begin to communicate with empathy[2]."],"2055695864":["Based on the facial expressions, Ekman proposed six prototypical emotions organized in discrete model[3] , and this analogous approach was followed by several authors[4-8]."],"2078833921":["In order to show the emotional properties, the classical 3D emotional space, pleasant/unpleasant, excitement/depression, tension/relaxation was used by Wundt and a large number of emotional dimensional theories have been proposed over the years[10-14]."],"2110583230":["Based on the facial expressions, Ekman proposed six prototypical emotions organized in discrete model[3] , and this analogous approach was followed by several authors[4-8]."],"2127536482":["In addition, Breazeal’s Arousal-Valence-Stance (AVS) space with the social robot Kismet was a noteworthy emotional space model[19]."],"2490787411":["Based on the facial expressions, Ekman proposed six prototypical emotions organized in discrete model[3] , and this analogous approach was followed by several authors[4-8]."],"2540108576":["Miwa constructed the 3D psychological vector space, Arousal-Pleasant-Certain, with machine learning, dynamic regulation and personality[18]."]},"abstract":"This paper proposes a continuous cognitive emotional regulation model for robot in the case of external emotional stimulus from interactive person's expressions. It integrates a guiding cognitive reappraisal strategy into the HMM (Hidden Markov Model) emotional interactive model for empathizing between robot and person. The emotion is considered as a source in the 3D space (Arousal, Valence, and Stance). State transition and emotion intensity can be quantitatively analyzed in the continuous space. This cognition-emotion interactive model have been verified by the expression and behavior robot. Empathizing is the main distinguishing feature of our work, and it is realized by the emotional regulation which operated in a continuous 3D emotional space enabling a wide range of intermediate emotions. The experiment results provide evidence with acceptability, accuracy, richness, fluency, interestingness, friendliness and exaggeration that the robot with cognition and emotional control ability could be better accepted in the human-robot interaction (HRI)."},{"id":2611261464,"microsoftAcademicId":2611261464,"numberInSourceReferences":2,"doi":"10.1109/RCAR.2017.8311945","title":"Evolving artificial pain from fault detection through pattern data analysis","authors":[{"LN":"Anshar","FN":"Muh","affil":"Faculty of Electrical Engineering, University of Hasanuddin UNHAS, Makassar, Indonesia"},{"LN":"Williams","FN":"Mary-Anne","affil":"Faculty of Engineering and Information Technology Sydney UTS, Sydney, Australia"}],"year":2017,"journal":"2017 IEEE International Conference on Real-time Computing and Robotics (RCAR)","references":[2158454296,106053879,2044223889,1530597628,2107329027,2147072697,2062665748,2272071639,1851656425,1891675889,2000428182,2147773288,2152685348],"citationsCount":1,"citationContext":{"106053879":["Earlier studies, reported in [1], [2], [3] provide the foundation for the importance of incorporating failure detection into robot planning mechanisms."],"1530597628":["At early implementation, the ASAF framework utilise the sequential pattern prediction [14], [15] in order to capture the behaviour of the observed real data and then use them to predict the future possible conditions."],"1851656425":["Various aspects of robot motion planning are investigated in [4], [5], [6], [7], [8] and extensions the scope to multiple robot planning [9], [10]."],"1891675889":["Various aspects of robot motion planning are investigated in [4], [5], [6], [7], [8] and extensions the scope to multiple robot planning [9], [10]."],"2000428182":["Earlier studies, reported in [1], [2], [3] provide the foundation for the importance of incorporating failure detection into robot planning mechanisms."],"2062665748":["Various aspects of robot motion planning are investigated in [4], [5], [6], [7], [8] and extensions the scope to multiple robot planning [9], [10]."],"2107329027":["Various aspects of robot motion planning are investigated in [4], [5], [6], [7], [8] and extensions the scope to multiple robot planning [9], [10]."],"2147072697":["Earlier studies, reported in [1], [2], [3] provide the foundation for the importance of incorporating failure detection into robot planning mechanisms."],"2147773288":["Various aspects of robot motion planning are investigated in [4], [5], [6], [7], [8] and extensions the scope to multiple robot planning [9], [10]."],"2152685348":["Various aspects of robot motion planning are investigated in [4], [5], [6], [7], [8] and extensions the scope to multiple robot planning [9], [10]."],"2272071639":["Artificial pain definition is integrated into a NAO robot mechanism, which utilise our novel Adaptive Self-awareness framework for robots (ASAF) [13].","Our artificial pain for robots contains three classifications of synthetic pain de-rived from the report [13], described as follows:"]},"abstract":"Fault detection is a classical area of study in robotics and extensive research works have been dedicated to investigate its broad applications. As the breath of robots applications requiring human interaction grow, it is important for robots to acquire sophisticated social skills such as empathy towards pain. However, it turns out that this is difficult to achieve without having an appropriate concept of pain that relies on robots being aware of their own body machinery aspects. This paper introduces the concept of pain, based on the ability to develop a state of awareness of robots own body and the use of the fault detection approach to generate artificial robot pain. Faults provide the stimulus and defines a classified magnitude value, which constitutes artificial pain generation, comprised of synthetic pain classes. Our experiment evaluates some of synthetic pain classes and the results show that the robot gains awareness of its internal state through its ability to predict its joint motion and generate appropriate artificial pain. The robot is also capable of alerting humans whenever a task will generate artificial pain, or whenever humans fails to acknowledge the alert, the robot can take a considerable preventive actions through joint stiffness adjustment."},{"id":2508565453,"microsoftAcademicId":2508565453,"numberInSourceReferences":126,"doi":"10.1007/978-3-319-44188-7_6","title":"Emotion Recognition Using Facial Expression Images for a Robotic Companion","authors":[{"LN":"Ruiz-Garcia","FN":"Ariel","affil":"Coventry University"},{"LN":"Elshaw","FN":"Mark","affil":"Coventry University"},{"LN":"Altahhan","FN":"Abdulrahman","affil":"Coventry University"},{"LN":"Palade","FN":"Vasile","affil":"Coventry University"}],"year":2016,"journal":"International Conference on Engineering Applications of Neural Networks","references":[2798766386,2116146623,2143908786,2144354855,1989477529,2251198138,2277498883,2139749000,2149260122,2191045312,1998820849,2055695864,2117648711,2176759570,2204977270,1981509185,2024157222,2132326707,2579873239,2009659028,2278023796,253816493,2056450798,1997267722,2338845825,2166574436,2547654653,1968985103,2188437042,1999468048,2058465242,2543298907,1821680514],"citationsCount":5,"abstract":"Social robots are gradually becoming part of society. However, social robots lack the ability to adequately interact with users in a natural manner and are in need of more human-like abilities. In this paper we present experimental results on emotion recognition through the use of facial expression images obtained from the KDEF database, a fundamental first step towards the development of an empathic social robot. We compare the performance of Support Vector Machines (SVM) and a Multilayer Perceptron Network (MLP) on facial expression classification. We employ Gabor filters as an image pre-processing step before classification. Our SVM model achieves an accuracy rate of 97.08 %, whereas our MLP achieves 93.5 %. These experiments serve as benchmark for our current research project in the area of social robotics."},{"id":2899525949,"microsoftAcademicId":2899525949,"numberInSourceReferences":144,"doi":"10.1109/ROMAN.2018.8525579","title":"Emotion Differentiation based on Decision-Making in Emotion Model","authors":[{"LN":"Hieida","FN":"Chie","affil":"University of Electro-Communications"},{"LN":"Horii","FN":"Takato","affil":"University of Electro-Communications"},{"LN":"Nagai","FN":"Takayuki","affil":"University of Electro-Communications"}],"year":2018,"journal":"2018 27th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)","references":[2339343773,1748703215,1984186949,1595732857,2149628368,1620203002,2003238582,2346811982,2096210009,2006681446,2149820118,2594064817,2126642912,2011722134,2334482187,1999543972,2045981909],"citationsCount":1,"citationContext":{"1595732857":["Toward this goal, many efforts have been made to design emotional expressions for robots [10]."],"1620203002":["From these viewpoints, our model is closely related to the Embodied Predictive Interoception Coding (EPIC) model, which was proposed recently [27]."],"1748703215":["Ledoux revealed that the parts of the brain employed during the sensation of pleasant emotions are different from those used during the sensation of unpleasant ones [23]."],"1984186949":["Cognitive theory is also famous for incorporating cognitive activities in the form of judgments, evaluations, or thoughts [6], [7]."],"1999543972":["In the area of cognitive neuroscience, many models have been proposed in the literature [4], [5]."],"2003238582":["Despite the differentiating property of emotions, the six basic emotions—anger, joy, disgust, fear, sorrow, and surprise—exist regardless of culture [3].","These observations are very important, coupled with the fact that there are six basic emotions regardless of culture, as shown by Ekman [3]."],"2006681446":["In this regard, the idea of “homeostasis,” which is a regulatory mechanism of the agent’s internal state [25], is adopted."],"2011722134":["For example, the dimensional model is well known among emotion expression studies [1], [2]."],"2045981909":["Emotions in literature Based on observations of infants, Bridges claimed that excitement, which is the origin of emotion, can be divided into several emotional categories [17].","” If we move away from manual design of emotions, emotion differentiation [17] could be an alternative approach to achieve this goal."],"2096210009":["This idea shares the same goal as the affective developmental robotics proposed by Asada [18]."],"2126642912":["In fact, conventional studies have only been able to accomplish simple basic emotions such as happiness and sadness [14], [15]."],"2149820118":["Since both of the experiences occurred simultaneously, the subject mistakenly relates the physical responses of fear to the arousal of meeting the other person [24]."],"2334482187":["The idea of intrinsic motivation that appeared as a series of counterarguments to drive reduction theory cannot be ignored [30]."],"2339343773":["Many researchers have studied emotion recognition in humans by examining facial expressions [9]."],"2346811982":["We are currently working on the “emotional symbol grounding problem” using the idea of language acquisition by robots [32]."],"2594064817":["First layer implemented by recurrent attention model: (a) IAPS[11], (b) RAM[12], and (c) Jaffe[13].","Recurrent attention model for affect generation (1st layer) As a first step toward realizing the proposed emotion model, we examined affect generation from visual stimuli using a neural network [12]."]},"abstract":"Having emotions is essential for robots in order for them to understand and sympathize with people's feelings. In addition, it may allow robots to be accepted in human society. The role of emotions in decision-making is another important perspective. In this paper, a model of emotions is proposed based on various neurological and psychological findings related to empathic communication between humans and robots. Subsequently, a decision-making mechanism based on affects using convolutional long short-term memory and deep deterministic policy gradient is examined. We set a “facial expression” task simulating mother-child interactions and verified emotion differentiation during the task."},{"id":3133317345,"microsoftAcademicId":3133317345,"numberInSourceReferences":151,"doi":"10.1093/JIGPAL/JZAA047","title":"A composite framework for supporting user emotion detection based on intelligent taxonomy handling","authors":[{"LN":"Cuzzocrea","FN":"Alfredo","affil":"University of Calabria"},{"LN":"Pilato","FN":"Giovanni","affil":"Indian Council of Agricultural Research"}],"year":2021,"journal":"Logic Journal of the IGPL","references":[1880262756,2126581182,1983578042,2158997610,2079521622,2135054431,2084626209,2096931991,2003238582,1562711890,2021668342,1963501922,2741860375,343057247,1992396738,2485539054,156035072,2040673088,2794009674,2148128819,299861242,2806264485],"citationsCount":0},{"id":2903301630,"microsoftAcademicId":2903301630,"numberInSourceReferences":29,"doi":"10.1007/978-3-030-05204-1_6","title":"Smiles of Children with ASD May Facilitate Helping Behaviors to the Robot","authors":[{"LN":"Kim","FN":"SunKyoung","affil":"University of Tsukuba"},{"LN":"Hirokawa","FN":"Masakazu","affil":"University of Tsukuba"},{"LN":"Matsuda","FN":"Soichiro","affil":"University of Tsukuba"},{"LN":"Funahashi","FN":"Atsushi","affil":"Nippon Sport Science University"},{"LN":"Suzuki","FN":"Kenji","affil":"University of Tsukuba"}],"year":2018,"journal":"International Conference on Social Robotics","references":[2050363967,2521535695,2133493571,2124156181,2009698817,2116096794,2141208341,2063238761,1997662771,1447379756,1982959085,2080929314,2028429411,2724718656,2017133406,2036323635,2343123216,2603377555,1979023596,2085353869,2279878284,2092824096,2808142914,2608493933,2045335190,1966513193,2002362732,170120],"citationsCount":0,"abstract":"Helping behaviors are one of the important prosocial behaviors in order to develop social communication skills based on empathy. In this study, we examined the potentials of using a robot as a recipient of help, and helping behaviors to a robot. Also, we explored the relationships between helping behaviors and smiles that is an indicator of a positive mood. The results of this study showed that there might be a positive correlation between the amount of helping behaviors and the number of smiles. It implies that smiles may facilitate helping behaviors to the robot. This preliminary research indicates the potentials of robot-assisted interventions to facilitate and increase helping behaviors of children with Autism Spectrum Disorder (ASD)."},{"id":2107915637,"microsoftAcademicId":2107915637,"numberInSourceReferences":133,"doi":"10.1109/ROMAN.2015.7333634","title":"Social and empathic behaviours: Novel interfaces and interaction modalities","authors":[{"LN":"Marti","FN":"Patrizia","affil":"Eindhoven University of Technology"},{"LN":"Iacono","FN":"Iolanda","affil":"University of Siena"}],"year":2015,"journal":"2015 24th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)","references":[2099019320,1933657216,2004603793,2037121244,1980083892,2021247507,2123952992,13388985,2124937956,1973974771,2076371757,2155015876,1585085236,1599635342,2979429647,2063189411,2013918364,2021568713,2057848277,1788790479,2045403074,2132088227,2492912707,2170616035,2009166149,3128134025,3176776235],"citationsCount":3,"citationContext":{"13388985":["In the field of assistive robotics, Tapus and Matarić [7] developed a model of empathic interaction based on verbal and non-verbal communication with robots."],"2009166149":["Early definitions broadly see empathy as taking the role of the other [11]; as a vicarious introspection [12]; and emotional knowing [13]."],"2013918364":["attraction, ranging from negative to positive, pleasant or unpleasant) and arousal (level of stimulation, ranging from low to high, calm or arousal) [29, 30], “Fig."],"2037121244":["The poster depicted an affective space, adapted by [29], “Fig.","attraction, ranging from negative to positive, pleasant or unpleasant) and arousal (level of stimulation, ranging from low to high, calm or arousal) [29, 30], “Fig."],"2057848277":["A third dimension was suggested by neuropsychologists who proposed to include motor empathy as an additional facet [18]."],"2099019320":["The behavioural features that are usually implemented in virtual assistants and social robots to stimulate empathy include [10]:"],"2123952992":["[9] investigated if computer-based personal assistants that express social behaviours and empathy can help older adults to behave healthy."],"2492912707":["More recently empathy has been conceptualized as a multidimensional construct having two main dimensions: affective and cognitive [15, 16, 17]."],"2979429647":["More recently empathy has been conceptualized as a multidimensional construct having two main dimensions: affective and cognitive [15, 16, 17]."],"3128134025":["Early definitions broadly see empathy as taking the role of the other [11]; as a vicarious introspection [12]; and emotional knowing [13]."]},"abstract":"This paper describes the results of a research conducted in the European project Accompany, whose aim is to provide older people with services in a motivating and socially acceptable manner to facilitate independent living at home. The project developed a system consisting of a robotic companion, Care-O-bot, as part of a smart environment. An intensive research was conducted to investigate and experiment with robot behaviours that trigger empathic exchanges between an older person and the robot. The paper is articulated in two parts. The first part illustrates the theory that inspired the development of a context-aware Graphical User Interface (GUI) used to interact with the robot. The GUI integrates an expressive mask allowing perspective taking with the aim to stimulate empathic exchanges. The second part focuses on the user evaluation, and reports the outcomes from three different tests. The results of the first two tests show a positive acceptance of the GUI by the older people. The final test reports qualitative comments by senior participants on the occurrence of empathic exchanges with the robot."},{"id":3094215526,"microsoftAcademicId":3094215526,"numberInSourceReferences":117,"doi":"10.1109/RO-MAN47096.2020.9223488","title":"Increasing Engagement with Chameleon Robots in Bartending Services","authors":[{"LN":"Rossi","FN":"Silvia","affil":"University of Naples Federico II"},{"LN":"Dell'Aquila","FN":"Elena","affil":"University of Naples Federico II"},{"LN":"Russo","FN":"Davide","affil":"University of Naples Federico II"},{"LN":"Maggi","FN":"Gianpaolo","affil":"Seconda Università degli Studi di Napoli"}],"year":2020,"journal":"2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)","references":[2135907989,2098676269,2345729520,2049515993,2132891289,2156521625,1844298566,2080184306,2785802062,3020006123,2155572870,2963534952,2031594354,2005552354,1984775709,2133852279,2806926741,2060889508,2068607682,2994831951,3014802244,2884006050,2128151589,1987875924,3005817519,2005008087,2904318072],"citationsCount":0,"citationContext":{"1844298566":["The entertaining BRILLO was designed to express extraversion with yellow eyes (associated with the joy) [19], and with high voice frequency, and a high speaking rate [17]; moreover, to express openness, [24], it performed broad gestures with its body leaning toward the user [16]."],"1984775709":["These findings further support previous results from a study on humanhuman interaction in a bar location [13] that underlined the crucial role of non-verbal signals in the relationship between the customers and the bartender.","[13] conducted at several bar locations by recording real-life customer-staff interactions, researchers have explored how robots can recognize social signals produced by customers and respond appropriately."],"1987875924":["The entertaining BRILLO was designed to express extraversion with yellow eyes (associated with the joy) [19], and with high voice frequency, and a high speaking rate [17]; moreover, to express openness, [24], it performed broad gestures with its body leaning toward the user [16].","dialog poses and to express correspondence [24], a Nearest Centroid Classifier was implemented."],"2005008087":["In [15], Mehrabian stated that the meaning of posture is outlined through two dimensions: dominance-submission and loosening-stress."],"2005552354":[", posture, mannerism, facial expressions) present findings on the importance of mimicry in social interaction [2], [11]."],"2031594354":["to adapt [3] and to interact in a socially acceptable way [21]."],"2049515993":["In detail, the robot voice was tuned on the user’s voice frequency by using an external Python library based on a Praat script called my voice analysis [6]."],"2060889508":["More recently, in [4], Burgoon revealed that dominance is expressed by closed/tense posture and close proximity but males can also use relaxed posture to signal dominance."],"2068607682":[", posture, mannerism, facial expressions) present findings on the importance of mimicry in social interaction [2], [11]."],"2080184306":["Moreover, it had a mechanic voice obtained by setting a low frequency and a slow speed of speech [17], and no animation gestures [23].","The entertaining BRILLO was designed to express extraversion with yellow eyes (associated with the joy) [19], and with high voice frequency, and a high speaking rate [17]; moreover, to express openness, [24], it performed broad gestures with its body leaning toward the user [16]."],"2098676269":["For example, experiments of [5] have shown that behavioral mimicry happening in conversations has a significant impact on the interaction and increases empathy towards the interaction partner.","In human relationships, mimicking the conversation partner’s gestures and attitude contributes to trigger an alignment process that allows to smoothly establish a connection between people; in [5], Chartrand and Bargh defined such behavior as the chameleon effect."],"2128151589":["Other studies have shown that the chameleon effect increases the likeability of a person who mimics another during a conversation, happening both with humans and with virtual agents [25]."],"2132891289":["Moreover, it had a mechanic voice obtained by setting a low frequency and a slow speed of speech [17], and no animation gestures [23]."],"2133852279":["Finally, in [8], the authors compared two implemented methods for estimating the engagement state of customers toward a robot bartender, both based on low-level sensor data.","[12], but also when humanoids robots are deployed in service robotics context performing tasks that involve the interaction with human users [8], [9], [18].","[8] have developed a barman robot able to exhibit social behaviors and to show appropriate social skills in response to states, affects, and to deal with the uncertainty of the customers it interacts with."],"2135907989":["The design of the robot’s behavior has an impact on the human perception of the interaction as well as on trust, its acceptance, and on the evaluation of the experience [10]."],"2155572870":["In [9], by using a highlevel automated planner, a robot bartender was implemented to explore and compare two different robot interaction styles: task-only setting, in which the robot simply filled its goal of asking for orders and serving drinks to customers; and socially-intelligent setting, in which the robot used to acts in a manner socially appropriate, planning the interaction and communication with the user according to humans behavior observed in natural bar interactions.","[12], but also when humanoids robots are deployed in service robotics context performing tasks that involve the interaction with human users [8], [9], [18].","[9], Petrick and Foster [18], and Foster et al."],"2156521625":["In [18], the authors showed how social states are inferred from low-level sensors, by using vision and speech input; moreover, they presented how to use the knowledge-level planner to define strategies (task, dialogue, and social actions) to be executed on a robot platform.","[12], but also when humanoids robots are deployed in service robotics context performing tasks that involve the interaction with human users [8], [9], [18].","[9], Petrick and Foster [18], and Foster et al."],"2345729520":["The engagement was evaluated from the video recorded by Pepper using a Dimensional Engagement Observational Scale (DEOS), a modified version of the engagement scale we employed in a previous study [7], and Affdex [14], a software which detects several parameters of individuals’ facial expressions, developed by the MIT’s Media Lab originated company Affectiva."],"2785802062":["Moreover, interacting with an empathic robot, which modulates its behavior according to the user’s one, is more suitable to improve engagement and positive emotions in public-service contexts than a neutral robot [20], [22]."],"2806926741":["Since previous studies suggested the influence of non-verbal aspects of the communication on engagement during HRI [27], [21], the implementation of the proposed different"],"2884006050":["An interesting review [1] on Human Interaction Research through Ecological Grounding suggested that the relevance"],"2904318072":["Other recent researchers have demonstrated that mimicry can lead to helping behavior from the human participant that could have an important impact on the establishment of cooperative interactions between humans and humanoid robots [26]."],"2963534952":["[12], but also when humanoids robots are deployed in service robotics context performing tasks that involve the interaction with human users [8], [9], [18]."],"2994831951":["For this interaction style, BRILLO was configured with green eyes, to express trust [19], whereas its body posture and arms movements were designed to adapt to the user’s ones (detected by the use of an additional Kinect sensor) as well as the robot voice, as described in the followings.","Here, we designed the interaction styles by modeling the posture, eyes’ color (based on the color emotion wheel by Plutchik [19]), the frequency and speed of speech, and the gestures as described in the followings sections.","The entertaining BRILLO was designed to express extraversion with yellow eyes (associated with the joy) [19], and with high voice frequency, and a high speaking rate [17]; moreover, to express openness, [24], it performed broad gestures with its body leaning toward the user [16].","Therefore, it was characterized by white eyes LED, since white is not associated with any emotion [19]."],"3005817519":["The engagement was evaluated from the video recorded by Pepper using a Dimensional Engagement Observational Scale (DEOS), a modified version of the engagement scale we employed in a previous study [7], and Affdex [14], a software which detects several parameters of individuals’ facial expressions, developed by the MIT’s Media Lab originated company Affectiva."],"3014802244":["Moreover, interacting with an empathic robot, which modulates its behavior according to the user’s one, is more suitable to improve engagement and positive emotions in public-service contexts than a neutral robot [20], [22].","We extend the work presented in [20], so an experimental setup was created in which the participants engaged in a dialog with the robot that is endowed with an empathic (mimicry) interactive styles by recognizing the user pose and voice tone."],"3020006123":["Since previous studies suggested the influence of non-verbal aspects of the communication on engagement during HRI [27], [21], the implementation of the proposed different","to adapt [3] and to interact in a socially acceptable way [21]."]},"abstract":"As the field of service robotics has been rapidly growing, it is expected for such robots to be endowed with the appropriate capabilities to interact with humans in a socially acceptable way. This is particularly relevant in the case of customer relationships where a positive and affective interaction has an impact on the users’ experience. In this paper, we address the question of whether a specific behavioral style of a barman-robot, acted through para-verbal and non-verbal behaviors, can affect users’ engagement and the creation of positive emotions. To that end, we endowed a barman-robot taking drink orders from human customers, with an empathic behavioral style. This aims at triggering to alignment process by mimicking the conversation partner’s behavior. This behavioral style is compared to an entertaining style, aiming at creating a positive relationship with the users, and a neutral style for control. Results suggest that when participants experienced more positive emotions, the robot was perceived as safer, so suggesting that interactions that stimulate positive and open relations with the robot may have a positive impact on the affective dimension of engagement. Indeed, when the empathic robot modulates its behavior according to the user’s one, this interaction seems to be more effective than when interacting with a neutral robot in improving engagement and positive emotions in public-service contexts."},{"id":2899713253,"microsoftAcademicId":2899713253,"numberInSourceReferences":43,"doi":"10.1007/978-3-030-01054-6_25","title":"Evaluation of Classifiers for Emotion Detection While Performing Physical and Visual Tasks: Tower of Hanoi and IAPS","authors":[{"LN":"Qureshi","FN":"Shahnawaz","affil":"Prince of Songkla University"},{"LN":"Hagelbäck","FN":"Johan","affil":"Linnaeus University"},{"LN":"Iqbal","FN":"Syed Muhammad Zeeshan","affil":"BrightWare, Riyadh, Saudi Arabia"},{"LN":"Javaid","FN":"Hamad","affil":"Jinnah International Hospital, Abbottabad, Pakistan"},{"LN":"Lindley","FN":"Craig A.","affil":"Commonwealth Scientific and Industrial Research Organisation"}],"year":2018,"journal":"Intelligent Systems Conference (IntelliSys), 6-7 September, 2018, London","references":[2139212933,2063978378,2149298154,1817561967,2120945046,1672197616,2142197234,2165857685,2001097956,1610836425,2477400917,2912854634,2151562475,1980122010,2067583143,2149186291,2056294943,2064149108,1753891864,2140702793,2058308089,2107469377,1966555477,1542123009,1873312687,2168956721,2121302072,2040378199,2146392153,587740133,2737786689,1984542317,262581405,2139829411,2004274367,93861386,2320024059,2122242993,2001619934,1490992266,1604307741,2904448512,1864137090,2607030640,2606343283,1488525522],"citationsCount":0,"abstract":"With the advancement in robot technology, smart human-robot interaction is of increasing importance for allowing the more excellent use of robots integrated into human environments and activities. If a robot can identify emotions and intentions of a human interacting with it, interactions with humans can potentially become more natural and effective. However, mechanisms of perception and empathy used by humans to achieve this understanding may not be suitable or adequate for use within robots. Electroencephalography (EEG) can be used for recording signals revealing emotions and motivations from a human brain. This study aimed to evaluate different machine learning techniques to classify EEG data associated with specific affective/emotional states. For experimental purposes, we used visual (IAPS) and physical (Tower of Hanoi) tasks to record human emotional states in the form of EEG data. The obtained EEG data processed, formatted and evaluated using various machine learning techniques to find out which method can most accurately classify EEG data according to associated affective/emotional states. The experiment confirms the choice of a method for improving the accuracy of results. According to the results, Support Vector Machine was the first, and Regression Tree was the second best method for classifying EEG data associated with specific affective/emotional states with accuracies up to 70.00% and 60.00%, respectively. In both tasks, SVM was better in performance than RT."},{"id":2559326378,"microsoftAcademicId":2559326378,"numberInSourceReferences":51,"doi":"10.1109/SICE.2016.7749252","title":"Evaluation of a head motion synchronization system in the communicative process between human and robot","authors":[{"LN":"Sin","FN":"Yap Miao","affil":"Tokyo Institute of Technology"},{"LN":"Robin","FN":"","affil":"Tokyo Institute of Technology"},{"LN":"Liang","FN":"Qiao","affil":"Tokyo Institute of Technology"},{"LN":"Tani","FN":"Koyu","affil":"Tokyo Institute of Technology"},{"LN":"Ogawa","FN":"Ken-ichiro","affil":"Tokyo Institute of Technology"},{"LN":"Miyake","FN":"Yoshihiro","affil":"Tokyo Institute of Technology"}],"year":2016,"journal":"2016 55th Annual Conference of the Society of Instrument and Control Engineers of Japan (SICE)","references":[2014886338,111157985,2134415008,2112249672,2152168312,2028053170,2122134053,2035093031,2050378831,2072688213,2105636130],"citationsCount":0,"citationContext":{"111157985":["Model 2: Interactive model The interactive model is based on Kuramoto model [9] which consists of two main modules, namely module 1 (mutual entrainment) and module 2 (phase control).","On the other hand, mutual entrainment is a well-known phenomenon in biological systems, whereby coupled non-linear oscillators with slightly different frequencies synchronized with a constant phase difference, if the coupling becomes sufficiently strong [9]."],"2014886338":["6 million new cases of dementia every year [2]."],"2035093031":["In addition, a positive correlation between participants’ degree of positivity and body movement synchrony has been reported in teacherstudent interaction [10], and psychotherapeutic session [11]."],"2072688213":["75% among the elderly people aged 65 years and above [3]."],"2105636130":["In this regard, many social robots have been developed to provide companionship [6], communication support [7] as well as for neurocognitive impairment therapy [8].","Model 1: Reactive model The reactive model is described using a moving average model based on a previous study [7], as","These systems use a behavior-based system [6], reactive system [7], or remote-control by a third party [8] to manage the interaction between human and robot."],"2122134053":["interactive system) as an intrinsic mechanism based on an earlier framework [12]."],"2134415008":["In this regard, many social robots have been developed to provide companionship [6], communication support [7] as well as for neurocognitive impairment therapy [8].","These systems use a behavior-based system [6], reactive system [7], or remote-control by a third party [8] to manage the interaction between human and robot."]},"abstract":"An aging population is world-wide social problem which affects many developed and developing countries. In this regard, many social robots based on reactive system have been developed to provide companionship for elderly adults with neurocognitive impairments such as dementia. However, these systems remain a problem such that no interactive dynamics of body gestures between human and robot has been considered. In this research, therefore, we developed a head motion synchronization system using mutual entrainment and implement it in a communication robot. This system was evaluated by conducting one-way face-to-face human-robot communication experiments with young native Japanese speakers under three conditions, namely unreactive, reactive and interactive conditions. Head motion synchrony analysis revealed a leader-follower relationship for the reactive model and a mutual entrainment of head motion for the interactive model. Also, questionnaire survey results showed the degree of empathy for interactive condition was significantly higher as compared with reactive and unreactive conditions. In addition, the degree of naturalness for interactive condition was significantly higher as compared with unreactive condition. Hence, these indicate that empathy was shared through mutual entrainment of head motion, which could provide a smooth interface in human-robot communication. This system would be extended to elderly adults as an assistive system for the elderly's rehabilitation."},{"id":3091242709,"microsoftAcademicId":3091242709,"numberInSourceReferences":135,"doi":"10.1145/3399715.3399820","title":"DELEX: a DEep Learning Emotive eXperience: Investigating empathic HCI","authors":[{"LN":"Abate","FN":"Andrea F.","affil":"University of Salerno"},{"LN":"Castiglione","FN":"Aniello","affil":"DIST, Parthenope Univ., Naples, Italy"},{"LN":"Nappi","FN":"Michele","affil":"University of Salerno"},{"LN":"Passero","FN":"Ignazio","affil":"University of Salerno"}],"year":2020,"journal":"Proceedings of the International Conference on Advanced Visual Interfaces","references":[2136757566,2041616772,3122081138,2481681431,2040857861,2171057921,2896277673,2798371872,1615813862,1765835586,2898961902,2982383280,2133311449,2325797212,2981754449],"citationsCount":1,"abstract":"Recent advances in Machine Learning have unveiled interesting possibilities for real-time investigating about user characteristics and expressions like, but not limited to, age, sex, body posture, emotions and moods. These new opportunities lay the foundations for new HCI tools for interactive applications that adopt user emotions as a communication channel.This paper presents an Emotion Controlled User Experience that changes according to user feelings and emotions analysed at runtime. Aiming at obtaining a preliminary evaluation of the proposed ecosystem, a controlled experiment has been performed in an engineering and software development company, where 60 people have been involved as volunteers. The subjective evaluation has been based on a standard questionnaire commonly adopted for measuring user perceived sense of immersion in Virtual Environments. The results of the controlled experiment encourage further investigations strengthen by the analysis of objective performance measurements and user physiological parameters."},{"id":2504756730,"microsoftAcademicId":2504756730,"numberInSourceReferences":21,"doi":"10.1007/978-3-319-40406-6_31","title":"Expression of Emotions by a Service Robot: A Pilot Study","authors":[{"LN":"Giambattista","FN":"Angela","affil":"Seconda Università degli Studi di Napoli"},{"LN":"Teixeira","FN":"Luís","affil":"IADE – Creative University"},{"LN":"Ayanoğlu","FN":"Hande","affil":"IADE – Creative University"},{"LN":"Saraiva","FN":"Magda","affil":"IADE – Creative University"},{"LN":"Duarte","FN":"Emília","affil":"IADE – Creative University"}],"year":2016,"journal":"International Conference of Design, User Experience, and Usability","references":[1586442777,2339343773,1595732857,2156516654,2911336608,2129491278,2010060754,2084618960,2093862925,2106199249,2172090995,1545457992,2187895870,1977819647,2094747810],"citationsCount":5,"abstract":"A successful Human-Robot Interaction (HRI) depends on the empathy that the robot has the capability of instantiating on the user, namely through the expression of emotions. In this pilot study, we examined the recognition of emotions being expressed by a service robot in a virtual environment (VE), by university students. The VE was a corridor, neutral in terms of context of use. The robot’s facial expressions, body movements, and displacement were manipulated to express eight basic emotions. Results showed that participants had difficulties in recognizing the emotions (33% of success). Also, results suggested that the participants established empathy with the robot. Further work is needed to improve the emotional expression of this robot, which aims to interact with hospitalized children."},{"id":3113397980,"microsoftAcademicId":3113397980,"numberInSourceReferences":87,"doi":"10.1080/0144929X.2020.1864018","title":"Perspective-Taking of Non-Player Characters in Prosocial Virtual Reality Games: Effects on Closeness, Empathy, and Game Immersion","authors":[{"LN":"Ho","FN":"Jeffrey C. F.","affil":"Hong Kong Polytechnic University"},{"LN":"Ng","FN":"Ryan","affil":"Hong Kong Polytechnic University"}],"year":2020,"journal":"Behaviour & Information Technology","references":[2151922135,2079476777,2160144355,2096438128,2014833927,2523053374,2770233082,2150077060,2124666103,2003924484,2099519430,2144139936,2895844696,1982045155,2559271218,2120771953,2084175929,2127355726,2540428363,2045712037,2136160600,2292941895,2529885055,2152550091,1486776460,2067939171,2416868893,2008629303,2885322646,2123577935,2319703347,2474166396,2102489151,2169555386,2795656167,2941310613,2761409433,1968686759,2133358297,1973808738,2081249040,2981464394,2957582866,2898606514,2020624443,2996932479,2734633677,2981806696,2997284973,2489028027,2596115138,164740630,2981643773,2084074104,2756081975,2982457786],"citationsCount":0,"abstract":"This study explores the effects of the perspective-taking of non-player characters (NPCs) on enhancing game immersion in prosocial virtual reality (VR) games. Prosocial games are games focusing on ..."},{"id":3131958508,"microsoftAcademicId":3131958508,"numberInSourceReferences":60,"doi":"10.5220/0010161801110118","title":"Personality Traits Assessment using P.A.D. Emotional Space in Human-robot Interaction.","authors":[{"LN":"Zafar","FN":"Zuhair","affil":"Kaiserslautern University of Technology"},{"LN":"Ashok","FN":"Ashita","affil":"Kaiserslautern University of Technology"},{"LN":"Berns","FN":"Karsten","affil":"Kaiserslautern University of Technology"}],"year":2021,"journal":"Proceedings of the 16th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications","references":[],"citationsCount":0},{"id":3019281085,"microsoftAcademicId":3019281085,"numberInSourceReferences":14,"doi":"10.1109/ICROM48714.2019.9071837","title":"Human-Robot Interaction based on Facial Expression Imitation","authors":[{"LN":"Esfandbod","FN":"Alireza","affil":"Sharif University of Technology"},{"LN":"Rokhi","FN":"Zeynab","affil":"Sharif University of Technology"},{"LN":"Taheri","FN":"Alireza","affil":"Sharif University of Technology"},{"LN":"Alemi","FN":"Minoo","affil":"Islamic Azad University"},{"LN":"Meghdari","FN":"Ali","affil":"Sharif University of Technology"}],"year":2019,"journal":"2019 7th International Conference on Robotics and Mechatronics (ICRoM)","references":[1686810756,2183341477,3097096317,2964350391,2150341604,2103943262,2145310492,2165731615,2233116163,2045472600,2021641437,2124773960,1650736245,2581531918,2147155775,2095809036,2063424400,2802746421,1994305689,2143875529,2004197686,2126554711,2772507957,2101783133,2763767667,2769544559,2884792551,2327030709,2098514828,2041112595,2604049780,1499437193],"citationsCount":1,"citationContext":{"1499437193":["In affective HRI, the majority of facial expression recognition systems implemented on social robots utilized hand-crafted features [20-22], these are generally inferior to the automated hierarchical features extracted via convolutional neural networks."],"1994305689":["In affective HRI, the majority of facial expression recognition systems implemented on social robots utilized hand-crafted features [20-22], these are generally inferior to the automated hierarchical features extracted via convolutional neural networks."],"2004197686":["Gabor filters [13], Local Binary Patterns [14], Histogram of Oriented Gradient [15], and Local Phase Quantization [16] are some of the methods utilized to extract Appearance features."],"2021641437":["Following advancements in the classical view of robotic systems, “socially interactive robots” with the capability of engaging social scenarios and interacting with humans have recently become a trendy topic [1]."],"2041112595":["Mimicry is an omnipresent behavior in daily interpersonal nonverbal communications, it plays an important role in establishing rapport and affiliation in human-human interactions [8]."],"2045472600":["Gabor filters [13], Local Binary Patterns [14], Histogram of Oriented Gradient [15], and Local Phase Quantization [16] are some of the methods utilized to extract Appearance features."],"2095809036":["Because of the advantages of the deep neural networks, such as automatic and efficient feature extraction, deep learning-based approaches especially “Convolutional Neural Networks” (CNNs) are able to perform successfully in diverse imagerelated applications, including FER [18, 19]."],"2098514828":["Because of the advantages of the deep neural networks, such as automatic and efficient feature extraction, deep learning-based approaches especially “Convolutional Neural Networks” (CNNs) are able to perform successfully in diverse imagerelated applications, including FER [18, 19]."],"2101783133":["In human communications facial expressions play an essential role in expressing and recognizing emotions, therefore, to enhance human-like performance these social robots need to being capable of comprehending human emotions [7]."],"2103943262":["In this paper, we utilized the Extended Cohn-Kanade (CK+) facial expression database, consisting of 327 annotated video sequences from 123 subjects, with eight expression states [28].","We utilized the well-known Extended Cohn-Kanade (CK+) facial expression database to train the model [28]."],"2124773960":["Generally, the two kinds of facial features are Geometric features and Appearance features [11]."],"2126554711":["In affective HRI, the majority of facial expression recognition systems implemented on social robots utilized hand-crafted features [20-22], these are generally inferior to the automated hierarchical features extracted via convolutional neural networks."],"2143875529":["Geometric featurebased methods extract the feature vector based on the facial element’s location [12]; whereas, Appearance feature-based approaches utilize the change in the appearance of the face in diverse emotional states to extract suitable features."],"2145310492":["Gabor filters [13], Local Binary Patterns [14], Histogram of Oriented Gradient [15], and Local Phase Quantization [16] are some of the methods utilized to extract Appearance features."],"2147155775":["In this regard, facial mimicry is one of the major form of affective human robot interaction [10]."],"2150341604":["Due to the dramatic increase in the processing capability of computers, many studies have recently begun to transform to utilize deep neural networks [17]."],"2165731615":["Gabor filters [13], Local Binary Patterns [14], Histogram of Oriented Gradient [15], and Local Phase Quantization [16] are some of the methods utilized to extract Appearance features."],"2183341477":["The prevailing trend in designing CNN architecture has been to make deeper in an attempt to increase the classification performance [30-32]."],"2233116163":["Nowadays, developed the small neural networks that are being developed have the capability of automatically extracting high-level features while simultaneously carrying out the recognition process in a timely manner [23-26]."],"2327030709":["Recently the impacts of employing social robots as teaching assistants on enhancing learners’ achievements notably in children with special needs were investigated in [2-4]."],"2604049780":["Moreover, we were able to successfully implement the facial expression imitation system on the RASA robot in real time [34-35]."],"2772507957":["Recently the impacts of employing social robots as teaching assistants on enhancing learners’ achievements notably in children with special needs were investigated in [2-4]."],"2802746421":["Our robotic platform, “RASA”, is a social assistive robot designed with the aim of teaching Persian sign language to deaf children [3].","RASA features an upper-body kinematic structure consisting of 32 degrees of freedom which enables it to effectively perform PSL [3].","RASA is a social robotic platform employed to facilitate teaching Persian Sign Language (PSL) to deaf and hard of hearing Iranian children [3].","Recently the impacts of employing social robots as teaching assistants on enhancing learners’ achievements notably in children with special needs were investigated in [2-4]."],"2884792551":["For instance, the positive effects of utilizing social robots in decreasing the distress level of children with cancer through establishing affective connection with patients was highlighted in [5, 6]."],"2964350391":["The prevailing trend in designing CNN architecture has been to make deeper in an attempt to increase the classification performance [30-32]."],"3097096317":["In this study, we utilized the well-designed face detector Viola and Jones’ method [29]."]},"abstract":"Mimicry during face-to-face interpersonal interactions is a meaningful nonverbal communication signal that affects the quality of the communications and increases empathy towards the interaction partner. In this paper we propose a facial expression imitation system that utilizes a convolutional neural network (CNN). The model was trained by means of the CK+ database., which is a popular benchmark in facial expression recognition. Then, we implemented the proposed system on a robotic platform and investigated the method's performance via 20 recruited participants. We observed a high mean score of the participants, viewpoints on the imitation capability of the robot of 4.1 out of 5."},{"id":3138049487,"microsoftAcademicId":3138049487,"numberInSourceReferences":104,"doi":"10.3390/APP11062502","title":"Vocal Synchrony of Robots Boosts Positive Affective Empathy","authors":[{"LN":"Nishimura","FN":"Shogo"},{"LN":"Nakamura","FN":"Takuya"},{"LN":"Sato","FN":"Wataru"},{"LN":"Kanbara","FN":"Masayuki"},{"LN":"Fujimoto","FN":"Yuichiro"},{"LN":"Kato","FN":"Hirokazu"},{"LN":"Hagita","FN":"Norihiro"}],"year":2021,"journal":"Applied Sciences","references":[2087484885,2098676269,2111040806,2150422125,2003118556,2122934497,2136179834,2168676811,2094856020,2115725315,2916945592,2047170290,2054560711,2432698062,2139717362,2318449776,2277597206,1990266238,2136132767,2058169200,1995814612,2086106924,1995155855,2804479262,2742970006,1967992372,2168839413,1965255698,1973535569,2076725044,2156342111,2077354323,2105500213,2150125870,2011286429,2055362754,2026760083,3114710213,1993627390,2317581101,2052186042,2328154863],"citationsCount":0},{"id":3026658219,"microsoftAcademicId":3026658219,"numberInSourceReferences":101,"doi":"10.3390/ROBOTICS9020039","title":"The Role of Personality Factors and Empathy in the Acceptance and Performance of a Social Robot for Psychometric Evaluations","authors":[{"LN":"Rossi","FN":"Silvia"},{"LN":"Conti","FN":"Daniela"},{"LN":"Garramone","FN":"Federica"},{"LN":"Santangelo","FN":"Gabriella"},{"LN":"Staffa","FN":"Mariacarla"},{"LN":"Varrasi","FN":"Simone"},{"LN":"Nuovo","FN":"Alessandro G. Di"}],"year":2020,"journal":"Robotics","references":[2100379340,2165758561,1578907434,1992745256,2010681077,2801702920,1968681351,1974108202,3122947385,2137577228,2128561111,2070605229,2097404405,2084641214,2417897973,2144093039,1575107006,2079836081,2011525365,2095899461,2023908618,2068000469,2605153561,2060200729,1979760954,2015349939,1991797265,2167727613,2136548003,2149037883,2785802062,2621905780,2119553435,2155433594,3016178751,2900261942,2811507747,2940246607,2791380498,2155015876,2002398380,2809937793,2809836222,105198038,193733993,2085117869,2953443132,2079520098,2593752601,2043616512,2026698181,2347158526,2899922431,2935091103,2789546142,2794215051,2972300972,1972933873,2085366382,2972516722,2790176108,3011219267,2791231726,3014251576,3125327051,2288770040],"citationsCount":4,"abstract":"Research and development in socially assistive robotics have produced several novel applications in the care of senior people. However, some are still unexplored such as their use as psychometric tools allowing for a quick and dependable evaluation of human users’ intellectual capacity. To fully exploit the application of a social robot as a psychometric tool, it is necessary to account for the users’ factors that might influence the interaction with a robot and the evaluation of user cognitive performance. To this end, we invited senior participants to use a prototype of a robot-led cognitive test and analyzed the influence of personality traits and user’s empathy on the cognitive performance and technology acceptance. Results show a positive influence of a personality trait, the “openness to experience”, on the human-robot interaction, and that other factors, such as anxiety, trust, and intention to use, are influencing technology acceptance and correlate the evaluation by psychometric tests."},{"id":3127104011,"microsoftAcademicId":3127104011,"numberInSourceReferences":66,"doi":"10.3389/FROBT.2020.591448","title":"Development and Testing of Psychological Conflict Resolution Strategies for Assertive Robots to Resolve Human-Robot Goal Conflict.","authors":[{"LN":"Babel","FN":"Franziska","affil":"University of Ulm"},{"LN":"Kraus","FN":"Johannes Maria","affil":"University of Ulm"},{"LN":"Baumann","FN":"Martin","affil":"University of Ulm"}],"year":2021,"journal":"Frontiers in Robotics and AI","references":[2032568497,2096563449,2167557160,2104999980,2044140042,2122570643,2101049671,2129491278,2121533105,1985926185,2097404405,2010060754,2096454110,2003350835,3121604639,2035928776,1991607477,2405357702,2121226869,2771070918,1977021594,2078680671,1973679562,2321768316,1977791518,2080184306,2150768354,2139410216,2953416276,2022293409,191146409,2549023150,2053486845,1971119872,2091899122,2074280884,2159643278,3126093562,2894609524,2803227857,2255049589,2169574110,2751386010,1997839619,2911465809,2784991108,2153000132,1975374640,2284581191,1600099874,2134884815,2745199108,2046324598,2143910019,1997206131,2034383548,1975462622,3000920514,2162380352,2023346536,2591782071,2909775365,2003782760,2055711194,1985122021,2623042518,2151543871,3001508716,2073126171,2549757537,2135630262,2499875684,3014320833,2067801423,3037164970,2093498548,2888191805,2073074679,2119008584,2921009271,2007652652,1966725752,1972532315,2599183884,2028990768,2034737887],"citationsCount":0,"abstract":"As service robots become increasingly autonomous and follow their own task-related goals, human-robot conflicts seem inevitable, especially in shared spaces. Goal conflicts can arise from simple trajectory planning to complex task prioritization. For successful human-robot goal-conflict resolution, humans and robots need to negotiate their goals and priorities. For this, the robot might be equipped with effective conflict resolution strategies to be assertive and effective but similarly accepted by the user. In this paper, conflict resolution strategies for service robots (public cleaning robot, home assistant robot) are developed by transferring psychological concepts (e.g., negotiation, cooperation) to HRI. Altogether, fifteen strategies were grouped by the expected affective outcome (positive, neutral, negative). In two online experiments, the acceptability of and compliance with these conflict resolution strategies were tested with humanoid and mechanic robots in two application contexts (public: n 1 = 61; private: n 2 = 93). To obtain a comparative value, the strategies were also applied by a human. As additional outcomes trust, fear, arousal, and valence, as well as perceived politeness of the agent were assessed. The positive/neutral strategies were found to be more acceptable and effective than negative strategies. Some negative strategies (i.e., threat, command) even led to reactance and fear. Some strategies were only positively evaluated and effective for certain agents (human or robot) or only acceptable in one of the two application contexts (i.e., approach, empathy). Influences on strategy acceptance and compliance in the public context could be found: acceptance was predicted by politeness and trust. Compliance was predicted by interpersonal power. Taken together, psychological conflict resolution strategies can be applied in HRI to enhance robot task effectiveness. If applied robot-specifically and context-sensitively they are accepted by the user. The contribution of this paper is twofold: conflict resolution strategies based on Human Factors and Social Psychology are introduced and empirically evaluated in two online studies for two application contexts. Influencing factors and requirements for the acceptance and effectiveness of robot assertiveness are discussed."},{"id":3005186062,"microsoftAcademicId":3005186062,"numberInSourceReferences":157,"doi":"10.1038/S41598-020-57711-6","title":"Empathic responses to unknown others are modulated by shared behavioural traits.","authors":[{"LN":"Anders","FN":"Silke","affil":"University of Lübeck"},{"LN":"Beck","FN":"Christian","affil":"University of Lübeck"},{"LN":"Domin","FN":"Martin","affil":"Greifswald University Hospital"},{"LN":"Lotze","FN":"Martin","affil":"Greifswald University Hospital"}],"year":2020,"journal":"Scientific Reports","references":[2025175823,2017108196,2052610531,2128472904,2136219431,2141796362,2923742357,2068715462,2153480757,1987599891,2150422125,2103551585,2017341863,1973030362,2121004914,2147594037,2132578438,1972224334,2019725315,2063792498,1969585040,2157566701,2042637398,2606561085,2099523660,2033078712,1964681711,2097389486,2025102384,2064205813,2154447920,2788469783,2169691351,2335408833,2734854022,2094694189,2011275943,1539842927,2070911318,2033134707,2101345151,2021608983,2154313865],"citationsCount":1,"abstract":"How empathically people respond to a stranger’s pain or pleasure does not only depend on the situational context, individual traits and intentions, but also on interindividual factors. Here we ask whether empathic responses towards unknown others are modulated by behavioural similarity as a potential marker of genetic relatedness. Participants watched two supposed human players who were modelled as having a strong (player LP) or weak (player NLP) tendency to lead in social situations executing penalty shots in a virtual reality robot soccer game. As predicted, empathic response were modulated by shared behavioural traits: participants whose tendency to lead was more similar to player LP’s tendency to lead experienced more reward, and showed stronger neural activity in reward-related brain regions, when they saw player LP score a goal, and participants whose tendency to lead was more similar to player NLP’s tendency to lead showed stronger empathic responses when they saw player NLP score a goal. These findings highlight the potentially evolutionary grounded role of phenotypic similarity for neural processes underlying human social perception."},{"id":2506698290,"microsoftAcademicId":2506698290,"numberInSourceReferences":145,"doi":"10.1142/9789813149137_0076","title":"DESIGNING A ROBOTIC INTERFACE FOR CHILDREN: THE MONARCH ROBOT EXAMPLE","authors":[{"LN":"Ferreira","FN":"Maria Isabel Aldinhas"},{"LN":"Sequeira","FN":"João Silva"}],"year":2016,"references":[2156256035],"citationsCount":3},{"id":2294294917,"microsoftAcademicId":2294294917,"numberInSourceReferences":132,"doi":"10.1007/978-3-319-16841-8_23","title":"The Affective Loop: A Tool for Autonomous and Adaptive Emotional Human-Robot Interaction","authors":[{"LN":"Vircikova","FN":"Maria","affil":"Technical University of Košice"},{"LN":"Magyar","FN":"Gergely","affil":"Technical University of Košice"},{"LN":"Sincak","FN":"Peter","affil":"Technical University of Košice"}],"year":2015,"journal":"Revista De Informática Teórica E Aplicada","references":[1975000068,2152521250,1514681518,2074165865,2080442667,2033640825,2033995044,2110102748,2187895870,1654420439,2143889467,2154655824,2046511269,2128452144],"citationsCount":2,"abstract":"The paper presents an affective model for social robotics, where the robot is capable of behavior adaptation, in accordance with the needs and preferences of a particular user. The proposed approach differs from other studies in human-robot interaction as these usually have been using the ‘Wizard of Oz’ technique, where a person remotely operates a robot. On the other side, simulated robots are not able of personalized behaviors and behave according to the preprogrammed set of rules. We provide a tool to personalize affective artificial behaviors in cooperative human—robot scenarios, where human emotion recognition, appropriate robotic behavior selection and expression of robotic emotions play a key role. The preliminary experiments show that the personalized affective robotic behavior can achieve better results in a scenario in which a robot motivates children in learning. We believe that human—robot interfaces which mimic how humans interact with one another in an empathic way could ultimately lead to robots being accepted in the wider domain."},{"id":2991463001,"microsoftAcademicId":2991463001,"numberInSourceReferences":62,"doi":"10.3390/PROCEEDINGS2019031071","title":"Affective Embodied Agents and Their Effect on Decision Making.","authors":[{"LN":"Acosta-Mitjans","FN":"Adrian"},{"LN":"Cruz-Sandoval","FN":"Dagoberto"},{"LN":"Hervás","FN":"Ramón"},{"LN":"Johnson","FN":"Esperanza"},{"LN":"Nugent","FN":"Chris D."},{"LN":"Favela","FN":"Jesús"}],"year":2019,"journal":"UCAmI","references":[],"citationsCount":1,"abstract":"Embodied agents, such as avatars and social robots, are increasingly incorporating a capacity to enact affective states and recognize the mood of their interlocutor. This influences how users perceive these technologies and how they interact with them. We report on an experiment aimed at assessing perceived empathy and fairness among individuals interacting with avatars and robots when compared to playing against a computer or a fellow human being. Twenty-one individuals were asked to play the ultimatum game, playing the role of a responder against another person, a computer, an avatar and a robot for a total of 32 games (8 per condition). We hypothesize that affective expressions by avatars and robots influence the emotional state of the users, leading them to irrational behavior by rejecting unfair proposals. We monitored galvanic skin response and heart rate of the players in the period when the offer was made by the proposer until the decision was announced by the responder. Our results show that most fair offers were accepted while most unfair offers were rejected. However, participants rejected more very unfair offers made by people and computers than by the avatars or robots."},{"id":2794314064,"microsoftAcademicId":2794314064,"numberInSourceReferences":164,"doi":"10.1016/J.CHB.2018.03.012","title":"An examination of gender differences versus similarities in a virtual world","authors":[{"LN":"Martens","FN":"Amanda L.","affil":"Kansas State University"},{"LN":"Grover","FN":"Cathy A.","affil":"Emporia State University"},{"LN":"Saucier","FN":"Donald A.","affil":"Kansas State University"},{"LN":"Morrison","FN":"Breanna A.","affil":"Emporia State University"}],"year":2018,"journal":"Computers in Human Behavior","references":[2161923363,2027450919,2149249197,2160144355,2141154555,2096101299,2151186028,2790019262,1539835075,2029315650,1497685398,1991635220,2154106762,2036416596,2022224322,2061146610,128780456,2118785178,2038950866,1989330390,2087669878,1978644275,3124069656,2741457160,2110219996,2193672388,2053950676,1985584528,2003192254,2512330729,1966425432,1973236963,2181751309],"citationsCount":1,"abstract":"Abstract  We derived competing hypotheses from the gender similarities perspective versus the gender differences perspective to examine participants' behavior in an online virtual world in which we manipulated participants' gender. To manipulate participants' gender in the virtual environment, we randomly assigned them to one of three avatars (female, male, or robot). Using a screen recording device, we measured the percentage of time participants spent interacting with empathizing (e.g., options for shopping, telephone) and systemizing (e.g., weapons, options for building) objects in a virtual reality house that we constructed to reflect evidence put forth by the differences perspective. Because we derived competing hypotheses we expected to find support for either the similarities perspective or the differences perspective; however, our results suggested support for both. Consistent with the differences perspective hypotheses, participants paid attention to objects in the environment that were consistent with the social representation of their own gender. However, our results were consistent with the similarities perspective hypotheses, such that the avatars’ gender also played a role in the percentage of time participants spent interacting with empathizing and systemizing objects. Therefore, we conclude that observable differences between men and women are the consequence of both biological and social forces, and research should focus on the interaction between the two as etiologies and explanations for sex and gender differences and similarities."},{"id":2888739605,"microsoftAcademicId":2888739605,"numberInSourceReferences":54,"doi":"10.1109/URAI.2018.8441767","title":"Incremental Learning of Human Emotional Behavior for Social Robot Emotional Body Expression","authors":[{"LN":"Tuyen","FN":"Nguyen Tan Viet","affil":"Japan Advanced Institute of Science and Technology"},{"LN":"Jeong","FN":"Sungmoon","affil":"Japan Advanced Institute of Science and Technology"},{"LN":"Chong","FN":"Nak Young","affil":"Japan Advanced Institute of Science and Technology"}],"year":2018,"journal":"2018 15th International Conference on Ubiquitous Robots (UR)","references":[1990517717,2110802877,2098676269,1995113806,203345490,2010150441,2138754805,2024221294,2130533816,2290727245,2112796159,2131123711,2165287812,2114138219,1977948674,2099198275,2036372164,141605101,2023397245,1529648884,2075437821,2247169076,23802235,2772644739,2014706514,1740158594,1996215131,2742847280,2083698397],"citationsCount":1,"citationContext":{"23802235":["It belongs to class of Topology Representing Networks which build perfectly topology preserving feature maps [21]."],"141605101":["by Andra [8] imitates human facial expressions with the main goal to improve the emotion recognition capabilities of autistic children."],"203345490":["The Covariance Descriptor method [17] is used to Fig."],"1529648884":["At the clustering phase, classifying trained neurons into different groups is conducted with Distance matrix based approach [26]."],"1740158594":["[12] where a 9 month old infant sees that his father plays with a novel toy."],"1977948674":["In recent years, many studies focused on generating emotional expressions by estimating and incorporating the emotional states of robot, which is believed to increase the engagement and empathy between humans and robots [2]."],"1990517717":["1) Self Organizing Map: In our previous paper [18], Self Organizing Map (SOM) [19] was utilized as the batch learning approach for the training phase in behavior selection as shown in Figure 2.","DCS inheres Kohonen type learning rule [19] for updating weight of neural vectors as SOM while using Hebbian learning rule [22] to dynamically update lateral connection","DCS then updates the weight of neuron vectors by Kohonen learning rule [19] which makes them move closer to the"],"1995113806":["Microsoft Research Cambridge-12 Kinect gesture dataset (MSRC-12) [28] was utilized.","On the other hand, actions class 11 were named ”Lay down the tempo of a song” presenting by the action of beating the air with both of arms [28].","up the music” [28]."],"2010150441":["Another approaches of growing neural network by dynamic allocation the feature map in order to evolve its structure are known as Growing Cell Structure (GCS) [23], Growing Gas Model or Growing Neural Gas [24]."],"2014706514":["In [15], the authors made comparisons between different unsupervised learning algorithms such as Self Organizing Maps (SOM), Fuzzy C Means (FCM) and K Means for recognizing human postures in video sequences."],"2023397245":["This study was mainly motivated by the work of Meijer [7] and other psychological researchers about the contribution of body movements to the attribution of emotions."],"2024221294":["Pepper robot’s bodily expressions were created in [3] based on the perspectives of social psychology about the connection between emotion and bodily movement [4] [5]."],"2036372164":["Cell Structure (DCS) [20] is an appropriate approach where topological properties could be preserved in a similar way to SOM.","between neural units are not initially defined, instead, they are dynamically learned during training phase [20] by Herbian learning rule."],"2075437821":["on-line learning and estimation of system parameters [25]."],"2083698397":["affecting to social outcome [1]."],"2098676269":["This assumption has been strongly supported by straightforward relation between individual cultural traits and robot behaviors which has been found by HRI researcher [9] or psychological evidence about mimicry of posture, facial expression, verbal and non-verbal behaviors of interacting partners [10]."],"2099198275":["This idea has been shared across many previous researches, Mohammad [14]"],"2110802877":["descriptors directly, significant gains in speed of clustering can be obtained [27]."],"2112796159":["DCS inheres Kohonen type learning rule [19] for updating weight of neural vectors as SOM while using Hebbian learning rule [22] to dynamically update lateral connection","Indeed, DCS dynamically modifies the lateral connections by Herbian learning rule [22] and adding new unit on the grid of neurons if the quantization error is still higher than stopping condition.","The neighboring neurons of mbmu, represented by Nbmu, are updated lateral connections by Herbian learning rule [22]."],"2114138219":["The capability of trajectory learning from human demonstrations for the robot arm was proposed by [16], where the trajectory clustering and approximation modules take human demonstrative trajectories as the input that belong to different clusters."],"2130533816":["and (2) utilizing human habitual behaviors which could be measured by assessing the frequency of past behaviors [13] as the reference for generating the robot’s emotional bodily expression."],"2131123711":["Pepper robot’s bodily expressions were created in [3] based on the perspectives of social psychology about the connection between emotion and bodily movement [4] [5]."],"2138754805":["Another approaches of growing neural network by dynamic allocation the feature map in order to evolve its structure are known as Growing Cell Structure (GCS) [23], Growing Gas Model or Growing Neural Gas [24]."],"2165287812":["proposed by Markus [6]."],"2247169076":["Because MSRC-12 dataset, which was gathered by Kinect sensor, was much more noisy than HDM05-MoCap dataset [29] obtained from optical marker sensors [30], as the result,"],"2290727245":["Because MSRC-12 dataset, which was gathered by Kinect sensor, was much more noisy than HDM05-MoCap dataset [29] obtained from optical marker sensors [30], as the result,"],"2742847280":["1) Self Organizing Map: In our previous paper [18], Self Organizing Map (SOM) [19] was utilized as the batch learning approach for the training phase in behavior selection as shown in Figure 2.","To validate performance of SOM and DCS training phase in behavior selection model, experiment setup was conducted in the similar way to our previous paper [18] where the public"],"2772644739":["In [3], the authors investigated the role of culture in representing robot emotions, where bodily","Pepper robot’s bodily expressions were created in [3] based on the perspectives of social psychology about the connection between emotion and bodily movement [4] [5]."]},"abstract":"Generating emotional body expressions for social robots has been gaining increased attention to enhance the engagement and empathy in human-robot interaction. In this paper, an enhanced model of robot emotional body expression is proposed which places emphasis on the individual user's cultural traits. Similar to our previous paper, this approach is inspired by social and emotional development of infants interacting with their parents who have a certain cultural background. Social referencing occurs when infants perceive their parents' facial expressions and vocal tones of emotional situations to form their own interpretation. On the other hand, this model replaces the batch learning self-organizing map with the dynamic cell structure, incrementally training a neural network model with a variety of emotional behaviors obtained from the users with whom the robot interacts. We demonstrate the validity of our incremental learning model through a public human action dataset, which will facilitate the acquisition of emotional body expression of socially assistive robots as a reflection of the individual user's culture."},{"id":2073972564,"microsoftAcademicId":2073972564,"numberInSourceReferences":17,"doi":"10.1145/2824893.2824897","title":"Evaluation of the emotional answer in HRI on a game situation","authors":[{"LN":"Franco","FN":"Gloria Adriana Mendoza","affil":"National Autonomous University of Mexico"}],"year":2015,"journal":"Proceedings of the Latin American Conference on Human Computer Interaction","references":[1595732857,1970356902,1986558703,2101665212,1966051761,2153457831,2000179001,2069106458,2160677154,2097634056,1498976739,2087761074,2124721658,2025365003,2064814624,33139668],"citationsCount":0,"abstract":"This project has as purpose to propose an adequate method for the assessment of the emotional answer after an interaction with a social and emotional robot. A lottery game application has been developed for playing with the robot Nao, and through an experimental scenario the empathy towards a robot has been demonstrated. As a result, the Emocards are presented as a promising assessment method for the emotional answer of the users."},{"id":3042250753,"microsoftAcademicId":3042250753,"numberInSourceReferences":97,"doi":"10.1111/NUP.12318","title":"Intelligent humanoid robots expressing artificial humanlike empathy in nursing situations","authors":[{"LN":"Pepito","FN":"Joseph Andrew","affil":"College of Allied Medical Sciences, Cebu Doctors' University, Cebu City, Philippines."},{"LN":"Ito","FN":"Hirokazu","affil":"University of Tokushima"},{"LN":"Betriana","FN":"Feni","affil":"University of Tokushima"},{"LN":"Tanioka","FN":"Tetsuya","affil":"University of Tokushima"},{"LN":"Locsin","FN":"Rozzano C.","affil":"Florida Atlantic University"},{"LN":"Locsin","FN":"Rozzano C.","affil":"University of Tokushima"}],"year":2020,"journal":"Nursing Philosophy","references":[2871950322,2612425534,2160376779,2273672928,2801271640,2895614216,2801150160,2786455401,2754425142,2338561746,2085083175,2773547303,2791361798,1481824989,2282380228,2911671614,2783599545,2967916686,2885373046,2999860852,2565643380,2410431690,131467608,2617535286,2096210009,2624526442,2018296297,2792400400,2791452829,2793799987,2902130859,2113213503,2462030343,2472811553,2885855554,2888147269,2766869936,3007668157,2945414289,2099028910,3007022309,2996950626,2963955731,2998706248,2077551340],"citationsCount":2,"abstract":"Intelligent humanoid robots (IHRs) are becoming likely to be integrated into nursing practice. However, a proper integration of IHRs requires a detailed description and explanation of their essential capabilities, particularly regarding their competencies in replicating and portraying emotive functions such as empathy. Existing humanoid robots can exhibit rudimentary forms of empathy; as these machines slowly become commonplace in healthcare settings, they will be expected to express empathy as a natural function, rather than merely to portray artificial empathy as a replication of human empathy. This article works with a twofold purpose: firstly, to consider the impact of artificial empathy in nursing and, secondly, to describe the influence of Affective Developmental Robotics (ADR) in anticipation of the empathic behaviour presented by artificial humanoid robots. The ADR has demonstrated that it can be one means by which humanoid nurse robots can achieve expressions of more relatable artificial empathy. This will be one of the vital models for intelligent humanoid robots currently in nurse robot development for the healthcare industry. A discussion of IHRs demonstrating artificial empathy is critical to nursing practice today, particularly in healthcare settings dense with technology."},{"id":3110311020,"microsoftAcademicId":3110311020,"numberInSourceReferences":24,"doi":"10.1007/978-3-030-63486-5_26","title":"Expression of Grounded Affect: How Much Emotion Can Arousal Convey?","authors":[{"LN":"Hickton","FN":"Luke","affil":"University of Hertfordshire"},{"LN":"Lewis","FN":"Matthew","affil":"University of Hertfordshire"},{"LN":"Cañamero","FN":"Lola","affil":"University of Hertfordshire"}],"year":2020,"journal":"Annual Conference Towards Autonomous Robotic Systems","references":[2102998034,2032568497,2018528947,2131123711,1977841100,2088958698,2315633916,2151768748,2057752206,2592036800,2141608182,2595857079,2155631548,2047029964,1584672188,2127023913,1512584022,2032455230,3021246446,1984867139,1968340857,2009375902],"citationsCount":0,"abstract":"In this paper we consider how non-humanoid robots can communicate their affective state via bodily forms of communication (kinesics), and the extent to which this influences how humans respond to them. We propose a simple model of grounded affect and kinesic expression before presenting the qualitative findings of an exploratory study (N = 9), during which participants were interviewed after watching expressive and non-expressive hexapod robots perform different ‘scenes’. A summary of these interviews is presented and a number of emerging themes are identified and discussed. Whilst our findings suggest that the expressive robot did not evoke significantly greater empathy or altruistic intent in humans than the control robot, the expressive robot stimulated greater desire for interaction and was also more likely to be attributed with emotion."},{"id":2135423632,"microsoftAcademicId":2135423632,"numberInSourceReferences":138,"doi":"10.1145/2702613.2726962","title":"Cyrafour: How Two Human Avatars Communicate With Each Other","authors":[{"LN":"Encinas","FN":"Enrique","affil":"University of Southern Denmark"}],"year":2015,"journal":"Proceedings of the 33rd Annual ACM Conference Extended Abstracts on Human Factors in Computing Systems","references":[1984218255,1998757724,2122069758,2182720885,2083590466,2021664265,2078922236,1987650481],"citationsCount":2,"abstract":"Human avatars or physical surrogates are becoming increasingly present in leisure, artistic and business activities that seek to augment the sensory richness available to telepresent participants. While a number of studies have focused on how human avatars relate to other humans, little attention has been paid to the particularities of human avatar to human avatar interaction. This paper examines characteristic features of such interaction through Cyrafour, a playful embodied identity game in which two human avatars clone various conversations generated elsewhere. Such cloning, or speech shadowing, seems to allow for an empathic embodiment of the meaning transmitted and appears to create a frame for further discussion on the topics raised. This project contributes to the study of telepresence with new insights applicable to the design and research of human computer and human robot interfaces."},{"id":2567601827,"microsoftAcademicId":2567601827,"numberInSourceReferences":128,"doi":"10.1109/IHMSC.2016.103","title":"Human-Robot Interactoin Design for Robot-Assisted Intervention for Children with Autism Based on E-S Theory","authors":[{"LN":"Li","FN":"Chaochao","affil":"Beijing University of Posts and Telecommunications"},{"LN":"Jia","FN":"Qingxuan","affil":"Beijing University of Posts and Telecommunications"},{"LN":"Feng","FN":"Yongli","affil":"Beijing University of Posts and Telecommunications"}],"year":2016,"journal":"2016 8th International Conference on Intelligent Human-Machine Systems and Cybernetics (IHMSC)","references":[2014824949,2131018314,2124848740,2028758511,2166103206,1976497813,2599395273,2947814520,2142866933,2885616197,2031328307],"citationsCount":4,"citationContext":{"2124848740":["In the European AURORA project launched in 1998, research group at the University of Hertfordshire utilized simple mobile robots like KASPAR as social mediators to assist children with autism in social interactions such as imitation and take turning [6]."],"2142866933":["The IROMEC project subsidized by European Union (EU) in 2006, developed a modular robot IROMEC meeting individual requirements of children with autism by adjusting functional modules [7]."]},"abstract":"The paper presents a novel human-robot interaction (HRI) framework to assist intervention for children with autism, based on Empathizing-Systemizing (E-S) theory. E-S theory explains the social difficulties in autism as the result of deficits or delays in empathizing, while explaining nonsocial behavior patterns as the effect of intact or even superior skills in systemizing. In this paper, the strength of systemizing is utilized to make up the deficiency and facilitate the development in empathizing via robot-assisted intervention, which has been identified as one of the most popular methods that are producing inspiring outcomes in the rehabilitation of children with autism. The design of HRI scenarios and tasks based on E-S theory makes the robot-assisted intervention more effective and efficient."},{"id":2546684688,"microsoftAcademicId":2546684688,"numberInSourceReferences":127,"doi":"10.1007/978-3-319-48746-5_54","title":"Analyzing Human-Avatar Interaction with Neurotypical and not Neurotypical Users","authors":[{"LN":"Johnson","FN":"Esperanza","affil":"University of Castilla–La Mancha"},{"LN":"Franca","FN":"Carlos Gutiérrez López de la","affil":"University of Castilla–La Mancha"},{"LN":"Hervás","FN":"Ramón","affil":"University of Castilla–La Mancha"},{"LN":"Mondéjar","FN":"Tania","affil":"University of Castilla–La Mancha"},{"LN":"Bravo","FN":"José","affil":"University of Castilla–La Mancha"}],"year":2016,"journal":"International Conference on Ubiquitous Computing and Ambient Intelligence","references":[2121790431,2163475948,2123705914,2511396044,2006440172,2525979364,1530689307,2408731186,2071618145,1862132244,1818428327,1507624899,2099536327,2395218824,2464425840,2270795985],"citationsCount":1,"abstract":"Assistive technologies have been used to improve the quality of life of people who have been diagnosed with health issues. In this case, we aim to use an assistive technology in the shape of an affective avatar to help people who have been diagnosed with different forms of Social Communications Disorders (SCD). The designed avatar presents a humanoid face that displays emotions with a subtlety akin to that of real life human emotions, with those emotions changing according to the interactions that the user chooses to perform on the avatar. We have used Blender for the design of the emotions, which are happiness, sadness, surprise, fear and anger, plus a neutral emotion, while Unity was used to dictate the behavior of the avatar when the interactions were performed, which could be positive (caress), negative (poke) or neutral (wait). The avatar has been evaluated by 48 people from different backgrounds and the results show the overall positive reception by the users, as well as the difference between neurotypical and non-neurotypical users in terms of emotion recognition and chosen interactions. A ground truth has been established in terms of prototypic empathic interactions by the users."},{"id":3011131157,"microsoftAcademicId":3011131157,"numberInSourceReferences":94,"doi":"10.1109/MTS.2020.2967493","title":"Robot Enhanced Therapy for Autistic Children: An Ethical Analysis","authors":[{"LN":"McBride","FN":"Neil","affil":"De Montfort University"}],"year":2020,"journal":"IEEE Technology and Society Magazine","references":[181665514,2141973273,2290350210,2167266355,24571089,2133241908,2119131407,2526584274],"citationsCount":2,"citationContext":{"24571089":["There is usually some biological imitation, whether dogs, ants, or spiders [24]."],"181665514":["If we look towards the singularity [11], when artificial intelligence becomes superior to humans and supersedes humans, we will be likely to raise the robot above the human."],"2119131407":["The robot ethics of the DREAM project should have been the ethics of the client [14], which in this case is the autistic child."],"2133241908":["The question here is why develop social robots at all? Second I apply the ACTIVE ethics framework [15] to establish an ethics for social robots that positions them not as replacements for human interaction, but facilitators of human relational interactions in a community context.","To illustrate such a shift, I will apply the ACTIVE ethics framework [15] to ASD robot-assisted therapy and hence identify the ethical positioning of robots in autism therapy."],"2141973273":["This may be achieved through the use of a social robot as a mediator for play [5]."],"2167266355":["However, in order to deploy robots in social care it will be necessary to reduce the variety of human behavior to match that of the robot (See [1])."],"2526584274":["57 M A R C H 2 0 2 0 ∕ IEEE TECHNOLOGY AND SOCIETY MAGAZINE between the robot and the human [16].","That balance must take into account the behavior and skills of each party, and it is mediated by the variation and intensity of communication between the two parties [16]."]},"abstract":"The use of social robots has been proposed for the delivery of therapy to autistic children. The aim of such projects, of which the DREAM project is an example, is to replace therapists by robots, operating in sensory environments that enable them to detect and respond to feedback from the child. This article considers the ethical concerns of autonomy, community, transparency, identity, value, and empathy to evaluate the ethics of such deployment of robots. In doing so it provides a response to the Richardson et al. article in IEEE Technology and Society Magazine , Mar. 2018 [20]. This article concludes that deployment of robots to control the behavior of autistic children is ethically suspect and should be questioned. The use of robots with children should be evaluated on the basis of the purpose of and process by which such robots are deployed, rather than on the basis of the technology itself. Particularly important is the roboticist's empathy with the user of the robot, and gaining an understanding of the individual child. The paper suggests how an understanding of the autistic child might lead to sensitive deployment of a robot to help the child manage social environments through supporting the child's regulation of emotions."},{"id":3087297405,"microsoftAcademicId":3087297405,"numberInSourceReferences":153,"doi":"10.3390/ACT9030091","title":"An open-source social robot based on CompliAnt SofT Robotics for therapy with children with ASD","authors":[{"LN":"Casas-Bocanegra","FN":"Diego"},{"LN":"Gomez-Vargas","FN":"Daniel"},{"LN":"Pinto-Bernal","FN":"Maria J."},{"LN":"Maldonado","FN":"Juan"},{"LN":"Munera","FN":"Marcela"},{"LN":"Villa-Moreno","FN":"Adriana"},{"LN":"Stoelen","FN":"Martin F."},{"LN":"Belpaeme","FN":"Tony"},{"LN":"Cifuentes","FN":"Carlos A."}],"year":2020,"journal":"Actuators","references":[2614349964,2111253117,2107911338,2887260263,2163455560,2031531183,2617211984,2001381166,1769013118,2576246534,1976497813,2807975560,2164490004,2115673874,2050365646,1985505929,2088831794,2569374490,2938404524,2144452746,2107340640,1974454998,2274703805,1515128239,2160449350,2773076209,1542153887,2080574976,2049194359,1761550677,2551513864,2611830581,2805316516,2786720485,2792695098,2076319497,2934061162,2145027679,2922133732,2279878284,2730783125,2953482672,2774521151,2942818524,2799540951,3004574186,2844935800,1637244709,1968149153,3009669436,3040568028,2560458652,2897693113],"citationsCount":0,"abstract":"Therapy with robotic tools is a promising way to help improve verbal and nonverbal communication in children. The robotic tools are able to increase aspects such as eye contact and the ability to follow instructions and to empathize with others. This work presents the design methodology, development, and experimental validation of a novel social robot based on CompliAnt SofT Robotics called the CASTOR robot, which intends to be used as an open-source platform for the long-term therapy of children with autism spectrum disorder (CwASD). CASTOR integrates the concepts of soft actuators and compliant mechanisms to create a replicable robotic platform aimed at real therapy scenarios involving physical interaction between the children and the robot. The validation shows promising results in terms of robustness and the safety of the user and robot. Likewise, mechanical tests assess the robot’s response to blocking conditions for two critical modules (i.e., neck and arm) in interaction scenarios. Future works should focus on the validation of the robot’s effectiveness in the therapy of CwASD."},{"id":3093876711,"microsoftAcademicId":3093876711,"numberInSourceReferences":95,"doi":"10.1145/3415218","title":"Intersectional AI: A Study of How Information Science Students Think about Ethics and Their Impact","authors":[{"LN":"McDonald","FN":"Nora","affil":"University of Maryland, Baltimore County"},{"LN":"Pan","FN":"Shimei","affil":"University of Maryland, Baltimore County"}],"year":2020,"journal":"Proceedings of the ACM on Human-Computer Interaction","references":[2898970033,2530395818,2337002970,2171161819,2809878087,2767269108,2964583491,2336816949,2970894203,2059219846,2809046742,2275337986,2885301047],"citationsCount":1,"abstract":"Recent literature has demonstrated the limited and, in some instances, waning role of ethical training in computing classes in the US. The capacity for artificial intelligence (AI) to be inequitable or harmful is well documented, yet it's an issue that continues to lack apparent urgency or effective mitigation. The question we raise in this paper is how to prepare future generations to recognize and grapple with the ethical concerns of a range of issues plaguing AI, particularly when they are combined with surveillance technologies in ways that have grave implications for social participation and restriction?from risk assessment and bail assignment in criminal justice, to public benefits distribution and access to housing and other critical resources that enable security and success within society. The US is a mecca of information and computer science (IS and CS) learning for Asian students whose experiences as minorities renders them familiar with, and vulnerable to, the societal bias that feeds AI bias. Our goal was to better understand how students who are being educated to design AI systems think about these issues, and in particular, their sensitivity to intersectional considerations that heighten risk for vulnerable groups. In this paper we report on findings from qualitative interviews with 20 graduate students, 11 from an AI class and 9 from a Data Mining class. We find that students are not predisposed to think deeply about the implications of AI design for the privacy and well-being of others unless explicitly encouraged to do so. When they do, their thinking is focused through the lens of personal identity and experience, but their reflections tend to center on bias, an intrinsic feature of design, rather than on fairness, an outcome that requires them to imagine the consequences of AI. While they are, in fact, equipped to think about fairness when prompted by discussion and by design exercises that explicitly invite consideration of intersectionality and structural inequalities, many need help to do this empathy 'work.' Notably, the students who more frequently reflect on intersectional problems related to bias and fairness are also more likely to consider the connection between model attributes and bias, and the interaction with context. Our findings suggest that experience with identity-based vulnerability promotes more analytically complex thinking about AI, lending further support to the argument that identity-related ethics should be integrated into IS and CS curriculums, rather than positioned as a stand-alone course."},{"id":2958813519,"microsoftAcademicId":2958813519,"numberInSourceReferences":119,"doi":"10.1109/FG.2019.8756532","title":"Towards a Multimodal Time-Based Empathy Prediction System","authors":[{"LN":"Barbieri","FN":"Francesco","affil":"Telefónica"},{"LN":"Guizzo","FN":"Eric","affil":"City University London"},{"LN":"Lucchesi","FN":"Federico","affil":"Telefónica"},{"LN":"Maffei","FN":"Giovanni","affil":"Telefónica"},{"LN":"Martin","FN":"Fermin Moscoso del Prado","affil":"Telefónica"},{"LN":"Weyde","FN":"Tillman","affil":"City University London"}],"year":2019,"journal":"2019 14th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2019)","references":[2194775991,2964121744,2064675550,2470673105,2115252128,2023736093,2046869671,2962750587],"citationsCount":1,"citationContext":{"2023736093":["Each word is represented as a vector of 11 dimensions, consisting of the features extracted from two emotional lexicons [10], [11]."],"2046869671":["discard the phase information and compute the power-law compression by exponentiating the spectrum magnitudes the power of 2/3 to approximate human perception [4]."],"2064675550":["An LSTM network [9] is used to predict a valence score after each word."],"2115252128":["The data pre-processing consists of a facial landmark detection, performed frame-by-frame using the dlib library [7]."],"2194775991":["ResNet 16 [8] architecture adapted to 3-dimensional data 4."],"2470673105":["com/transcribe/ implemented with the following attention module as in [12]:"],"2962750587":["Each word is represented as a vector of 11 dimensions, consisting of the features extracted from two emotional lexicons [10], [11]."],"2964121744":["We trained the model with a batch size of 50 samples, using the ADAM optimizer [6] and applying Early Stopping."]},"abstract":"We describe our system for empathic emotion recognition. It is based on deep learning on multiple modalities in a late fusion architecture. We describe the modules of our system and discuss the evaluation results. Our code is also available for the research community1"},{"id":2400584822,"microsoftAcademicId":2400584822,"numberInSourceReferences":23,"doi":"10.1007/978-3-319-22979-9_47","title":"Biophilic Evolutionary Buildings that Restore the Experience of Animality in the City","authors":[{"LN":"Gil","FN":"Pablo","affil":"European University of Madrid"},{"LN":"Rossi","FN":"Claudio","affil":"Technical University of Madrid"},{"LN":"Coral","FN":"William","affil":"Technical University of Madrid"}],"year":2015,"journal":"Living Machines 2015 Proceedings of the 4th International Conference on Biomimetic and Biohybrid Systems - Volume 9222","references":[1580414357,1975793762,2140685838,576200315,2050870241,2105963908,2072267104,2050079439,2890581055,2737299620,562054134,1479719087],"citationsCount":1,"abstract":"In this paper, we present our work on the training of robotised architectural components of intelligent buildings, focusing on how architectural components can learn to behave animalistically, according to the judgment of human users. Our work aims at recovering the lost contact with animals in the urban context, taking advantage of biophilic empathy. The parameters governing the robotised elements we propose are mainly qualitative emotions and aesthetical perception, which cannot easily be described by mathematical parameters. Additionally, due to their complexity, it is often impossible ---or at least impractical, to hardcode suitable controllers for such structures. Thus, we propose the use of Artificial Intelligence learning techniques, concretely Evolutionary Algorithms, to allow the user to teach the robotised components how to behave in response to their resemblance to specific animal behaviors. This idea is tested on an intelligent facade that learns optimal configurations according to the perception of aggressiveness and calmness."},{"id":2425741675,"microsoftAcademicId":2425741675,"numberInSourceReferences":74,"doi":"10.1515/IJDHD-2016-0003","title":"Poetry writing and artistic ability in problem-based learning","authors":[{"LN":"Chan","FN":"Zenobia C.Y."}],"year":2017,"journal":"International Journal on Disability and Human Development","references":[2032321789,2170698282,2128885791,2090109518,1575093732,2074690435,2159756823,1992870674,1968249266,1971453730,2034432405,2035650414,2118003647,2039195603,2059800201,2102986400,2324822647,2095823800,1998478951,2129109184,2076492277,2014042597,2171094822,2405264691,2054566062,139419618,2071281414,2063849146,2062274478],"citationsCount":2},{"id":2944492778,"microsoftAcademicId":2944492778,"numberInSourceReferences":161,"doi":"10.3390/APP9091909","title":"Hybrid spiral stc-hedge algebras model in knowledge reasonings for robot coverage path planning and its applications","authors":[{"LN":"Pham","FN":"Hai Van"},{"LN":"Asadi","FN":"Farzin"},{"LN":"Abut","FN":"Nurettin"},{"LN":"Kandilli","FN":"Ismet"}],"year":2019,"journal":"Applied Sciences","references":[2019738489,1969472392,2126842639,2883180136,2792427381,2408002876,1970499532,2801166180,2062290140,1998416534,2055654006,2903577080,2143648139],"citationsCount":1,"abstract":"Robotics is a highly developed field in industry, and there is a large research effort in terms of humanoid robotics, including the development of multi-functional empathetic robots as human companions. An important function of a robot is to find an optimal coverage path planning, with obstacle avoidance in dynamic environments for cleaning and monitoring robotics. This paper proposes a novel approach to enable robotic path planning. The proposed approach combines robot reasoning with knowledge reasoning techniques, hedge algebra, and the Spiral Spanning Tree Coverage (STC) algorithm, for a cleaning and monitoring robot with optimal decisions. This approach is used to apply knowledge inference and hedge algebra with the Spiral STC algorithm to enable autonomous robot control in the optimal coverage path planning, with minimum obstacle avoidance. The results of experiments show that the proposed approach in the optimal robot path planning avoids tangible and intangible obstacles for the monitoring and cleaning robot. Experimental results are compared with current methods under the same conditions. The proposed model using knowledge reasoning techniques in the optimal coverage path performs better than the conventional algorithms in terms of high robot coverage and low repetition rates. Experiments are done with real robots for cleaning in dynamic environments."},{"id":3038265118,"microsoftAcademicId":3038265118,"numberInSourceReferences":155,"doi":"10.1111/NUP.12306","title":"Artificial Intelligence and Robotics in Nursing: Ethics of Caring as a Guide to Dividing Tasks Between AI and Humans","authors":[{"LN":"Stokes","FN":"Felicia","affil":"American Nurses Association"},{"LN":"Palmer","FN":"Amitabha","affil":"Bowling Green State University"}],"year":2020,"journal":"Nursing Philosophy","references":[1892502720,2764587809,138047100,1519302612,2106175837,1480674497,2037289888,2518520875,2757629518,2317411237,3080186923,2140691049,2078612810,2078231698,1562091962,2121988878,1828801014,2906080203,2136248575,2108735670,1966396457,2159983377,2949450271,2800789377,2900471656,2901315453,2981505877,2771256564,2902050281,2913790922,2888147269,2304144219,2953884621,2740247345,2768567966],"citationsCount":1,"abstract":"Nurses have traditionally been regarded as clinicians that deliver compassionate, safe, and empathetic health care (Nurses again outpace other professions for honesty & ethics, 2018). Caring is a fundamental characteristic, expectation, and moral obligation of the nursing and caregiving professions (Nursing: Scope and standards of practice, American Nurses Association, Silver Spring, MD, 2015). Along with caring, nurses are expected to undertake ever-expanding duties and complex tasks. In part because of the growing physical, intellectual and emotional demandingness, of nursing as well as technological advances, artificial intelligence (AI) and AI care robots are rapidly changing the healthcare landscape. As technology becomes more advanced, efficient, and economical, opportunities and pressure to introduce AI into nursing care will only increase. In the first part of the article, we review recent and existing applications of AI in nursing and speculate on future use. Second, situate our project within the recent literature on the ethics of nursing and AI. Third, we explore three dominant theories of caring and the two paradigmatic expressions of caring (touch and presence) and conclude that AI-at least for the foreseeable future-is incapable of caring in the sense central to nursing and caregiving ethics. We conclude that for AI to be implemented ethically, it cannot transgress the core values of nursing, usurp aspects of caring that can only meaningfully be carried out by human beings, and it must support, open, or improve opportunities for nurses to provide the uniquely human aspects of care."},{"id":3129587193,"microsoftAcademicId":3129587193,"numberInSourceReferences":148,"doi":"10.1007/978-3-030-70569-5_23","title":"Artificial Empathy for Clinical Companion Robots with Privacy-By-Design","authors":[{"LN":"Martin","FN":"Miguel Vargas","affil":"University of Ontario Institute of Technology"},{"LN":"Valle","FN":"Eduardo Pérez","affil":"Monterrey Institute of Technology and Higher Education"},{"LN":"Horsburgh","FN":"Sheri","affil":"Ontario Shores Centre for Mental Health Sciences"}],"year":2020,"journal":"Wireless Mobile Communication and Healthcare. 9th EAI International Conference, MobiHealth 2020, Virtual Event, November 19, 2020, Proceedings","references":[2149628368,2154543439,2072206615,2082494017,2803609229,2031528526,3029022390,2985913519,2062500561,1777859530,3032875465,2891399311,1807810766,2963077926,3048796438,3004834163,3006066848],"citationsCount":0,"abstract":"We present a prototype whereby we enabled a humanoid robot to be used to assist mental health patients and their families. Our approach removes the need for Cloud-based automatic speech recognition systems to address healthcare privacy expectations. Furthermore, we describe how the robot could be used in a mental health facility by giving directions from patient selection to metrics for evaluation. Our overarching goal is to make the robot interaction as natural as possible to the point where the robot can develop artificial empathy for the human companion through the interpretation of vocals and facial expressions to infer emotions."},{"id":3041224585,"microsoftAcademicId":3041224585,"numberInSourceReferences":28,"doi":"10.1007/978-3-030-49062-1_44","title":"Emotion Synchronization Method for Robot Facial Expression","authors":[{"LN":"Kajihara","FN":"Yushun","affil":"Shibaura Institute of Technology"},{"LN":"Sripian","FN":"Peeraya","affil":"Shibaura Institute of Technology"},{"LN":"Feng","FN":"Chen","affil":"Shibaura Institute of Technology"},{"LN":"Sugaya","FN":"Midori","affil":"Shibaura Institute of Technology"}],"year":2020,"journal":"Thematic Area on Human Computer Interaction, HCI 2020, held as part of the 22nd International Conference on Human-Computer Interaction, HCII 2020","references":[2167557160,2111926505,2149628368,2044807399,34686378,2112668762,2132787602,2751747655,2520864854,2961087220,652662681,3000026036,1743677160],"citationsCount":0,"abstract":"Nowadays, communication robots are becoming popular since they are actively used in both commercially and personally. Increasing empathy between human-robot can effectively enhance the positive impression. Empathy can be created by syncing human emotion with the robot expression. Emotion estimation can be done by analyzing controllable expressions like facial expression, or uncontrollable expression like biological signals. In this work, we propose the comparison of robot expression synchronization with estimated emotion based on either facial expression or biological signal. In order to find out which of the proposed methods yield the best impression, subjective impression rating is used in the experiment. From the result of the impression evaluation, we found that the robot’s facial expression synchronization using the synchronization based on periodical emotion value performs the best and best suitable for emotion estimated both from facial expression and biological signal."},{"id":2941805380,"microsoftAcademicId":2941805380,"numberInSourceReferences":106,"doi":"10.1299/JAMDSM.2019JAMDSM0032","title":"A pupil response system using hemispherical displays for enhancing affective conveyance","authors":[{"LN":"Sejima","FN":"Yoshihiro","affil":"Kansai University"},{"LN":"Egawa","FN":"Shoichi","affil":"Okayama Prefectural University"},{"LN":"Sato","FN":"Yoichiro","affil":"Okayama Prefectural University"},{"LN":"Watanabe","FN":"Tomio","affil":"Okayama Prefectural University"}],"year":2019,"journal":"Journal of Advanced Mechanical Design Systems and Manufacturing","references":[2106395586,2124436197,165886685,2156257989,1985479696,1600099874,2474020747,2530839684,2047759366,2106938390,2034275633,2557119683,1979675537,2774735940,2344372820,2512365838,2055864799,2313415856,2026307846,2516982236,2319306420,2018365655,2004685567,2330476375],"citationsCount":0},{"id":3041933250,"microsoftAcademicId":3041933250,"numberInSourceReferences":137,"doi":"10.1145/3372923.3404795","title":"A Sociolinguistic Route to the Characterization and Detection of the Credibility of Events on Twitter","authors":[{"LN":"Patro","FN":"Jasabanta","affil":"Indian Institute of Technology Kharagpur"},{"LN":"Rathore","FN":"Pushpendra Singh","affil":"Indian Institute of Technology Kharagpur"}],"year":2020,"journal":"Proceedings of the 31st ACM Conference on Hypertext and Social Media","references":[2153579005,2101196063,2470673105,2022204871,2084591134,2140910804,2250966211,2081212507,2050619059,2170240176,2128721751,1934362406,1638051351,2740721704,2133046612,3125491592,2170196196,2100974526,149915781,3102375904,2026037750,2167024389,3124798136,3103319922,65754723,2115893922,2563852449,2009433065,3122743006,1996892032,2749784378,2290484796,1492030249,2886813870,2137449855,2079621256,2006525417,2402880877,2335241139,2014658588,2567100580,2588172982,289326142,2156545603,2153798276,2973745144,2952979842,2332269665,3037078099,2076940688],"citationsCount":0,"abstract":"Although Twitter constitutes as one of the primary sources of real-time news with users acting as the sensors updating the content from all across the globe, yet the spread of rumours via Twitter is becoming an increasingly alarming issue and is known to have caused significant damage already. We propose a credibility analysis approach based on the linguistic structure of the tweets. We not only characterize the Twitter events but also predict their perceived credibility of them by a novel deep learning architecture. We use the huge CREDBANK data to conduct our experiments. Some of our exciting findings are that standard LIWC categories like 'negate', 'discrep', 'cogmech', 'swear' and the Empath categories like 'hate', 'poor', 'government', 'worship' and 'swearing-terms' correlate negatively with the credibility of events. While some of our results resonate with the earlier literature others represent novel insights of the fake and legitimate twitter events. Using the above observations and the current deep learning architecture we predict the credibility of an event (a four-class classification problem in our case) with an accuracy of 0.54 that improves the best-known state-of-the-art (current accuracy 0.43) by ~ 26%. A fascinating observation is that even by looking at the first few tweets of an event, it is possible to make the prediction almost as accurate as in the case where the entire volume of tweets is observed."},{"id":2245472831,"microsoftAcademicId":2245472831,"numberInSourceReferences":8,"doi":"10.1109/SMC.2015.45","title":"Facial Expression of Social Interaction Based on Emotional Motivation of Animal Robot","authors":[{"LN":"Chumkamon","FN":"Sakmongkon","affil":"Kyushu Institute of Technology"},{"LN":"Masato","FN":"Koike","affil":"Kyushu Institute of Technology"},{"LN":"Hayashi","FN":"Eiji","affil":"Kyushu Institute of Technology"}],"year":2015,"journal":"2015 IEEE International Conference on Systems, Man, and Cybernetics","references":[2164598857,2125838338,2099019320,1769974409,2163998463,2001381166,2066064791,2177951040,2101783133,2009533999,2139656607,1979675537,1578752807],"citationsCount":1,"citationContext":{"1578752807":["Accordingly, the psychological process of organism evolved in the phylogenesis from a unicellular organism to human according to the human consciousness development [8]."],"1769974409":["In addition to CLM, active appearance model (AAM), which is precise, has also been very popular [11]."],"2001381166":["The research outwardly plays with environment or object [4], [5]."],"2009533999":["The behavior category classified into low to high level depending to the complexity of the action [7].","Two arms and one face with respect to semihumanoid architecture constructed this robot [7]."],"2066064791":["The construction of the emotion structure and relationship of expression is considered according to Robert Plutchik’s model of emotion [16]."],"2099019320":["The research outwardly plays with environment or object [4], [5]."],"2101783133":["They investigate the interaction method with the human such that system is applied to imitate human facial expression [6]."],"2125838338":["HMM, where we use a left-right model [13].","probability distribution given by (8) [13]."],"2177951040":["Macarthy proposed to the idea that the robot should have its own instinct such as the humanity that has consciousness, emotion, craving and introspection [2],[3]."]},"abstract":"This paper aims to develop the research based on a pet robot and its artificial consciousness. We propose the animal behavior and emotion using the artificial neurotransmitter and motivation. This research still implements the communication between human and a pet robot respecting to a social cognitive and interaction. Thus, the development of cross-creature communication is crucial for friendly companionship. This system focuses on three points. The first that is the organization of the behavior and emotion model regarding the phylogenesis. The second is the method of the robot that can have empathy with user expression. The third is how the robot can socially perform its expression to human for encouragement or being delighted based on its own emotion and the human expression. This paper eventually presents the performance and the experiment that the robot using cross-perception and cross-expression between animal robot and social interaction of human communication based on the consciousness based architecture (CBA)."},{"id":2528822309,"microsoftAcademicId":2528822309,"numberInSourceReferences":5,"doi":"10.1007/978-3-319-47437-3_15","title":"The Effects of Cognitive Biases in Long-Term Human-Robot Interactions: Case Studies Using Three Cognitive Biases on MARC the Humanoid Robot","authors":[{"LN":"Biswas","FN":"Mriganka","affil":"University of Lincoln"},{"LN":"Murray","FN":"John Christopher","affil":"University of Lincoln"}],"year":2016,"journal":"International Conference on Social Robotics","references":[2146006411,2158390034,2021641437,2304108689,1987250353,1994799114,2134415008,2002024051,2108626787,2269742369,2162417293,2016377072,2105496692,1637669343,2140182371,2094223914,2096929453,2127987946,1988403718,2057451632,1979082218,2142224342,2159289865,2203606878],"citationsCount":2,"abstract":"The research presented in this paper is part of a wider study investigating the role cognitive bias plays in developing long-term companionship between a robot and human. In this paper we discuss, how cognitive biases such as misattribution, Empathy gap and Dunning-Kruger effects can play a role in robot-human interaction with the aim of improving long-term companionship. One of the robots used in this study called MARC (See Fig. 1) was given a series of biased behaviours such as forgetting participant’s names, denying its own faults for failures, unable to understand what a participant is saying, etc. Such fallible behaviours were compared to a non-biased baseline behaviour. In the current paper, we present a comparison of two case studies using these biases and a non-biased algorithm. It is hoped that such humanlike fallible characteristics can help in developing a more natural and believable companionship between Robots and Humans. The results of the current experiments show that the participants initially warmed to the robot with the biased behaviours."},{"id":3133640410,"microsoftAcademicId":3133640410,"numberInSourceReferences":92,"doi":"10.14419/IJET.V7I2.12.11038","title":"Impact of human-robot interaction on user satisfaction with humanoid-based healthcare","authors":[{"LN":"Kwon","FN":"Ohbyung"},{"LN":"Kim","FN":"Jeonghun"},{"LN":"Jin","FN":"Yoonsun"},{"LN":"Lee","FN":"Namyeon"}],"year":2018,"journal":"International journal of engineering and technology","references":[],"citationsCount":0},{"id":2952114518,"microsoftAcademicId":2952114518,"numberInSourceReferences":41,"doi":"10.1109/SIEDS.2019.8735613","title":"Ideal Warrior and Robot Relations: Stress and Empathy's Role in Human-Robot Teaming","authors":[{"LN":"Peterson","FN":"Jordan","affil":"United States Air Force Academy"},{"LN":"Cohen","FN":"Chase","affil":"United States Air Force Academy"},{"LN":"Harrison","FN":"Paige","affil":"United States Air Force Academy"},{"LN":"Novak","FN":"Jonathan","affil":"United States Air Force Academy"},{"LN":"Tossell","FN":"Chad","affil":"United States Air Force Academy"},{"LN":"Phillips","FN":"Elizabeth","affil":"United States Air Force Academy"}],"year":2019,"journal":"2019 Systems and Information Engineering Design Symposium (SIEDS)","references":[2157289187,1853322427,2062845262,2149260122,2483821574,2912641090,2012511508,2536272183,2810522772,2794057029],"citationsCount":1,"citationContext":{"2012511508":["The study went on to note there is a larger difference in empathy felt towards humans and robots that are in distress as compared to those receiving affection [7]."],"2062845262":["check for stress induced by time pressure was conducted using galvanic skin response (GSR) and the NASA-TLX [4] measure of subjective workload.","deserved fair treatment, however the extent of these feelings lessened as the population’s age increased [4]."],"2149260122":["The ability to form empathetic bonds stems from a person’s ability to recognize and share the emotions of another organism [8]."],"2157289187":["NASA-TLX Participants also completed the NASA-TLX [12] to provide a measure of perceived workload as a result of the stress manipulation."],"2483821574":["As the use of technology transitions from tools to teammates [6], the United States Military must consider what a human-machine team will look like and how an appropriate relationship between the two assets can be formed [17,18]."],"2536272183":["Based on the report of the Colonel’s unwillingness to continue the training operation because of the IED robot’s sacrifice [2] in conjunction with research stating that people show a more intensified ability to share others’ emotions when under higher stress conditions [10],","and feelings [10]."],"2794057029":["Prior research studies have shown that people do indeed show empathy toward robots [15, 16]."],"2810522772":["Altering the appearance of the robot could also change empathetic reactions toward the robot as well [13,14]."],"2912641090":["As the use of technology transitions from tools to teammates [6], the United States Military must consider what a human-machine team will look like and how an appropriate relationship between the two assets can be formed [17,18]."]},"abstract":"The battlefield of the future will look very different than the battlefields of the past. Automated technologies are finding themselves more and more integrated into every aspect of the fight. As technology continues to advance, the United States Military must consider what a human-machine team will look like and how an optimal relationship between the two assets can be formed, especially under the stressful conditions that often characterize military contexts. For a human-machine team in a military context to work at maximum efficiency, an ideal level of empathy towards an automated teammate must be obtained. The goal of this study is to determine the effect stress can have on an individual's empathetic reaction toward a Pepper robot. Twenty-eight participants interacted with a Pepper robot either under stress or not. Empathy toward the robot was measured through subjective assessments as well as by participant decisions to continue interacting with Pepper even though doing so would harm the robot. Although not conclusive, the results suggest an interaction between participant gender and stress on empathy toward the Pepper robot. Women showed more empathy toward Pepper under higher levels of stress than lower levels of stress. However, the opposite was true for men. Men showed less empathy toward Pepper under higher levels of stress. The results of this study could help to inform military training and robot design."},{"id":3087665370,"microsoftAcademicId":3087665370,"numberInSourceReferences":154,"doi":"10.3390/APP10186531","title":"A Robot Has a Mind of Its Own Because We Intuitively Share It","authors":[{"LN":"Sumitani","FN":"Mizuho"},{"LN":"Osumi","FN":"Michihiro"},{"LN":"Abe","FN":"Hiroaki"},{"LN":"Azuma","FN":"Kenji"},{"LN":"Tsuchida","FN":"Rikuhei"},{"LN":"Sumitani","FN":"Masahiko"}],"year":2020,"journal":"Applied Sciences","references":[],"citationsCount":0,"abstract":"People perceive the mind in two dimensions: intellectual and affective. Advances in artificial intelligence enable people to perceive the intellectual mind of a robot through their semantic interactions. Conversely, it has been still controversial whether a robot has an affective mind of its own without any intellectual actions or semantic interactions. We investigated pain experiences when observing three different facial expressions of a virtual agent modeling affective minds (i.e., painful, unhappy, and neutral). The cold pain detection threshold of 19 healthy subjects was measured as they watched a black screen, then changes in their cold pain detection thresholds were evaluated as they watched the facial expressions. Subjects were asked to rate the pain intensity from the respective facial expressions. Changes of cold pain detection thresholds were compared and adjusted by the respective pain intensities. Only when watching the painful expression of a virtual agent did, the cold pain detection threshold increase significantly. By directly evaluating intuitive pain responses when observing facial expressions of a virtual agent, we found that we ‘share’ empathic neural responses, which can be intuitively emerge, according to observed pain intensity with a robot (a virtual agent)."},{"id":3089962427,"microsoftAcademicId":3089962427,"numberInSourceReferences":140,"doi":"10.1145/3404983.3410414","title":"Personal quizmaster: a pattern approach to personalized interaction experiences with the MiRo robot","authors":[{"LN":"Pollmann","FN":"Kathrin","affil":"Institute for Industrial Engineering, Stuttgart, Germany"},{"LN":"Ziegler","FN":"Daniel","affil":"Institute for Industrial Engineering, Stuttgart, Germany"}],"year":2020,"journal":"Proceedings of the Conference on Mensch und Computer","references":[],"citationsCount":0,"abstract":"In Human-Robot Interaction, personalization has been proposed as a strategy to increase acceptance for social robots. The present paper describes how behavioral design patterns can be used to tailor the interaction experience to the individual user's characteristics and needs. To demonstrate this approach, we designed a quiz game application for the MiRo robot. The robot acts as the quizmaster and shows different behaviors (coach-like/empathic vs. challenging/provocative) depending on the type of user who is playing the game (community-focused vs. competition-focused player). We describe the process of creating the two quizmaster personalities and related behavioral patterns as well as the technical background for integrating them with the interaction model for the quiz game. The result is a Wizard-of-Oz demonstration of the personalizable quiz game that is accompanied by an interactive video prototype remote for user studies and demo purposes."},{"id":2965222815,"microsoftAcademicId":2965222815,"numberInSourceReferences":49,"doi":"10.1007/978-3-030-20441-9_11","title":"Development of a Pupil Response System with Empathy Expression in Face-to-Face Body Contact","authors":[{"LN":"Sejima","FN":"Yoshihiro","affil":"Okayama Prefectural University"},{"LN":"Sato","FN":"Yoichiro","affil":"Okayama Prefectural University"},{"LN":"Watanabe","FN":"Tomio","affil":"Okayama Prefectural University"}],"year":2018,"journal":"The Proceedings of JSME annual Conference on Robotics and Mechatronics (Robomec)","references":[],"citationsCount":0,"abstract":"Pupil response is closely related to human affects and emotions. Focusing on the pupil response in human- robot interaction, we developed a pupil response interface using hemisphere displays for enhancing affective expression. This interface can generate pupil response like human by speech input and enhance affective expression. In this study, for the basic research of forming an intimate communication between human and pet-robot, we analyzed the pupil response during his or her body contact stroking forearm or head by using a pupil measurement device. Based on the analysis, we developed an advanced pupil response system for enhancing intimacy. This system generates the empathy expression when the talker touches any surface of hemisphere displays. The effectiveness of the system was confirmed experimentally."},{"id":2014831908,"microsoftAcademicId":2014831908,"numberInSourceReferences":136,"doi":"10.1145/2808435.2808445","title":"Modeling and Simulating Empathic Behavior in Social Assistive Robots","authors":[{"LN":"Carolis","FN":"Berardina De","affil":"Dipartimento di Informatica, Univerisita' di Bari, Aldo Moro, Via Orabona 4, 70126 Bari, Italy"},{"LN":"Ferilli","FN":"Stefano","affil":"Dipartimento di Informatica, Univerisita' di Bari, Aldo Moro, Via Orabona 4, 70126 Bari, Italy"},{"LN":"Palestra","FN":"Giuseppe","affil":"Dipartimento di Informatica, Univerisita' di Bari, Aldo Moro, Via Orabona 4, 70126 Bari, Italy"},{"LN":"Carofiglio","FN":"Valeria","affil":"Dipartimento di Informatica, Univerisita' di Bari, Aldo Moro, Via Orabona 4, 70126 Bari, Italy"}],"year":2015,"journal":"Proceedings of the 11th Biannual Conference on Italian SIGCHI Chapter","references":[2339343773,1755360231,1841352775,1977137834,1645937837,1985945240,1975000068,2154543439,2111040806,1967769980,2911336608,1941267885,1993925701,2109243771,2146799102,2010943014,2080593835,2153457831,2005210865,2088060499,2083307552,2124937956,2133247294,2102548748,2146879304,2136930480,2273893016,2151340886,1722056324,1997318488,1597748886,2047366844,1968425037,2735835184,64064869,6022847,2129760663,88910451],"citationsCount":2,"abstract":"Several studies report successful results on how social assistive robots can be employed as interface in the assisted living domain. In our opinion, to plan their response and interact successfully with people, it is crucial to recognize human emotions. To this aim, features of the prosody of the speech together with facial expressions and gestures may be used to recognize the emotional state of the user. The information gained from these different sources may be fused in order to endow the robot with the capability to reason on the user's affective state. In this paper we describe how this capability has been implemented in the NAO robot and how this allows simulating empathic behaviors in the context of Ambient Assisted Living."},{"id":3013280359,"microsoftAcademicId":3013280359,"numberInSourceReferences":6,"doi":"10.1109/ISPA-BDCLOUD-SUSTAINCOM-SOCIALCOM48970.2019.00230","title":"Predicting Future Alleviation of Mental Illness in Social Media: An Empathy-Based Social Network Perspective","authors":[{"LN":"Chai","FN":"Yibo","affil":"Central University of Finance and Economics"},{"LN":"Wu","FN":"Fengyang","affil":"Central University of Finance and Economics"},{"LN":"Sun","FN":"Rui","affil":"Central University of Finance and Economics"},{"LN":"Zhang","FN":"Zhongliang","affil":"Central University of Finance and Economics"},{"LN":"Bao","FN":"Jie","affil":"Central University of Finance and Economics"},{"LN":"Ma","FN":"Runxin","affil":"Central University of Finance and Economics"},{"LN":"Peng","FN":"Qizhou","affil":"Central University of Finance and Economics"},{"LN":"Wu","FN":"Danqin","affil":"Central University of Finance and Economics"},{"LN":"Wan","FN":"Yexing","affil":"Shenzhen Corerain Technologies Co. Ltd"},{"LN":"Li","FN":"Keyu","affil":"Central University of Finance and Economics"}],"year":2019,"journal":"2019 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & Communications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom)","references":[],"citationsCount":0,"abstract":"Numerous studies have shown that users' posts on social media can explicitly or implicitly reflect various human psychological characteristics. Through mining these data, predictive models can be built to forecast and analyze potential mental illness, which can facilitate therapeutic decision making and offer the best hope for early interventions and treatments. However, most existing approaches face severe information loss and ignore the time dynamics of user behaviour. To fill the research gaps, we use time-aware social networks to combine various information sources in social media posts. Also, machine learning detectors are trained to automatically identify empathic interactions, filter out irrelevant information and construct empathy-based networks. Finally, we devise a hybrid deep learning algorithm to learn embeddings from the dynamic feature-rich networks and predict future alleviation of mental illness. Compared with strong baselines, our approach achieves the best-performing results with efficient computation speed."},{"id":3099130608,"microsoftAcademicId":3099130608,"numberInSourceReferences":90,"doi":"10.3390/ROBOTICS9040092","title":"Differential facial articulacy in robots and humans elicit different levels of responsiveness, empathy, and projected feelings","authors":[{"LN":"Konijn","FN":"Elly A.","affil":"Communication Science"},{"LN":"Hoorn","FN":"Johan F.","affil":"Communication Science"}],"year":2020,"journal":"Robotics","references":[2148905283,2099019320,1841352775,2998827830,2887260263,1991015565,2136009731,2538626945,2062845262,2154041895,2095436958,2080543434,2161518694,2521535695,2588736771,2011525365,2788647475,1974294498,2170179129,2143399655,2090189546,2026871208,3080366724,1981509185,2770664338,2511084678,2056571671,2732925385,2035128922,2110739052,2282380228,2012511508,2113921716,2059975322,2027288040,2791147019,2598596840,2116301112,2187030616,2062637074,2996323795,2081007454,1642069785,44623247,3041437411,2513874396,2773152657,2143087571,2066758746,20018439,3089169339,2082674756,3091437854,2968388249],"citationsCount":1,"abstract":"Life-like humanoid robots are on the rise, aiming at communicative purposes that resemble humanlike conversation. In human social interaction, the facial expression serves important communicative functions. We examined whether a robot’s face is similarly important in human-robot communication. Based on emotion research and neuropsychological insights on the parallel processing of emotions, we argue that greater plasticity in the robot’s face elicits higher affective responsivity, more closely resembling human-to-human responsiveness than a more static face. We conducted a between-subjects experiment of 3 (facial plasticity: human vs. facially flexible robot vs. facially static robot) × 2 (treatment: affectionate vs. maltreated). Participants (N = 265; Mage = 31.5) were measured for their emotional responsiveness, empathy, and attribution of feelings to the robot. Results showed empathically and emotionally less intensive responsivity toward the robots than toward the human but followed similar patterns. Significantly different intensities of feelings and attributions (e.g., pain upon maltreatment) followed facial articulacy. Theoretical implications for underlying processes in human-robot communication are discussed. We theorize that precedence of emotion and affect over cognitive reflection, which are processed in parallel, triggers the experience of ‘because I feel, I believe it’s real,’ despite being aware of communicating with a robot. By evoking emotional responsiveness, the cognitive awareness of ‘it is just a robot’ fades into the background and appears not relevant anymore."},{"id":966236943,"microsoftAcademicId":966236943,"numberInSourceReferences":116,"doi":"10.1007/978-3-319-10783-7_9","title":"Teach Your Robot How You Want It to Express Emotions","authors":[{"LN":"Virčíkova","FN":"Mária","affil":"Technical University of Košice"},{"LN":"Sinčák","FN":"Peter","affil":"Technical University of Košice"}],"year":2015,"references":[2120945046,1976869056,1987250353,2136175037,1174325445,2109196208,2169166781,2053782908,2157484251,2482025096,2127536482,2080442667,2096214539,2157441726,1558032949,2043180902,2139283211,2065517080,1884629038,2071962346,89226271,1480774840,1551005385,2097333215,1497038487,2499340346,2109964945,2166278853,2113396990,2545800234,2099423386,1494017879,2152926099,2046511269,139572001,1556012235,2168651528],"citationsCount":0,"abstract":"We believe that in order for robots to interact naturally with humans, they should be able to express affective behavior. This paper deals with the development of an affective model for social robotics in which the resulting robotic expressions adapt according to the human subjective preferences. We have developed a method which can be used by non-technical individuals to design the affective models of humanoid robots. Our vision of the future research is that the proposed personalization will be treated, from user’s perspective, as an empathic response of the machine. We see the major contribution of this unique approach especially in long-term human-robot relationships and it could ultimately lead to robots being accepted in a wider domain."},{"id":3094037564,"microsoftAcademicId":3094037564,"numberInSourceReferences":20,"doi":"10.1109/RO-MAN47096.2020.9223466","title":"The Maze of Realizing Empathy with Social Robots","authors":[{"LN":"Corretjer","FN":"Marialejandra Garcia","affil":"La Salle University"},{"LN":"Ros","FN":"Raquel","affil":"La Salle University"},{"LN":"Martin","FN":"Fernando","affil":"La Salle University"},{"LN":"Miralles","FN":"David","affil":"La Salle University"}],"year":2020,"journal":"2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)","references":[1985945240,2147093380,2044954931,2101182646,2138949866,2136132767,3082245414],"citationsCount":0,"citationContext":{"2044954931":["In psychology, its origin is situated in the context of an expanding curiosity over sympathy and how people perceive their feelings in nature and in their context [5]."],"2101182646":["appraise the other’s situation when the other is not someone nor a situation you might initially relate to [11], [12]."],"2136132767":["Current research on empathy suggests examining it as a set of stages with well defined mechanisms[2]."],"2138949866":["In contrast, the Appraisal Theory of Empathy is not based on how people perceive another person’s emotions, but based on how people interpret the other’s situation [8]."],"2147093380":["Empathy as a term has had great evolution and transformation since its origin during the late 19th century captivating scientists across multiple disciplines [3]."]},"abstract":"Current trends envisage an evolution of collaboration, engagement, and relationship between humans and devices, intelligent agents and robots in our everyday life. Some of the key elements under study are affective states, motivation, trust, care, and empathy. This paper introduces an empathy test-bed that serves as a case study for an existing empathy model. The model describes the steps that need to occur in the process to provoke meaning in empathy, as well as the variables and elements that contextualise those steps. Based on this approach we have developed a fun collaborative scenario where a user and a social robot work together to solve a maze. A set of exploratory trials are carried out to gather insights on how users perceive the proposed test-bed around attachment and trust, which are basic elements for the realisation of empathy."},{"id":3133501623,"microsoftAcademicId":3133501623,"numberInSourceReferences":7,"doi":"10.1145/3434074.3447165","title":"Helper's High with a Robot Pet","authors":[{"LN":"Chirapornchai","FN":"Chatchai","affil":"Bristol Robotics Laboratory, Bristol, United Kingdom"},{"LN":"Bremner","FN":"Paul","affil":"Bristol Robotics Laboratory, Bristol, United Kingdom"},{"LN":"Daly","FN":"Joseph E.","affil":"Bristol Robotics Laboratory, Bristol, United Kingdom"}],"year":2021,"journal":"Companion of the 2021 ACM/IEEE International Conference on Human-Robot Interaction","references":[2032568497,3026935399,3036704904,1814107227,2170784062,2078550575,2014496024,2242014171,2166260038,2110373108,2895843825,2032105153,2789445413],"citationsCount":0,"abstract":"Helper's high is the phenomenon that helping someone or something else can lead to psychological benefits such as mood improvement. This study investigates if a robot pet can, like a real pet, induce helpers high in people interacting with it. A Vector robot was programmed to express the need for daily exercise and attention, and participants were instructed how to help the robot meet those needs. Our within subjects design had two conditions: with and without emotional behaviour modifiers to the robot's behaviour. Our primary research question is whether behaviours that conveyed emotion as well as needs would lead to empathy in the participants, which would create a stronger helper's high effect than purely functional need expression behaviours. We present a long-term (4 day) remote study design that not only facilitates the kind of interactions needed for helper's high, but abides by government guidelines on Covid-19 safety (under which a laboratory study is not possible). Preliminary results suggest that Vector was able to improve the mood of some participants, and mood changes tend to be greater when Vector expressed behaviours with emotional components. Our post-study interview data suggests that individual differences in living environment and mood impacting external factors, affected Vector's efficacy in mood influencing."},{"id":2938957361,"microsoftAcademicId":2938957361,"numberInSourceReferences":16,"doi":"10.1109/ICAITI.2018.8686759","title":"Effectiveness Of Android-Based Mobile Robots For Children Asperger Syndrome","authors":[{"LN":"Febtriko","FN":"Anip","affil":"Faculty of Engineering, Informatics Engineering Department, Pekanbaru, Indonesian"},{"LN":"Rahayuningsih","FN":"Tri","affil":"Faculty Psycology, Psycology Department, Pekanbaru, Indonesian"},{"LN":"Septiani","FN":"Dinda","affil":"Faculty Psycology, Psycology Department, Pekanbaru, Indonesian"},{"LN":"Trisnawati","FN":"Liza","affil":"Faculty of Engineering, Informatics Engineering Department, Pekanbaru, Indonesian"},{"LN":"Arisandi","FN":"Diki","affil":"Faculty of Engineering, Informatics Engineering Department, Pekanbaru, Indonesian"},{"LN":"Sukri","FN":"","affil":"Faculty of Engineering, Informatics Engineering Department, Pekanbaru, Indonesian"}],"year":2018,"journal":"2018 International Conference on Applied Information Technology and Innovation (ICAITI)","references":[2729492425,2312470954,2509036013,2801609452],"citationsCount":0,"abstract":"Autistic disorder is a disorder or developmental disorder in social interaction and communication and is characterized by limited activity and interest. One type of autism is Asperger Syndrome is a personal qualitative weakness in communicating and social interaction. Just like any other autistic child with Asperger's Syndrome, it is very difficult to understand emotions. Limitations in expressing and understanding emotions cause children with Asperger's Syndrome to retreat socially like aloof, indifferent, less interested in others, lack empathy, think in one direction, and think hard. Therefore it is necessary to apply play therapy for children with Asperger Syndrome disorder by using Android Mobile-based robot as a robot control tool. Wheel-shaped robot or wheeled robot with work area in the form of obstacles and obstacles with the aim that there is a challenge to run the robot. Research using Pre experimental design. The population in sampling is 15 children. Children with Asperger's Syndrome take the 6 -12 year age example at special school for Pekanbaru children. Data collection to assess the outcome of play therapy using Mobile robot, the data collected were analyzed by descriptive analysis and Rank Wilcoxon test. The main purpose of this study is the influence or effectiveness of the use of android-based mobile robots as a control tool against Asperger's Syndrome disorder in children in independent schools Pekanbaru to communicate and interact socially."},{"id":3160128201,"microsoftAcademicId":3160128201,"numberInSourceReferences":107,"doi":"10.35940/IJRTE.B1027.078219","title":"Deep learning-based facial expression recognition and analysis for filipino gamers","authors":[{"LN":"Sena","FN":"Juan Raphael"},{"LN":"Cabatuan","FN":"Melvin"}],"year":2019,"journal":"International Journal of Recent Technology and Engineering","references":[],"citationsCount":0}]