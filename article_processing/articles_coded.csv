Title,Abstract,Domain,Topic,Citations,pub_type,journal_conference,Year,country,def_imp,emp_dom,study_design,measurement_method,robot_human,Embodiment,embodiment,app_space,approach
A Deep Learning Approach to Modeling Empathy in Addiction Counseling,"Motivational interviewing is a goal-oriented psychotherapy, employed in cases such as addiction, that aims to help clients explore and resolve their ambivalence about their problem. In motivational interviewing, it is desirable for the counselor to communicate empathy towards the client to promote better therapy outcomes. In this paper, we propose a deep neural network (DNN) system for predicting counselors' session level empathy ratings from transcripts of the interactions. First, we train a recurrent neural network mapping the text of each speaker turn to a set of task-specific behavioral acts that represent local dynamics of the client-counselor interaction. Subsequently, this network is used to initialize lower layers of a deep network predicting session level counselor empathy rating. We show that this method outperforms training the DNN end-to-end in a single stage and also outperforms a baseline neural network model that attempts to predict empathy ratings directly from text without modeling turn level behavioral dynamics.",Machine learning,Machine learning,20,conferencePaper,"17TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2016), VOLS 1-5: UNDERSTANDING SPEECH PROCESSING IN HUMANS AND MACHINES",2016,US,Emotional empathy,Language,Machine learning,Machine learning,R,None / Text,Text,Human health,Machine learning
Empathic concern and the effect of stories in human-robot interaction,"People have been shown to project lifelike attributes onto robots and to display behavior indicative of empathy in human-robot interaction. Our work explores the role of empathy by examining how humans respond to a simple robotic object when asked to strike it. We measure the effects of lifelike movement and stories on people's hesitation to strike the robot, and we evaluate the relationship between hesitation and people's trait empathy. Our results show that people with a certain type of high trait empathy (empathic concern) hesitate to strike the robots. We also find that high empathic concern and hesitation are more strongly related for robots with stories. This suggests that high trait empathy increases people's hesitation to strike a robot, and that stories may positively influence their empathic responses.",Abuse study,Perception of robot,84,conferencePaper,2015 24TH IEEE INTERNATIONAL SYMPOSIUM ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION (RO-MAN),2015,US,Social intelligence;Embodiment,Embodied cues,Social interaction;Pain,"Physiological behaviour, Interview",h,Hexbug nano,Robot,HRI,Not specified
Social and empathic behaviours: novel interfaces and interaction modalities,"This paper describes the results of a research conducted in the European project Accompany, whose aim is to provide older people with services in a motivating and socially acceptable manner to facilitate independent living at home. The project developed a system consisting of a robotic companion, Care-O-bot, as part of a smart environment. An intensive research was conducted to investigate and experiment with robot behaviours that trigger empathic exchanges between an older person and the robot. The paper is articulated in two parts. The first part illustrates the theory that inspired the development of a context-aware Graphical User Interface (GUI) used to interact with the robot. The GUI integrates an expressive mask allowing perspective taking with the aim to stimulate empathic exchanges. The second part focuses on the user evaluation, and reports the outcomes from three different tests. The results of the first two tests show a positive acceptance of the GUI by the older people. The final test reports qualitative comments by senior participants on the occurrence of empathic exchanges with the robot.",Companion robot,Robot / AI development,6,conferencePaper,2015 24TH IEEE INTERNATIONAL SYMPOSIUM ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION (RO-MAN),2015,EU,Cognitive empathy; Emotional empathy,Language; Embodied cues,Social interaction,Interview; Interview,h,None / Graphical User Interface,GUI,HRI ; Human health,Not specified
Facial Expression of Social Interaction Based on Emotional Motivation of Animal Robot,"This paper aims to develop the research based on a pet robot and its artificial consciousness. We propose the animal behavior and emotion using the artificial neurotransmitter and motivation. This research still implements the communication between human and a pet robot respecting to a social cognitive and interaction. Thus, the development of cross-creature communication is crucial for friendly companionship. This system focuses on three points. The first that is the organization of the behavior and emotion model regarding the phylogenesis. The second is the method of the robot that can have empathy with user expression. The third is how the robot can socially perform its expression to human for encouragement or being delighted based on its own emotion and the human expression. This paper eventually presents the performance and the experiment that the robot using cross-perception and cross-expression between animal robot and Social interaction of human communication based on the consciousness based architecture (CBA).",Companion robot,Robot / AI development,4,conferencePaper,"2015 IEEE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN, AND CYBERNETICS (SMC 2015): BIG DATA ANALYTICS FOR HUMAN-CENTRIC SYSTEMS",2015,JP,Emotional empathy,Facial expression,Social interaction,"Machine learning, Physiological behaviour",h,Conbe Robot,Robot,HRI,Machine learning;Cognitive model
Empathic Interaction using the Computational Emotion Model,"This paper describes the empathy oriented human-robot interaction model. It is projected to design the model capable of different empathic responses (parallel and reactive) during the course of interaction with the user, depending upon the personality and mood factors of the robot. The proposed model encompasses three main stages i.e., perception, empathic appraisal and empathic expression. Perception refers to capturing user's emotion state via facial expression recognition. Empathic appraisal is based on the computational emotional model for generating its internal emotions, mood state and empathic responses. The internal emotions are defined using psychological studies and generated on 2D (pleasure-arousal) scaling model; whereas, fuzzy logic is used to calculate the intensity of the each emotion. A virtual facial expression simulator is applied for expression of resultant empathic emotions. Preliminary experimental results show that the proposed model is capable of exhibiting different empathic responses with respect to the personality and mood factors.",Model,Perception of robot,5,conferencePaper,2015 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL IN℡LIGENCE (IEEE SSCI),2015,MY,Social intelligence;Emotional empathy;Embodiment,Facial expression,Social interaction;Behaviour model,Machine learning,R,Videos,Visual,HRI ; Computer Vision,GOFAI;Machine learning
Evaluation of a Head Motion Synchronization System in the Communicative Process Between Human and Robot,"An aging population is world-wide social problem which affects many developed and developing countries. In this regard, many social robots based on reactive system have been developed to provide companionship for elderly adults with neurocognitive impairments such as dementia. However, these systems remain a problem such that no interactive dynamics of body gestures between human and robot has been considered. In this research, therefore, we developed a head motion synchronization system using mutual entrainment and implement it in a communication robot. This system was evaluated by conducting one-way face-to-face human-robot communication experiments with young native Japanese speakers under three conditions, namely unreactive, reactive and interactive conditions. Head motion synchrony analysis revealed a leader-follower relationship for the reactive model and a mutual entrainment of head motion for the interactive model. Also, Questionnaire survey results showed the degree of empathy for interactive condition was significantly higher as compared with reactive and unreactive conditions. In addition, the degree of naturalness for interactive condition was significantly higher as compared with unreactive condition. Hence, these indicate that empathy was shared through mutual entrainment of head motion, which could provide a smooth interface in human-robot communication. This system would be extended to elderly adults as an assistive system for the elderly's rehabilitation.",Health,Robot / AI development,0,conferencePaper,2016 55TH ANNUAL CONFERENCE OF THE SOCIETY OF INSTRUMENT AND CONTROL ENGINEERS OF JAPAN (SICE),2016,JP,Imitation,Embodied cues,Social interaction,"Physiological behaviour, Interview",h,NAO Robot,Robot,HRI,GOFAI
A Laughing-driven Pupil Response System for Inducing Empathy,"Laughing response plays an important role in supporting human interaction and communication, and enhances empathy by sharing laughter each other. Therefore, in order to develop communication systems which enhance empathy, it is desired to design the media representation using the pupil response which is related to affective response such as pleasure-unpleasure. In this paper, we aim to enhance empathy during human and robot interaction and communication, and develop a pupil response system for inducing empathy by laughing response using hemispherical display. In addition, we evaluate the pupil response with the laughing response by using the developed system. The results demonstrate that the dilated pupil response with laughing response is effective for enhancing empathy.",Perception of robot,Perception of robot,2,conferencePaper,2016 IEEE/SICE INTERNATIONAL SYMPOSIUM ON SYSTEM INTEGRATION (SII),2016,JP,Emotional empathy,Social behavior,Social interaction,Physiological behaviour,h,Robotic Eye,Robot,HRI,GOFAI
Evolving Artificial Pain from Fault Detection through Pattern Data Analysis,"Fault detection is a classical area of study in robotics and extensive research works have been dedicated to investigate its broad applications. As the breath of robots applications requiring human interaction grow, it is important for robots to acquire sophisticated social skills such as empathy towards pain. However, it turns out that this is difficult to achieve without having an appropriate concept of pain that relies on robots being aware of their own body machinery aspects. This paper introduces the concept of pain, based on the ability to develop a state of awareness of robots own body and the use of the fault detection approach to generate artificial robot pain. Faults provide the stimulus and defines a classified magnitude value, which constitutes artificial pain generation, comprised of synthetic pain classes. Our experiment evaluates some of synthetic pain classes and the results show that the robot gains awareness of its internal state through its ability to predict its joint motion and generate appropriate artificial pain. The robot is also capable of alerting humans whenever a task will generate artificial pain, or whenever humans fails to acknowledge the alert, the robot can take a considerable preventive actions through joint stiffness adjustment.",Embodiment,Robot / AI development,1,conferencePaper,2017 IEEE INTERNATIONAL CONFERENCE ON REAL-TIME COMPUTING AND ROBOTICS (RCAR),2017,ID,Embodiment,Embodied cues,Pain,Physiological behaviour ,h,NAO Robot,Robot,HRI,Cognitive model
Stacked Deep Convolutional Auto-Encoders for Emotion Recognition from Facial Expressions,"Emotion recognition is critical for everyday living and is essential for meaningful interaction. If we are to progress towards human and machine interaction that is engaging the human user, the machine should be able to recognize the emotional state of the user. Deep Convolutional Neural Networks (CNN) have proven to be efficient in emotion recognition problems. The good degree of performance achieved by these classifiers can be attributed to their ability to self-learn a down-sampled feature vector that retains spatial information through filter kernels in convolutional layers. Given the view that random initialization of weights can lead to convergence to non-optimal local minima, in this paper we explore the impact of training the initial weights in an unsupervised manner. We study the effect of pre-training a Deep CNN as a Stacked Convolutional Auto-Encoder (SCAE) in a greedy layer-wise unsupervised fashion for emotion recognition using facial expression images. When trained with randomly initialized weights, our CNN emotion recognition model achieves a performance rate of 91.16% on the Karolinska Directed Emotional Faces (KDEF) dataset. In contrast, when each layer of the model, including the hidden layer, is pre-trained as an Auto-Encoder, the performance increases to 92.52%. Pre-training our CNN as a SCAE also reduces training time marginally. The emotion recognition model developed in this work will form the basis of a real-time empathic robot system.",Image emotion classification,Machine learning,38,conferencePaper,2017 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN),2017,UK,Cognitive empathy,Facial expression,Machine learning; Visual,Machine learning,R,Image dataset,Visual,Computer Vision,Machine learning
Incremental Learning of Human Emotional Behavior for Social Robot Emotional Body Expression,"Generating emotional body expressions for social robots has been gaining increased attention to enhance the engagement and empathy in human-robot interaction. In this paper, an enhanced model of robot emotional body expression is proposed which places emphasis on the individual user's cultural traits. Similar to our previous paper, this approach is inspired by social and emotional development of infants interacting with their parents who have a certain cultural background. Social referencing occurs when infants perceive their parents' facial expressions and vocal tones of emotional situations to form their own interpretation. On the other hand, this model replaces the batch learning self-organizing map with the dynamic cell structure, incrementally training a neural network model with a variety of emotional behaviors obtained from the users with whom the robot interacts. We demonstrate the validity of our incremental learning model through a public human action dataset, which will facilitate the acquisition of emotional body expression of socially assistive robots as a reflection of the individual user's culture.",Machine learning,Machine learning,1,conferencePaper,2018 15TH INTERNATIONAL CONFERENCE ON UBIQUITOUS ROBOTS (UR),2018,JP;EU,Emotional empathy,Embodied cues,Behaviour model;Machine learning,Machine learning,R,Pepper Robot,Robot,HRI,Cognitive model;Machine learning
Artificial Empathy in Social Robots: An analysis of Emotions in Speech,"Artificial speech developed using speech synthesizers has been used as the voice for robots in Human Robot Interaction (HRI). As humans anthropomorphize robots, an empathetically interacting robot is expected to increase the level of acceptance of social robots. Here, a human perception experiment evaluates whether human subjects perceive empathy in robot speech. For this experiment, empathy is expressed only by adding appropriate emotions to the words in speech. Also, humans' preferences for a robot interacting with empathetic speech versus a standard robotic voice are also assessed. The results show that humans are able to perceive empathy and emotions in robot speech, and prefer it over the standard robotic voice. It is important for the emotions in empathetic speech to be consistent with the language content of what is being said, and with the human users' emotional state. Analyzing emotions in empathetic speech using valence-arousal model has revealed the importance of secondary emotions in developing empathetically speaking social robots.",Mirroring,Perception of robot,12,conferencePaper,2018 27TH IEEE INTERNATIONAL SYMPOSIUM ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION (IEEE RO-MAN 2018),2018,NZ,Emotional empathy,Embodied cues,Behaviour model,Interview,h,Heathbot,Robot,HRI,GOFAI
Effect of Explicit Emotional Adaptation on Prosocial Behavior of Humans towards Robots depends on Prior Robot Experience,"Emotional adaptation increases pro-social behavior of humans towards robotic interaction partners. Social cues are an important factor in this context. This work investigates, if emotional adaptation still works under absence of human-like facial Action Units. A human-robot dialog scenario is chosen using NAO pretending to work for a supermarket and involving humans providing object names to the robot for training purposes. In a user study, two conditions are implemented with or without explicit emotional adaptation of NAO to the human user in a between-subjects design. Evaluations of user experience and acceptance are conducted based on evaluated measures of human-robot interaction (HRI). The results of the user study reveal a significant increase of helpfulness (number of named objects), anthropomorphism, and empathy in the explicit emotional adaptation condition even without social cues of facial Action Units, but only in case of prior robot contact of the test persons. Otherwise, an opposite effect is found. These findings suggest, that reduction of these social cues can be overcome by robot experience prior to the interaction task, e.g. realizable by an additional bonding phase, confirming the importance of such from previous work. Additionally, an interaction with academic background of the participants is found.",Service,Robot / AI development,5,conferencePaper,2018 27TH IEEE INTERNATIONAL SYMPOSIUM ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION (IEEE RO-MAN 2018),2018,DE,Social intelligence;Emotional empathy;Cognitive empathy,Facial expression;Embodied cues;Social behavior,Social interaction,"Interview, Physiological behaviour",h,NAO Robot,Robot,HRI,GOFAI
Studying Effects of Incorporating Automated Affect Perception with Spoken Dialog in Social Robots,"Social robots are becoming an integrated part of our daily lives with the goal of understanding humans' social intentions and feelings, a capability which is often referred to as empathy. Despite significant progress towards the development of empathic social agents, current social robots have yet to reach the full emotional and social capabilities. This paper presents our recent effort on incorporating an automated Facial Expression Recognition (FER) system based on deep neural networks into the spoken dialog of a social robot (Ryan) to extend and enrich its capabilities beyond spoken dialog and integrate the user's affect state into the robot's responses. In order to evaluate whether this incorporation can improve social capabilities of Ryan, we conducted a series of Human-Robot-Interaction (HRI) experiments. In these experiments the subjects watched some videos and Ryan engaged them in a conversation driven by user's facial expressions perceived by the robot. We measured the accuracy of the automated FER system on the robot when interacting with different human subjects as well as three social/interactive aspects, namely task engagement, empathy, and likability of the robot. The results of our HRI study indicate that the subjects rated empathy and likability of the affect-aware Ryan significantly higher than non-empathic (the control condition) Ryan. Interestingly, we found that the accuracy of the FER system is not a limiting factor, as subjects rated the affect-aware agent equipped with a low accuracy FER system as empathic and likable as when facial expression was recognized by a human observer.",Companion robot,Robot / AI development,4,conferencePaper,2018 27TH IEEE INTERNATIONAL SYMPOSIUM ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION (IEEE RO-MAN 2018),2018,US,Emotional empathy,Facial expression,Social interaction,"Interview, Physiological behaviour",h,Ryan Companionbot,Robot,HRI,GOFAI;Machine learning
Counseling Robot Implementation and Evaluation,"A lot of IT personnel have psychological distress and counselors to help them are lack in number. Therefore, we proposed a counseling agent (CA) called CRECA (context respectful counseling agent), which listens to clients and promotes their reflection context respectfully namely in a context preserving way. This agent is now enhanced using a body language called “unazuki” in Japanese, a kind of nodding to greatly promote dialogue, often accompanying “un-un” (meaning “exactly”) of Japanese onomatopoeia. This body language significantly helps represent empathy or entire approval. Our agent is enhanced with such dialog promotion nodding robot to continue the conversation naturally or context respectfully towards clients' further reflection. To realize it, the robot nods twice at each end of dialog sentence input by clients. Here, we introduce a robot that behaves human-like by an appropriate nodding behavior. The motivation for such a more human-like robot was the extension of application fields from IT workers' counselling to people, who suffer from more social problems such as financial debt, or anxiety of victory or defeat. For such applications, it is important that the agent behaves as much as possible human-like. Here, we present an enhanced experimental evaluation. The quantitative evaluation is based on the utterance amounts of a test group of individuals. These amount with and without the nodding feature are compared. Additionally, the robots with and without nodding are compared according several subjective feelings by the evaluation subjects.",Therapy,Robot / AI development,3,conferencePaper,"2018 IEEE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN, AND CYBERNETICS (SMC)",2018,JP,Social intelligence,Embodied cues;Language,Social interaction,"Interview, Physiological behaviour",h,Custom Robotic Head,Robot,HRI ; Human health,GOFAI;Cognitive model
Band of Brothers and Bolts: Caring About Your Robot Teammate,It has been observed that a robot shown as suffering is enough to cause an empathic response from a person. Whether the response is a fleeting reaction with no consequences or a meaningful perspective change with associated behavior modifications is not clear. Existing work has been limited to measurements made at the end of empathy inducing experimental trials rather measurements made over time to capture consequential behavioral pattern. We report on preliminary results collected from a study that attempts to measure how the actions of a participant may be altered by empathy for a robot companion. Our findings suggest that induced empathy can in fact have a significant impact on a person's behavior to the extent that the ability to fulfill a mission may be affected.,Military,Robot / AI development,3,conferencePaper,2018 IEEE/RSJ INTERNATIONAL CONFERENCE ON IN℡LIGENT ROBOTS AND SYSTEMS (IROS),2018,US,Social intelligence;Embodiment,Social behavior,Social interaction;Game,Physiological behaviour ,h,Virtual Agent,GUI,HRI ; Game,GOFAI
Emotional Bodily Expressions for Culturally Competent Robots through Long Term Human-Robot Interaction,"Generating emotional bodily expressions for culturally competent robots has been gaining increased attention to enhance the engagement and empathy between robots and humans in a multi-culture society. In this paper, we propose an incremental learning model for selecting the user's representative or habitual emotional behaviors which place emphasis on individual users' cultural traits identified through long term interaction. Furthermore, a transformation model is proposed to convert the obtained emotional behaviors into a specific robot's motion space. To validate the proposed approach, the models were evaluated by two example scenarios of interaction. The experimental results confirmed that the proposed approach endows a social robot with the capability to learn emotional behaviors from individual users, and to generate its emotional bodily expressions. It was also verified that the imitated robot motions are rated emotionally acceptable by the demonstrator and recognizable by the subjects from the same cultural background with the demonstrator.",Model,Perception of robot,11,conferencePaper,2018 IEEE/RSJ INTERNATIONAL CONFERENCE ON IN℡LIGENT ROBOTS AND SYSTEMS (IROS),2018,JP;EU,Emotional empathy;Social intelligence,Facial expression;Embodied cues,Behaviour model;Machine learning,Machine learning,R,Pepper Robot,Robot,HRI,Cognitive model;Machine learning
Effectiveness of Android-Based Mobile Robots For Children Asperger Syndrome,"Autistic disorder is a disorder or developmental disorder in Social interaction and communication and is characterized by limited activity and interest. One type of autism is Asperger Syndrome is a personal qualitative weakness in communicating and Social interaction. Just like any other autistic child with Asperger's Syndrome, it is very difficult to understand emotions. Limitations in expressing and understanding emotions cause children with Asperger's Syndrome to retreat socially like aloof, indifferent, less interested in others, lack empathy, think in one direction, and think hard. Therefore it is necessary to apply play therapy for children with Asperger Syndrome disorder by using Android Mobile-based robot as a robot control tool. Wheel-shaped robot or wheeled robot with work area in the form of obstacles and obstacles with the aim that there is a challenge to run the robot. Research using Pre experimental design. The population in sampling is 15 children. Children with Asperger's Syndrome take the 6 -12 year age example at special school for Pekanbaru children. Data collection to assess the outcome of play therapy using Mobile robot, the data collected were analyzed by descriptive analysis and Rank Wilcoxon test. The main purpose of this study is the influence or effectiveness of the use of android-based mobile robots as a control tool against Asperger's Syndrome disorder in children in independent schools Pekanbaru to communicate and interact socially.",Education,Robot / AI development,0,conferencePaper,2018 International Conference on Applied Information Technology and Innovation (ICAITI),2018,ID,Social intelligence,Not specified,Social interaction,Physiological behaviour,h,Custom small wheeled robot / Cozmo-like,Robot,HRI; Human health,Not specified
Learning Empathy-Driven Emotion Expressions using Affective Modulations,"Human-Robot Interaction (HRI) studies, particularly the ones designed around social robots, use emotions as important building blocks for interaction design. In order to provide a natural interaction experience, these social robots need to recognise the emotions expressed by the users across various modalities of communication and use them to estimate an internal affective model of the interaction. These internal emotions act as motivation for learning to respond to the user in different situations, using the physical capabilities of the robot. This paper proposes a deep hybrid neural model for multi-modal affect recognition, analysis and Behaviour model in social robots. The model uses growing self-organising network models to encode intrinsic affective states for the robot. These intrinsic states are used to train a reinforcement learning model to learn facial expression representations on the Neuro-Inspired Companion (NICO) robot, enabling the robot to express empathy towards the users.",Machine learning,Machine learning,12,conferencePaper,2018 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN),2018,DE,Emotional empathy,Facial expression,Machine learning;Behaviour model,Physiological behaviour,R,NICO Robot,Robot,HRI,Machine learning
Performance Analysis of Unimodal and Multimodal Models in Valence-Based Empathy Recognition,"The human ability to empathise is a core aspect of successful interpersonal relationships. In this regard, human-robot interaction can be improved through the automatic perception of empathy, among other human attributes, allowing robots to affectively adapt their actions to interactants' feelings in any given situation. This paper presents our contribution to the generalised track of the One-Minute Gradual ( OMG) Empathy Prediction Challenge by describing our approach to predict a listener's valence during semi-scripted actor-listener interactions. We extract visual and acoustic features from the interactions and feed them into a bidirectional long short-term memory network to capture the time-dependencies of the valence-based empathy during the interactions. Generalised and personalised unimodal and multimodal valence-based empathy models are then trained to assess the impact of each modality on the system performance. Furthermore, we analyse if intra-subject dependencies on empathy perception affect the system performance. We assess the models by computing the concordance correlation coefficient ( CCC) between the predicted and self-annotated valence scores. The results support the suitability of employing multimodal data to recognise participants' valence-based empathy during the interactions, and highlight the subject-dependency of empathy. In particular, we obtained our best result with a personalised multimodal model, which achieved a CCC of 0.11 on the test set.",Machine learning,Machine learning,2,conferencePaper,2019 14TH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION (FG 2019),2019,EU;DE,Social intelligence,Facial expression;Embodied cues;Language,Machine learning;Social interaction,Machine learning,h,Videos,Visual,Computer Vision,Machine learning
Towards a Multimodal Time-Based Empathy Prediction System,We describe our system for empathic emotion recognition. It is based on deep learning on multiple modalities in a late fusion architecture. We describe the modules of our system and discuss the evaluation results. Our code is also available for the research community(1),Machine learning,Machine learning,1,conferencePaper,2019 14TH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION (FG 2019),2019,ES;UK,Emotional empathy,Facial expression,Machine learning,Machine learning,h,Video Dataset,Visual,Machine Learning ; HRI,Machine learning
Study of Empathy on Robot Expression Based on Emotion Estimated from Facial Expression and Biological Signals,"Empathy, the ability to share the other's feeling, is one of the effective elements in promoting mutual reliability and construction of a good relationship. In order to create empathy between human-robot, a robot must be able to estimate the emotion of human and reflect the same emotion on its expression. In general, emotion can be estimated based on observable expressions such as facial expression, or unobservable expressions such as biological signals. Although there are many methods for measuring emotion from both facial expression and biological signals, few studies have been done on the comparison of estimated emotion. In this paper, we investigate whether emotion estimated from facial expression or biological signals could lead to empathy toward a robot. Using our proposed emotion estimation system, we performed two experiments and found that higher impression was rated on sociability elements with significant when the reflected emotion is estimated from uncontrollable emotion.",Machine learning,Machine learning,2,conferencePaper,2019 28TH IEEE INTERNATIONAL CONFERENCE ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION (RO-MAN),2019,JP,Emotional empathy,Facial expression;Embodied cues,Behaviour model;Machine learning,"Physiological behaviour, Machine learning",R,SAM,Robot,HRI,Machine learning
Human-Robot Interaction based on Facial Expression Imitation,"Mimicry during face-to-face interpersonal interactions is a meaningful nonverbal communication signal that affects the quality of the communications and increases empathy towards the interaction partner. In this paper we propose a facial expression imitation system that utilizes a convolutional neural network (CNN). The model was trained by means of the CK+ database, which is a popular benchmark in facial expression recognition. Then, we implemented the proposed system on a robotic platform and investigated the method's performance via 20 recruited participants. We observed a high mean score of the participants' viewpoints on the imitation capability of the robot of 4.1 out of 5.",Mirroring,Perception of robot,1,conferencePaper,2019 7TH INTERNATIONAL CONFERENCE ON ROBOTICS AND MECHATRONICS (ICROM 2019),2019,IR,Imitation,Facial expression,Behaviour model;Machine learning,"Interview, Physiological behaviour","h, R",RASA Robot,Robot,HRI,Machine learning
Detection of Real-World Driving-Induced Affective State Using Physiological Signals and Multi-View Multi-Task Machine Learning,"Affective states have a critical role in driving performance and safety. They can degrade driver situation awareness and negatively impact cognitive processes, severely diminishing road safety. Therefore, detecting and assessing drivers' affective states is crucial in order to help improve the driving experience, and increase safety, comfort and well-being. Recent advances in affective computing have enabled the detection of such states. This may lead to empathic automotive user interfaces that account for the driver's emotional state and influence the driver in order to improve safety. In this work, we propose a multiview multi-task Machine learning method for the detection of driver's affective states using physiological signals. The proposed approach is able to account for inter-drive variability in physiological responses while enabling interpretability of the learned models, a factor that is especially important in systems deployed in the real world. We evaluate the models on three different datasets containing real-world driving experiences. Our results indicate that accounting for drive-specific differences significantly improves model performance.",Machine learning,Machine learning,1,conferencePaper,2019 8th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW),2019,US,Cognitive empathy,Embodied cues ,Machine learning,Machine learning ; Physiological behaviour,R,Physiological signals Dataset,Physiology,Machine Learning ,Machine learning
Predicting Future Alleviation of Mental Illness in Social Media: An Empathy-Based Social Network Perspective,"Numerous studies have shown that users' posts on social media can explicitly or implicitly reflect various human psychological characteristics. Through mining these data, predictive models can be built to forecast and analyze potential mental illness, which can facilitate therapeutic decision making and offer the best hope for early interventions and treatments. However, most existing approaches face severe information loss and ignore the time dynamics of user behaviour. To fill the research gaps, we use time-aware social networks to combine various information sources in social media posts. Also, Machine learning detectors are trained to automatically identify empathic interactions, filter out irrelevant information and construct empathy-based networks. Finally, we devise a hybrid deep learning algorithm to learn embeddings from the dynamic feature-rich networks and predict future alleviation of mental illness. Compared with strong baselines, our approach achieves the best-performing results with efficient computation speed.",Machine learning,Machine learning,0,conferencePaper,"2019 IEEE Intl Conf on Parallel Distributed Processing with Applications, Big Data Cloud Computing, Sustainable Computing Communications, Social Computing Networking (ISPA/BDCloud/SocialCom/SustainCom)",2019,CN,Emotional empathy;Cognitive empathy,Language,Social interaction,Machine learning,h,None / Text,Text,Human health ; NLP,Machine learning
Ideal Warrior and Robot Relations: Stress and Empathy's Role in Human-Robot Teaming,"The battlefield of the future will look very different than the battlefields of the past. Automated technologies are finding themselves more and more integrated into every aspect of the fight. As technology continues to advance, the United States Military must consider what a human-machine team will look like and how an optimal relationship between the two assets can be formed, especially under the stressful conditions that often characterize military contexts. For a human-machine team in a military context to work at maximum efficiency, an ideal level of empathy towards an automated teammate must be obtained. The goal of this study is to determine the effect stress can have on an individual's empathetic reaction toward a Pepper robot. Twenty-eight participants interacted with a Pepper robot either under stress or not. Empathy toward the robot was measured through subjective assessments as well as by participant decisions to continue interacting with Pepper even though doing so would harm the robot. Although not conclusive, the results suggest an interaction between participant gender and stress on empathy toward the Pepper robot. Women showed more empathy toward Pepper under higher levels of stress than lower levels of stress. However, the opposite was true for men. Men showed less empathy toward Pepper under higher levels of stress. The results of this study could help to inform military training and robot design.",Military,Robot / AI development,1,conferencePaper,2019 SYSTEMS AND INFORMATION ENGINEERING DESIGN SYMPOSIUM (SIEDS),2019,US,Emotional empathy,Social behavior,Social interaction,Interview,h,Pepper Robot,Robot,HRI,Not specified
Increasing Engagement with Chameleon Robots in Bartending Services,"As the field of service robotics has been rapidly growing, it is expected for such robots to be endowed with the appropriate capabilities to interact with humans in a socially acceptable way. This is particularly relevant in the case of customer relationships where a positive and affective interaction has an impact on the users' experience. In this paper, we address the question of whether a specific behavioral style of a barman-robot, acted through para-verbal and non-verbal behaviors, can affect users' engagement and the creation of positive emotions. To that end, we endowed a barman-robot taking drink orders from human customers, with an empathic behavioral style. This aims at triggering to alignment process by mimicking the conversation partner's behavior. This behavioral style is compared to an entertaining style, aiming at creating a positive relationship with the users, and a neutral style for control. Results suggest that when participants experienced more positive emotions, the robot was perceived as safer, so suggesting that interactions that stimulate positive and open relations with the robot may have a positive impact on the affective dimension of engagement. Indeed, when the empathic robot modulates its behavior according to the user's one, this interaction seems to be more effective than when interacting with a neutral robot in improving engagement and positive emotions in public-service contexts.",Service,Robot / AI development,0,conferencePaper,2020 29TH IEEE INTERNATIONAL CONFERENCE ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION (RO-MAN),2020,IT,Cognitive empathy,Embodied cues,Social Interaction,Interview,h,Pepper Robot,Robot,HRI ; Customer Service,Machine learning
Robot Mirroring: Promoting Empathy with an Artificial Agent by Reflecting the User's Physiological Affective States,"Self-tracking aims to increase awareness, decrease undesired behaviors, and ultimately lead towards a healthier lifestyle. However, inappropriate communication of self-tracking results might cause the opposite effect. Subtle self-tracking feedback is an alternative that can be provided with the aid of an artificial agent representing the self. Hence, we propose a wearable pet that reflects the user's affective states through visual and haptic feedback. By eliciting empathy and fostering helping behaviors towards it, users would indirectly help themselves. A wearable prototype was built, and three user studies performed to evaluate the appropriateness of the proposed affective representations. Visual representations using facial and body cues were clear for valence and less clear for arousal. Haptic interoceptive patterns emulating heart-rate levels matched the desired feedback urgency levels with a saturation frequency. The integrated visuo-haptic representations matched to participants own affective experience. From the results, we derived three design guidelines for future robot mirroring wearable systems: physical Embodiment, interoceptive feedback, and customization.",Health,Robot / AI development,0,conferencePaper,2020 29TH IEEE INTERNATIONAL CONFERENCE ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION (RO-MAN),2020,JP;CH;FR,Imitation;Emotional empathy,Embodied cues;Facial expression,Behaviour model,"Interview, Physiological behaviour",h,Virtual Agent,GUI,HRI,GOFAI
Robot-on-Robot Gossiping to Improve Sense of Human-Robot Conversation,"In recent years, a substantial amount of research has been aimed at realizing a social robot that can maintain long-term user interest. One approach is using a dialogue strategy in which the robot makes a remark based on previous dialogues with users. However, privacy problems may occur owing to private information of the user being mentioned. We propose a novel dialogue strategy whereby a robot mentions another robot in the form of gossiping. This dialogue strategy can improve the sense of conversation, which results in increased interest while avoiding the privacy issue. We examined our proposal by conducting a conversation experiment evaluated by subject impressions. The results demonstrated that the proposed method could help the robot to obtain higher evaluations. In particular, the perceived mind was improved in the Likert scale evaluation, whereas the robot empathy and intention to use were improved in the binary comparison evaluation. Our dialogue strategy may contribute to understanding the factors regarding the sense of conversation, thereby adding value to the field of human-robot interaction.",Chatbot,Robot / AI development,1,conferencePaper,2020 29TH IEEE INTERNATIONAL CONFERENCE ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION (RO-MAN),2020,JP,Social intelligence,Language,Social interaction,Interview,h,CommU,Robot,HRI,GOFAI
The Maze of Realizing Empathy with Social Robots,"Current trends envisage an evolution of collaboration, engagement, and relationship between humans and devices, intelligent agents and robots in our everyday life. Some of the key elements under study are affective states, motivation, trust, care, and empathy. This paper introduces an empathy test-bed that serves as a case study for an existing empathy model. The model describes the steps that need to occur in the process to provoke meaning in empathy, as well as the variables and elements that contextualise those steps. Based on this approach we have developed a fun collaborative scenario where a user and a social robot work together to solve a maze. A set of exploratory trials are carried out to gather insights on how users perceive the proposed test-bed around attachment and trust, which are basic elements for the realisation of empathy.",Model,Perception of robot,0,conferencePaper,2020 29TH IEEE INTERNATIONAL CONFERENCE ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION (RO-MAN),2020,ES,Emotional empathy;Social intelligence,Social behavior,Game,Interview,h,Robobo,Robot,HRI,GOFAI
Robot Facial Expression Framework for Enhancing Empathy in Human-Robot Interaction,"A social robot interacts with humans based on social intelligence, for which related applications are being developed across diverse fields to be increasingly integrated in modern society. In this regard, Social intelligence and interaction are the keywords of a social robot. Social intelligence refers to the ability to control interactions or thoughts and feelings of relationships with other people; primal empathy, which is the ability to empathize by perceiving emotional signals, among the components of Social intelligence was applied to the robot in this study. We proposed that the empathic ability of a social robot can be improved if the social robot can create facial expressions based on the emotional state of a user. Moreover, we suggested a framework of facial expressions for robots. These facial expressions can be repeatedly used in various social robot platforms to achieve such a strategy.",Human-robot interaction,Human-robot interaction,2,conferencePaper,2021 30th IEEE International Conference on Robot &amp; Human Interactive Communication (RO-MAN),2021,KR,Emotional empathy;Social intelligence ,Facial expression,Machine learning,Machine learning,R,ZipSa,Robot,HRI,Machine learning
"""Pretending to Be Okay in a Sad Voice"": Social Robot’s Usage of Verbal and Nonverbal Cue Combination and Its Effect on Human Empathy and Behavior Inducement<sup>*</sup>","Inducing a user’s behavior through Social interaction is a goal that a social robot aims to achieve. It has been argued that empathy has a strong effect on behavior inducement. In human-human interaction, it has been verified that the influence of a nonverbal cue on empathy outweighs that of a verbal cue when those are used in a combined way. The objectives of this study are to explore if such outweighing effect of nonverbal cues is maintained in human-robot interaction (HRI) and to investigate the mechanism of communication cues’ effects by analyzing the mediation structure with the following mediator variables: perceived emotion, perceived intentionality, perceived malfunction, and empathy inducement. To this end, we conducted 2 (verbal type: positive verbal cue vs. negative verbal cue) × 2 (nonverbal type: positive nonverbal cue vs. negative nonverbal cue) within-participant experiment (N = 48). The experiment created a situation in which the social robot was harshly criticized during a conversation. The analysis of experiment results showed the outweighing effect of a nonverbal cue was maintained. When a nonverbal cue conveyed a negative, situation-accordant emotion, it had a decisive effect on perceived emotion, empathy, and behavior inducement. In contrast, a verbal cue induced participants’ empathy and behavior when it conveyed a positive, situation-discordant emotion. This inconsistency between verbal cue and nonverbal cue made the combination of positive verbal cue and negative nonverbal cue have the strongest effect. It implies that participants had different expectations for each of the two communication cues, just as they did in Social interactions with human beings. It implies that participants might have applied normative expectations for Social interaction with human beings to the social robot.",Abuse study,Perception of robot,0,conferencePaper,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),2021,KR,Emotional empathy;Social intelligence,Language; Social behavior,Social Interaction,Interview,h,HuBot,Robot,HRI,Cognitive model
"Humanlikeness and Aesthetic Customization's Effect on Trust, Performance, and Affect","Human-machine interactions have become a staple of people's daily lives through the use of mobile devices, robotics, and a myriad of smart technologies. Previous research has established that anthropomorphism can significantly affect subjective perceptions of, and interactions with, machines. Furthermore, the ability to customize digital tools has been shown to affect user preferences, video game enjoyment, and the efficacy of digital mental health interventions. This study examined whether the customization of a machine teammate could influence the performance of the human-machine team and generate an affective response on the part of the human teammate. To evaluate this premise, we developed a bomb-defusing task simulation using the Unity game engine wherein participants were randomly assigned to one of two (humanlike or machinelike) robot avatars or were given the ability to customize one. The customizable robot avatar allows the participant to select either a humanlike or machinelike robot and customize the color of the wheels and casing. The customization is aesthetic in nature and has no effect on the functionality of the robot. The game design incorporates a high-risk environment and uncertainty with respect to the bomb-defusing distance and required button presses to encourage cautious guidance of the robot. We predicted that the ability to customize the robot will increase performance and subjective measures of trust, affect, attachment, identification, immersion, and control. We also predicted that the humanlikeness of the robot would increase performance and our subjective measures. Finally, we expected to see a significant effect of customization and humanlikeness such that the customization and humanlikeness have an additive effect on performance and our subjective measures. The results of all analyses were nonsignificant. These results may help inform the design of such systems and address fears that customization could lead to over-empathizing with a machine teammate in a way that would reduce use in high-risk environments.",Perception of robot,Perception of robot,0,conferencePaper,2022 Systems and Information Engineering Design Symposium (SIEDS),2022,US,Emotional empathy,Social behavior,Game,Interview,h,Virtual agent,GUI,HRI,GOFAI
"Compassion, empathy and sympathy expression features in affective robotics","The present paper identifies differences in the expression features of compassion, sympathy and empathy in British English and Polish that need to be tuned accordingly in socially interactive robots to enable them to operate successfully in these cultures. The results showed that English compassion is characterised by more positive valence and more of a desire to act than Polish współczucie. Polish empatia is also characterised by a more negative valence than English empathy, which has a wider range of application. When used in positive contexts, English sympathy corresponds to Polish sympatia; however, it also acquires elements of negative valence in English. The results further showed that although the processes of emotion recognition and expression in robotics must be tuned to culture-specific emotion models, the more explicit patterns of responsiveness (British English for the compassion model in our case) is also recommended for the transfer to make the cognitive and sensory infocommunication more readily interpretable by the interacting agents. © 2016 IEEE.",Chatbot,Robot / AI development,17,conferencePaper,"7th IEEE International Conference on Cognitive Infocommunications, CogInfoCom 2016 - Proceedings",2017,PL,Emotional empathy,Language,Game,Cognitive test,h,None,None,NLP,GOFAI
Empathic Robot for Group Learning: A Field Study,"This work explores a group learning scenario with an autonomous empathic robot. We address two research questions: (1) Can an autonomous robot designed with empathic competencies foster collaborative learning in a group context? (2) Can an empathic robot sustain positive educational outcomes in long-term collaborative learning interactions with groups of students? To answer these questions, we developed an autonomous robot with empathic competencies that is able to interact with a group of students in a learning activity about sustainable development. Two studies were conducted. The first study compares learning outcomes in children across three conditions: learning with an empathic robot; learning with a robot without empathic capabilities; and learning without a robot. The results show that the autonomous robot with empathy fosters meaningful discussions about sustainability, which is a learning outcome in sustainability education. The second study features groups of students who interact with the robot in a school classroom for 2 months. The long-term educational interaction did not seem to provide significant learning gains, although there was a change in game-actions to achieve more sustainability during game-play. This result reflects the need to perform more long-term research in the field of educational robots for group learning.",Education,Robot / AI development,28,journalArticle,ACM TRANSACTIONS ON HUMAN-ROBOT INTERACTION,2019,PT;US;EU,Emotional empathy;Cognitive empathy,Social behavior,Social interaction;Game,"Physiological behaviour, Cognitive test",h,NAO Robot,Robot,HRI,Machine learning
An Autonomous Cognitive Empathy Model Responsive to Users' Facial Emotion Expressions,"Successful social robot services depend on how robots can interact with users. The effective service can be obtained through smooth, engaged, and humanoid interactions in which robots react properly to a user's affective state. This article proposes a novel Automatic Cognitive Empathy Model, ACEM, for humanoid robots to achieve longer and more engaged human-robot interactions (HRI) by considering humans' emotions and replying to them appropriately. The proposed model continuously detects the affective states of a user based on facial expressions and generates desired, either parallel or reactive, empathic behaviors that are already adapted to the user's personality. Users' affective states are detected using a stacked autoencoder network that is trained and tested on the RAVDESS dataset. The overall proposed empathic model is verified throughout an experiment, where different emotions are triggered in participants and then empathic behaviors are applied based on proposed hypothesis. The results confirm the effectiveness of the proposed model in terms of related social and friendship concepts that participants perceived during interaction with the robot.",Service,Robot / AI development,2,journalArticle,ACM TRANSACTIONS ON INTERACTIVE IN℡LIGENT SYSTEMS,2020,EU;BE,Emotional empathy;Cognitive empathy;Social intelligence,Facial expression;Language;Embodied cues,Social interaction,Interview,h,Pepper Robot,Robot,HRI,GOFAI;Machine learning
Towards Metrics of Evaluation of Pepper Robot as a Social Companion for the Elderly,"For the design of socially acceptable robots, field studies in Human-Robot Interaction are necessary. Constructing dialogue benchmarks can have a meaning only if researchers take into account the evaluation of robot, human, and their interaction. This paper describes a study aiming at finding an objective evaluation procedure of the dialogue with a social robot. The goal is to build an empathic robot (JOKER project) and it focuses on elderly people, the end-users expected by ROMEO2 project. The authors carried out three experimental sessions. The first time, the robot was NAO, and it was with a Wizard of Oz (emotions were entered manually by experimenters as inputs to the program). The other times, the robot was Pepper, and it was totally autonomous (automatic detection of emotions and decision according to). Each interaction involved various scenarios dealing with emotion recognition, humor, negotiation and cultural quiz. The paper details the system functioning, the scenarios and the evaluation of the experiments.",Companion robot,Robot / AI development,15,conferencePaper,ADVANCED SOCIAL INTERACTION WITH AGENTS,2019,FR;TR;BE,Emotional empathy,Embodied cues; Social behavior,Social Interaction,Interview; Human behaviour,h,NAO and Pepper Robot,Robot,HRI,GOFAI;Machine learning
Human-robot interaction: the impact of robotic aesthetics on anticipated human trust,"Background Human senses have evolved to recognise sensory cues. Beyond our perception, they play an integral role in our emotional processing, learning, and interpretation. They are what help us to sculpt our everyday experiences and can be triggered by aesthetics to form the foundations of our interactions with each other and our surroundings. In terms of Human-Robot Interaction (HRI), robots have the possibility to interact with both people and environments given their senses. They can offer the attributes of human characteristics, which in turn can make the interchange with technology a more appealing and admissible experience. However, for many reasons, people still do not seem to trust and accept robots. Trust is expressed as a persons ability to accept the potential risks associated with participating alongside an entity such as a robot. Whilst trust is an important factor in building relationships with robots, the presence of uncertainties can add an additional dimension to the decision to trust a robot. In order to begin to understand how to build trust with robots and reverse the negative ideology, this paper examines the influences of aesthetic design techniques on the human ability to trust robots. Method This paper explores the potential that robots have unique opportunities to improve their facilities for empathy, emotion, and social awareness beyond their more cognitive functionalities. Through conducting an online Questionnaire distributed globally, we explored participants ability and acceptance in trusting the Canbot U03 robot. Participants were presented with a range of visual questions which manipulated the robots facial screen and asked whether or not they would trust the robot. A selection of questions aimed at putting participants in situations where they were required to establish whether or not to trust a robots responses based solely on the visual appearance. We accomplished this by manipulating different design elements of the robots facial and chest screens, which influenced the human-robot interaction. Results We found that certain facial aesthetics seem to be more trustworthy than others, such as a cartoon face versus a human face, and that certain visual variables (i.e., blur) afforded uncertainty more than others. Consequentially, this paper reports that participants uncertainties of the visualisations greatly influenced their willingness to accept and trust the robot. The results of introducing certain anthropomorphic characteristics emphasised the participants embrace of the uncanny valley theory, where pushing the degree of human likeness introduced a thin line between participants accepting robots and not. By understanding what manipulation of design elements created the aesthetic effect that triggered the affective processes, this paper further enriches our knowledge of how we might design for certain emotions, feelings, and ultimately more socially acceptable and trusting robotic experiences.",Perception of robot,Perception of robot,1,journalArticle,Advances in Computational Learning for Robotics - RiTA 2020,2022,GB,none,Social behavior ,Social Interaction,Interview,h,Canbot U03 robot,Robot,HRI,GOFAI
Development and Evaluation of an Interactive Therapy Robot,"Interactions with animals can enhance emotions and improve mood by engendering feelings of healing, relaxation, comfort, and reduced stress. Un-fortunately, many people cannot live with animals because of allergies, infection risk, or risk of damage to rental housing. To address these problems, some research groups have investigated robot-based psychotherapy. However, the important healing elements for therapy robots were not identified. Therefore, we conducted an Internet survey to determine the design elements of such a robot that might engender a healing mood and the functions that should be implemented. We assumed that a healing mood could be induced based on the interactive functions and appearance. To verify this hypothesis, we developed and evaluated a new interactive therapy robot. Next, we conducted interviews with individuals who interacted with a prototype therapy robot. The interviews revealed that the appearance of the robot was critical to engendering feelings of healing, comfort, and empathy. In addition, the size, softness, and comfort of the interactive therapy robot contributed to people feeling affection towards it. We also confirmed the importance of the robot appearing to listen to those who interacted with it. Our results should be useful for designing companion robots for therapy purposes.",Companion robot,Robot / AI development,4,conferencePaper,"ADVANCES IN COMPUTER ENTERTAINMENT TECHNOLOGY, ACE 2017",2018,JP,Social intelligence;Emotional empathy,Embodied cues,Social interaction,"Interview, Machine learning",h,Teddy bear,Robot,Human health ,GOFAI;Machine learning
Teach your robot how you want it to express emotions: On the personalized affective human-humanoid interaction,"We believe that in order for robots to interact naturally with humans, they should be able to express affective behavior. This paper deals with the development of an affective model for social robotics in which the resulting robotic expressions adapt according to the human subjective preferences. We have developed a method which can be used by non-technical individuals to design the affective models of humanoid robots. Our vision of the future research is that the proposed personalization will be treated, from user’s perspective, as an empathic response of the machine. We see the major contribution of this unique approach especially in long-term human-robot relationships and it could ultimately lead to robots being accepted in a wider domain. © Springer International Publishing Switzerland 2015.",Model,Perception of robot,0,bookChapter,Advances in Intelligent Systems and Computing,2015,SK,Emotional empathy,Embodied cues,Social Interaction,Human behaviour,h; R,NAO Robot,Robot,HRI,GOFAI;Machine learning
Development of a Pupil Response System with Empathy Expression in Face-to-Face Body Contact,"Pupil response is closely related to human affects and emotions. Focusing on the pupil response in human- robot interaction, we developed a pupil response interface using hemisphere displays for enhancing affective expression. This interface can generate pupil response like human by speech input and enhance affective expression. In this study, for the basic research of forming an intimate communication between human and pet-robot, we analyzed the pupil response during his or her body contact stroking forearm or head by using a pupil measurement device. Based on the analysis, we developed an advanced pupil response system for enhancing intimacy. This system generates the empathy expression when the talker touches any surface of hemisphere displays. The effectiveness of the system was confirmed experimentally. © 2020, Springer Nature Switzerland AG.",Embodiment,Robot / AI development,1,conferencePaper,Advances in Intelligent Systems and Computing,2020,JP,Embodiment;Emotional empathy,Language;Embodied cues,Social interaction,Physiological behaviour,h,Robotic Eye,Robot,HRI,GOFAI;Not specified
Vocal Synchrony of Robots Boosts Positive Emotional empathy,"Robots that can talk with humans play increasingly important roles in society. However, current conversation robots remain unskilled at eliciting empathic feelings in humans. To address this problem, we used a robot that speaks in a voice synchronized with human vocal prosody. We conducted an experiment in which human participants held positive conversations with the robot by reading scenarios under conditions with and without vocal synchronization. We assessed seven subjective responses related to Emotional empathy (e.g., emotional connection) and measured the physiological emotional responses using facial electromyography from the corrugator supercilii and zygomatic major muscles as well as the skin conductance level. The subjective ratings consistently revealed heightened empathic responses to the robot in the synchronization condition compared with that under the de-synchronizing condition. The physiological signals showed that more positive and stronger emotional arousal responses to the robot with synchronization. These findings suggest that robots that are able to vocally synchronize with humans can elicit empathic emotional responses.",Chatbot,Robot / AI development,0,journalArticle,Applied Sciences,2021,JP,Emotional empathy,Embodied cues,Social Interaction,Interview; Physiological behaviour,h,NAO Robot,Robot,HRI,GOFAI;Machine learning
Designing the Mind of a Social Robot,"Humans have an innate tendency to anthropomorphize surrounding entities and have always been fascinated by the creation of machines endowed with human-inspired capabilities and traits. In the last few decades, this has become a reality with enormous advances in hardware performance, computer graphics, robotics technology, and artificial intelligence. New interdisciplinary research fields have brought forth cognitive robotics aimed at building a new generation of control systems and providing robots with social, empathetic and affective capabilities. This paper presents the design, implementation, and test of a human-inspired cognitive architecture for social robots. State-of-the-art design approaches and methods are thoroughly analyzed and discussed, cases where the developed system has been successfully used are reported. The tests demonstrated the system's ability to endow a social humanoid robot with human social behaviors and with in-silico robotic emotions.",Model,Perception of robot,21,journalArticle,APPLIED SCIENCES-BASEL,2018,EU,Embodiment,Facial expression; Embodied cues,Social interaction,Interview,h,FACE Robot,Robot,HRI,Cognitive model
Development of an Effective Information Media Using Two Android Robots,"Conversational robots have been used to convey information to people in the real world. Android robots, which have a human-like appearance, are expected to be able to convey not only objective information but also subjective information, such as a robot's feelings. Meanwhile, as an approach to realize attractive conversation, multi-party conversation by multiple robots was the focus of this study. By collaborating among several robots, the robots provide information while maintaining the naturalness of conversation. However, the effectiveness of interaction with people has not been surveyed using this method. In this paper, to develop more efficient media to convey information, we propose a scenario-based, semi-passive conversation system using two androids. To verify its effectiveness, we conducted a subjective experiment comparing it to a system that does not include any interaction with people, and we investigated how much information the proposed system successfully conveys by using a recall test and a Questionnaire about the conversation and androids. The experimental results showed that participants who engaged with the proposed system recalled more content from the conversation and felt more empathic concern for androids.",Chatbot,Robot / AI development,1,journalArticle,APPLIED SCIENCES-BASEL,2019,JP,Cognitive empathy,Language,Social interaction; Machine learning,Interview; Machine learning ,h;R,Geminoids,Robot,HRI,Prerecorded
Emotion-infused deep neural network for emotionally resonant conversation,"The widespread development of conversational agents (chatbots) has enabled us to communicate and collaborate with different forms and functions of robots using natural language, thus facilitating a closer relationship between humans and technology. Given that chatbot services infused with domain knowledge are of great interest to not only global businesses but also academics, chatbots have in recent years become a popular research topic in the field of natural language processing. We therefore aim at improving current chatbots with the addition of natural emotions. In contrast to previous work, we intend to distinguish fine-grained emotion differences between words in order to better understand emotion expressions in sentences. Our approach infuses fine-grained emotion content into the response generation process to make the dialog more emotionally resonant. The experimental results demonstrate that this method can classify emotions more effectively. In addition, the proposed hybrid model, which consists of recurrent and convolutional neural networks with additional emotion-specific valence-arousal features, can correctly identify five emotions with a 67.89% overall F1-score. We further evaluate the subjective quality of the responses and discover that the infusion of fine-grained emotion information substantially improves the quality and fluency of automatically generated empathetic conversation. We conclude that the proposed model can greatly improve the efficiency and usability of a conversational chatbot system.",Chatbot,Robot / AI development,2,journalArticle,Applied Soft Computing,2021,TW,Emotional empathy,Language,Machine learning,Machine learning,R,Text dataset,Text,NLP,Machine learning
Empathetic Robot Evaluation through Emotion Estimation Analysis and Facial Expression Synchronization from Biological Information,"Empathy is an important factor in human communication. For a robot to apply a matching emotion in human–robot communication, the robot needs to be able to understand human feelings. Therefore, in this study, we aimed to improve the human impression of the robot using a robot that expresses human-like expressions by synchronizing with human biological information and changing the expressions in real time. We first measured and estimated human emotions using an emotion estimation method based on biological information (brain waves and heartbeats). The three-emotion estimation methods were proposed and evaluated in the preliminary experiment. Among the three-emotion estimation methods proposed, the one that yields the highest impression rating was chosen to be used in the second experiment which was based on the emotional value in each cycle method. Then, we developed a robot that shows expressions in two patterns: (1) synchronized emotion (same emotion as subject conveyed) and (2) inversed emotion with the human. The subjects evaluated the robot’s expression from both patterns using semantic differential (SD) method while having their biological information measured based on the selected emotion estimation method from previous preliminary experiment. The evaluation by SD method and biological information results showed that when the human experienced the happiness emotion, and the robot synchronized and expressed the same emotion, this could increase the intimacy between human and robot. Here, it can be said that the impression created by the robot’s expression can be improved using biological information.",Embodiment,Robot / AI development,0,journalArticle,Artif. Life Robot.,2021,JP,Social intelligence,Facial expression,Machine learning;Behaviour model,,h,Not mentioned,Visual,HRI,Cognitive model
"Perspective-Taking of Non-Player Characters in Prosocial Virtual Reality Games: Effects on Closeness, Empathy, and Game Immersion","This study explores the effects of the perspective-taking of non-player characters (NPCs) on enhancing game immersion in prosocial virtual reality (VR) games. Prosocial games are games focusing on helping others. Game researchers have been keen to investigate factors that influence the immersive experience in digital games. Previous studies show that VR allows people to take the perspective of others, inducing empathy and prosocial behaviour in the real world. In this lab-based study, we explore whether and how taking the perspective of other game characters - NPCs in a prosocial VR game - influences players' in-game empathy towards NPCs and game immersion. Participants first experienced either a robot's perspective of being destroyed by fire in VR or read a text description about the same event. Then, they participated a prosocial VR game in which they saved robots. The findings show that perspective-taking experiences indirectly enhance participants' game immersion via the effects of closeness with the destroyed robot and empathy towards the four robots protected by the player. This indirect effect is moderated by players' weekly exposure to video games. These results suggest that VR-based perspective-taking of NPCs can indirectly enhance gameplay experiences in prosocial VR games. Theoretical and game design implications are discussed.",VR,Perception of robot,0,journalArticle,BEHAVIOUR & INFORMATION TECHNOLOGY,2020,HK,Emotional empathy; Prosociality,Language,Visual; Game,Interview,h,VR / Avatar,GUI,HRI ,Not specified
Intelligent emotion and behavior based on topological consciousness and adaptive resonance theory in a companion robot,"Companion or `pet' robots can be expected to be an important part of a future in which robots contribute to our lives in many ways. An understanding of emotional interactions would be essential to such robots' behavior. To improve the cognitive and behavior systems of such robots, we propose the use of an artificial topological consciousness that uses a synthetic neurotransmitter and motivation, including a biologically inspired emotion system. A fundamental aspect of a companion robot is a cross communication system that enables natural interactions between humans and the robot. This paper focuses on three points in the development of our proposed framework: (1) the organization of the behavior including inside-state emotion regarding the phylogenetic consciousness-based architecture; (2) a method whereby the robot can have empathy toward its human user's expressions of emotion; and (3) a method that enables the robot to select a facial expression in response to the human user, providing instant human-like `emotion' and based on emotional intelligence (El) that uses a biologically inspired topological online method to express, for example, encouragement or being delighted. We also demonstrate the performance of the artificial consciousness based on the complexity level and a robot's social expressions that are designed to enhance the users affinity with the robot. (C) 2016 Elsevier B.V. All rights reserved.",Companion robot,Robot / AI development,14,journalArticle,BIOLOGICALLY INSPIRED COGNITIVE ARCHITECTURES,2016,JP,Emotional empathy,Facial expression,Machine learning;Behaviour model;Social interaction,Machine learning,R,Conbe Robot,Robot,HRI,Cognitive model;Machine learning
A Preliminary Framework for a Social Robot “Sixth Sense”,"Building a social robot that is able to interact naturally with people is a challenging task that becomes even more ambitious if the robots' interlocutors are children involved in crowded scenarios like a classroom or a museum. In such scenarios, the main concern is enabling the robot to track the subjects' social and affective state modulating its behaviour on the basis of the engagement and the emotional state of its interlocutors. To reach this goal, the robot needs to gather visual and auditory data, but also to acquire physiological signals, which are fundamental for understating the interlocutors' psycho-physiological state. Following this purpose, several Human-Robot Interaction (HRI) frameworks have been proposed in the last years, although most of them have been based on the use of wearable sensors. However, wearable equipments are not the best technology for acquisition in crowded multi-party environments for obvious reasons (e.g., all the subjects should be prepared before the experiment by wearing the acquisition devices). Furthermore, wearable sensors, also if designed to be minimally intrusive, add an extra factor to the HRI scenarios, introducing a bias in the measurements due to psychological stress. In order to overcome this limitations, in this work, we present an unobtrusive method to acquire both visual and physiological signals from multiple subjects involved in HRI. The system is able to integrate acquired data and associate them with unique subjects' IDs. The implemented system has been tested with the FACE humanoid in order to assess integrated devices and algorithms technical features. Preliminary tests demonstrated that the developed system can be used for extending the FACE perception capabilities giving it a sort of sixth sense that will improve the robot empathic and behavioural capabilities.",Embodiment,Robot / AI development,3,conferencePaper,"BIOMIMETIC AND BIOHYBRID SYSTEMS, LIVING MACHINES 2016",2016,EU,Cognitive empathy,Embodied cues,Social interaction; Visual,Machine learning; Physiological behaviour,h,FACE Robot,Robot,HRI,Not specified
Emotion-recognition from speech-based interaction in AAL environment,"In Ambient Assisted Living environments assistance and care are delegated to the intelligence embedded in the environment that, in our opinion, should provide not only a task-oriented support but also an interface able to establish a social empathic relation with the user. To this aim social assistive robots are being employed as a mediator interface and, in order to achieve a relation with the user, they should be endowed with the capability of recognizing the user affective state. Since a natural way to interact with a robot is speech, spoken user's input can be used to give to the robot the capability of recognizing the emotions and attitude of the user, thus providing more detail information about the user state. This paper focuses on this topic and proposes an approach based on the dimensional model of emotions in which the valence and arousal of user's spoken input are recognized. The experimental analysis shows the performance in terms of accuracy of the proposed approach on an Italian dataset. In order to show its application in the context of Ambient Assisted Living, an example is provided. © 2017, CEUR-WS. All rights reserved.",Companion robot,Robot / AI development,1,conferencePaper,CEUR Workshop Proceedings,2017,IT,Emotional empathy,Language,Machine learning,Machine learning,R,Speech Dataset / NAO Robot,Auditory;Robot,HRI,Machine learning
Towards an empathic social robot for ambient assisted living,"In the context of Ambient Assisted Living, assistance and care are delegated to the intelligence embedded in the environment that, in our opinion, should provide not only a task-oriented support but also an interface able to establish a social empathic relation with the user. This can be achieved, for instance, using a social assistive robot as interface towards the environment services. In the context of the NICA (Natural Interaction with a Caring Agent) project we developed the behavioral architecture of a social robot able to assist the user in the interaction with a smart home environment. In this paper we describe how this robot has been endowed with the capability of recognizing the user affective state from the combination of facial expressions and spoken utterances and to reason on in order to simulate an empathic behavior.",Machine learning,Machine learning,7,conferencePaper,CEUR Workshop Proceedings,2015,IT,Emotional empathy,Facial expression; Language,Machine learning,Machine learning,R,NAO Robot,Robot,HRI,Machine learning
Empathizing with Emotional Robot Based on Cognition Reappraisal,"This paper proposes a continuous cognitive emotional regulation model for robot in the case of external emotional stimulus from interactive person's expressions. It integrates a guiding cognitive reappraisal strategy into the HMM (Hidden Markov Model) emotional interactive model for empathizing between robot and person. The emotion is considered as a source in the 3D space (Arousal, Valence, and Stance). State transition and emotion intensity can be quantitatively analyzed in the continuous space. This cognition-emotion interactive model have been verified by the expression and behavior robot. Empathizing is the main distinguishing feature of our work, and it is realized by the emotional regulation which operated in a continuous 3D emotional space enabling a wide range of intermediate emotions. The experiment results provide evidence with acceptability, accuracy, richness, fluency, interestingness, friendliness and exaggeration that the robot with cognition and emotional control ability could be better accepted in the human-robot interaction (HRI).",Machine learning,Machine learning,7,journalArticle,CHINA COMMUNICATIONS,2017,CN,Emotional empathy,Facial expression,Behaviour model; Social interaction,Machine learning; Interview,h; R,Custom,Robot,HRI,GOFAI;Machine learning
Building a Collaborative Relationship between Human and Robot through Verbal and Non-Verbal Interaction,"Interpersonal communication and relationship building promote successful collaborations. This study investigated the effect of conversational nonverbal and verbal interactions of a robot on bonding and relationship building with a human partner.Participants interacted with two robots that differed in their nonverbal and verbal expressiveness. The interactive robot actively engaged the participant in a conversation before, during and after a collaborative task whereas the non-interactive robot remained passive. The robots' nonverbal and verbal interactions increased participants' perception of the robot as a social actor and strengthened bonding and relationship building between human and robot. The results of our study indicate that the evaluation of the collaboration improves when the robot maintains eye contact, the robot is attributed a certain personality, and the robot is perceived as being alive.Our study could not show that an interactive robot receives more help by the collaboration partner. Future research should investigate additional factors that facilitate helpful behavior among humans, such as similarity, attributional judgement and empathy.",Embodiment,Robot / AI development,0,conferencePaper,Companion of the 2021 ACM/IEEE International Conference on Human-Robot Interaction,2021,JP,Emotional empathy;Embodiment;Social intelligence,Embodied cues;Language,Social interaction,"Interview, Physiological behaviour",h,Cozmo Robot,Robot,HRI,GOFAI
Helper's High with a Robot Pet,"Helper's high is the phenomenon that helping someone or something else can lead to psychological benefits such as mood improvement. This study investigates if a robot pet can, like a real pet, induce helpers high in people interacting with it. A Vector robot was programmed to express the need for daily exercise and attention, and participants were instructed how to help the robot meet those needs. Our within subjects design had two conditions: with and without emotional behaviour modifiers to the robot's behaviour. Our primary research question is whether behaviours that conveyed emotion as well as needs would lead to empathy in the participants, which would create a stronger helper's high effect than purely functional need expression behaviours. We present a long-term (4 day) remote study design that not only facilitates the kind of interactions needed for helper's high, but abides by government guidelines on Covid-19 safety (under which a laboratory study is not possible). Preliminary results suggest that Vector was able to improve the mood of some participants, and mood changes tend to be greater when Vector expressed behaviours with emotional components. Our post-study interview data suggests that individual differences in living environment and mood impacting external factors, affected Vector's efficacy in mood influencing.",Perception of robot,Perception of robot,0,conferencePaper,Companion of the 2021 ACM/IEEE International Conference on Human-Robot Interaction,2021,UK,Emotional empathy,Language,Social interaction,"Interview, Interview",h,Vector Robot,Robot,HRI,Not specified
FECTS: A Facial Emotion Cognition and Training System for Chinese Children with Autism Spectrum Disorder.,"Traditional training methods such as card teaching, assistive technologies (e.g., augmented reality/virtual reality games and smartphone apps), DVDs, human-computer interactions, and human-robot interactions are widely applied in autistic rehabilitation training in recent years. In this article, we propose a novel framework for human-computer/robot interaction and introduce a preliminary intervention study for improving the emotion recognition of Chinese children with an autism spectrum disorder. The core of the framework is the Facial Emotion Cognition and Training System (FECTS, including six tasks to train children with ASD to match, infer, and imitate the facial expressions of happiness, sadness, fear, and anger) based on Simon Baron-Cohen's E-S (empathizing-systemizing) theory. Our system may be implemented on PCs, smartphones, mobile devices such as PADs, and robots. The training record (e.g., a tracked record of emotion imitation) of the Chinese autistic children interacting with the device implemented using our FECTS will be uploaded and stored in the database of a cloud-based evaluation system. Therapists and parents can access the analysis of the emotion learning progress of these autistic children using the cloud-based evaluation system. Deep-learning algorithms of facial expressions recognition and attention analysis will be deployed in the back end (e.g., devices such as a PC, a robotic system, or a cloud system) implementing our FECTS, which can perform real-time tracking of the imitation quality and attention of the autistic children during the expression imitation phase. In this preliminary clinical study, a total of 10 Chinese autistic children aged 3-8 are recruited, and each of them received a single 20-minute training session every day for four consecutive days. Our preliminary results validated the feasibility of the developed FECTS and the effectiveness of our algorithms based on Chinese children with an autism spectrum disorder. To verify that our FECTS can be further adapted to children from other countries, children with different cultural/sociological/linguistic contexts should be recruited in future studies.",Education,Robot / AI development,0,journalArticle,Comput Intell Neurosci Computational intelligence and neuroscience,2022,CN,Emotional empathy; Embodiment,Embodied cues; Facial expression,Social Interaction,Machine learning; Physiological behaviour,h,Can be employed on any system,None,HRI,Machine learning
"The Design and Implementation of XiaoIce, an Empathetic Social Chatbot","This article describes the development of Microsoft XiaoIce, the most popular social chatbot in the world. XiaoIce is uniquely designed as an artifical intelligence companion with an emotional connection to satisfy the human need for communication, affection, and social belonging. We take into account both intelligent quotient and emotional quotient in system design, cast human-machine social chat as decision-making over Markov Decision Processes, and optimize XiaoIce for long-term user engagement, measured in expected Conversation-turns Per Session (CPS). We detail the system architecture and key components, including dialogue manager, core chat, skills, and an empathetic computing module. We show how XiaoIce dynamically recognizes human feelings and states, understands user intent, and responds to user needs throughout long conversations. Since the release in 2014, XiaoIce has communicated with over 660 million active users and succeeded in establishing long-term relationships with many of them. Analysis of large-scale online logs shows that XiaoIce has achieved an average CPS of 23, which is significantly higher than that of other chatbots and even human conversations.",Chatbot,Robot / AI development,189,journalArticle,COMPUTATIONAL LINGUISTICS,2020,CN,Cognitive empathy,Language,Machine learning,Machine learning,R,Virtual Agent,GUI,Chatbot,Machine learning
Towards Empathetic Human-Robot Interactions,"Since the late 1990s when speech companies began providing their customer-service software in the market, people have gotten used to speaking to machines. As people interact more often with voice and gesture controlled machines, they expect the machines to recognize different emotions, and understand other high level communication features such as humor, sarcasm and intention. In order to make such communication possible, the machines need an empathy module in them, which is a software system that can extract emotions from human speech and behavior and can decide the correct response of the robot. Although research on empathetic robots is still in the primary stage, current methods involve using signal processing techniques, sentiment analysis and Machine learning algorithms to make robots that can `understand' human emotion. Other aspects of human-robot interaction include facial expression and gesture recognition, as well as robot movement to convey emotion and intent. We propose Zara the Supergirl as a prototype system of empathetic robots. It is a software-based virtual android, with an animated cartoon character to present itself on the screen. She will get `smarter' and more empathetic, by having Machine learning algorithms, and gathering more data and learning from it. In this paper, we present our work so far in the areas of deep learning of emotion and sentiment recognition, as well as humor recognition. We hope to explore the future direction of android development and how it can help improve people's lives.",Service,Robot / AI development,39,conferencePaper,"COMPUTATIONAL LINGUISTICS AND IN℡LIGENT TEXT PROCESSING, (CICLING 2016), PT II",2018,HK,Emotional empathy;Social intelligence,Language,Machine learning;Behaviour model,"Physiological behaviour, Machine learning ",R,Avatar,GUI,HRI,Machine learning
"Sympathy for the digital: Influence of synthetic voice on affinity, social presence and empathy for photorealistic virtual humans","In this paper, we investigate the effect of a realism mismatch in the voice and appearance of a photorealistic virtual character in both immersive and screen-mediated virtual contexts. While many studies have investigated voice attributes for robots, not much is known about the effect voice naturalness has on the perception of realistic virtual characters. We conducted the first experiment in Virtual Reality (VR) with over two hundred participants investigating the mismatch between realistic appearance and unrealistic voice on the feeling of presence, and the emotional response of the user to the character expressing a strong negative emotion. We predicted that the mismatched voice would lower social presence and cause users to have a negative emotional reaction and feelings of discomfort towards the character. We found that the concern for the virtual character was indeed altered by the unnatural voice, though interestingly it did not affect social presence. The second experiment was conducted with a view towards heightening the appearance realism of the same character for the same scenarios, with an additional lower level of voice realism employed to strengthen the mismatch of perceptual cues. While voice type did not appear to impact reports of empathic responses towards the character, there was an observed effect of voice realism on reported social presence, which was not detected in the first study. There were also significant results on affinity and voice trait measurements that provide evidence in support of perceptual mismatch theories of the Uncanny Valley.",VR,Perception of robot,2,journalArticle,Computers & Graphics,2022,IE,Emotional empathy;Social intelligence,Language,Game,Interview,h,Virtual agent ,GUI,VR,Not specified
"What makes an ai device human-like? the role of interaction quality, empathy and perceived psychological anthropomorphic characteristics on the acceptance of artificial intelligence in the service industry","intelligent ai devices have become a common presence in the business landscape, offering a wide range of services, from the medical sector to the hospitality industry. from an organizational perspective, ai devices have several advantages, by performing certain tasks quicker and more accurately in comparison to humans while at the same time being more cost-efficient. however, in order to maintain the high standards of a brand, they have to be accepted by consumers and deliver socially adequate performance. therefore, it is important to determine the characteristics of ai devices which make them accepted and trusted by consumers. based on the computers as social actors (casa) theory, we have researched on the role of psychological anthropomorphic characteristics, perceived empathy, and interaction quality in the acceptance of ai devices in the service industry. the results show that anthropomorphic characteristics alone do not influence acceptance and trust towards ai devices. however, both perceived empathy and interaction quality mediate the relation between anthropomorphic characteristics and acceptance. a human-like ai device has higher acceptance when it has the ability to show empathy and interaction in relation to the human consumer. this result reveals the importance of developing forms of strong intelligence and empathetic behaviour in service robots and ai devices.",Service,Robot / AI development,111,journalArticle,Computers in Human Behavior,2021,RO,Emotional empathy;Cognitive empathy,Language,Behaviour model,Interview,h,Images,Visual,HRI,Cognitive model
Socially grounded game strategy enhances bonding and perceived smartness of a humanoid robot,"In search for better technological solutions for education, we adapted a principle from economic game theory, namely that giving a help will promote collaboration and eventually long-term relations between a robot and a child. This principle has been shown to be effective in games between humans and between humans and computer agents. We compared the social and cognitive engagement of children when playing checkers game combined with a social strategy against a robot or against a computer. We found that by combining the social and game strategy the children (average age of 8.3 years) had more empathy and social engagement with the robot since the children did not want to necessarily win against it. This finding is promising for using social strategies for the creation of long-term relations between robots and children and making educational tasks more engaging. An additional outcome of the study was the significant difference in the perception of the children about the difficulty of the game - the game with the robot was seen as more challenging and the robot - as a smarter opponent. This finding might be due to the higher perceived or expected intelligence from the robot, or because of the higher complexity of seeing patterns in three-dimensional world.",Education,Robot / AI development,11,journalArticle,CONNECTION SCIENCE,2018,NL,Social intelligence;Cognitive empathy,Social behavior,Game,Physiological behaviour; Interview,h,NAO Robot,Robot,HRI,GOFAI
Evaluation of an algorithm for optical pulse detection in children for application to the Pepper robot,"To engage in socio-emotional interactions, children with autism spectrum conditions (ASC) need support to understand and convey emotions. In our approach, a humanoid robot (Pepper, Softbanks Robotics) acts as a tutor for the child within autism care. The robot, equipped with multimodal sensor technology to acquire the emotional feedback of the child, stimulates the child to perform tasks, adapted to its current arousal state. By in-, or decreasing the difficulties of implemented training modules, the child can be given the appropriate task according to its emotional state. The childs arousal is measured with different techniques implemented in and on the robot: emotion detection based on audio recordings of the speech signal and camera detected facial expressions, or heart rate. To this end, the remote Photoplethysmography (rPPG) signal from camera recordings of the subjects face is acquired. While its unintrusive measurement is an advantage, a major drawback for rPPG is its proneness to motion and light artefacts requiring de-noising steps. A wavelet transform based on log-Gabor wavelets and a filter bank with 32 filters was implemented. The signal was filtered with a prior filter and afterwards with a Markov chain in order to extract the underlying pulse rate. Within an initial study, five children were observed watching videos with different co-notated emotions. As reference for the heart rate (HR), a wristband (empatica E4) was used. The captured emotions of all subjects were annotated to identify low and high arousal parts and positive and negative emotions. Extracted HR from rPPG-data indicated a correlation with the annotated emotions.",Education,Robot / AI development,0,journalArticle,Current Directions in Biomedical Engineering 2021,2021,DE,Embodiment,Facial expression; Emobied Cues,Social Interaction,Machine learning,h,Pepper robot,Robot,HRI,Machine learning
Application of autoencoders in cyber-empathic design,"A critical task in product design is mapping information from the consumer space to the design space. This process is largely dependent on the designer to identify and relate psychological and consumer level factors to engineered product attributes. In this way, current methodologies lack provision to test a designer's cognitive reasoning and may introduce bias through the mapping process. Prior work on Cyber-Empathic Design (CED) supports this mapping by relating user-product interaction data from embedded sensors to psychological constructs. To understand consumer perceptions, a network of psychological constructs is developed using Structural Equation Modeling for parameter estimation and hypothesis testing, making the framework falsifiable in nature. The focus of this technical brief is toward automating CED through unsupervised deep learning to extract features from raw data. Additionally, Partial Least Square Structural Equation Modeling is used with extracted sensor features as inputs. To demonstrate the effectiveness of the approach a case study involving sensor-integrated shoes compares three models - a survey-only model (no sensor data), the existing CED approach with manually extracted sensor features, and the proposed deep learning based CED approach. The deep learning based approach results in improved model fit.",Machine learning,Machine learning,2,journalArticle,DESIGN SCIENCE,2018,US,Embodiment,Embodied cues,Behaviour model,Machine learning,h,Time-series dataset,Physiology; Human Behaviour ,HRI,Machine learning
Expression of Emotions by a Service Robot: A Pilot Study,"A successful Human-Robot Interaction (HRI) depends on the empathy that the robot has the capability of instantiating on the user, namely through the expression of emotions. In this pilot study, we examined the recognition of emotions being expressed by a service robot in a virtual environment (VE), by university students. The VE was a corridor, neutral in terms of context of use. The robot's facial expressions, body movements, and displacement were manipulated to express eight basic emotions. Results showed that participants had difficulties in recognizing the emotions (33% of success). Also, results suggested that the participants established empathy with the robot. Further work is needed to improve the emotional expression of this robot, which aims to interact with hospitalized children.",Service,Robot / AI development,7,conferencePaper,"DESIGN, USER EXPERIENCE, AND USABILITY: TECHNOLOGICAL CONTEXTS, PT III",2016,IT,Emotional empathy;Imitation,Facial expression;Embodied cues,Behaviour model,Interview,h,Virtual Reality,Visual,HRI,Not specified
Emotion Recognition Using Facial Expression Images for a Robotic Companion,"Social robots are gradually becoming part of society. However, social robots lack the ability to adequately interact with users in a natural manner and are in need of more human-like abilities. In this paper we present experimental results on emotion recognition through the use of facial expression images obtained from the KDEF database, a fundamental first step towards the development of an empathic social robot. We compare the performance of Support Vector Machines (SVM) and a Multilayer Perceptron Network (MLP) on facial expression classification. We employ Gabor filters as an image pre-processing step before classification. Our SVM model achieves an accuracy rate of 97.08 %, whereas our MLP achieves 93.5%. These experiments serve as benchmark for our current research project in the area of social robotics.",Image emotion classification,Machine learning,6,conferencePaper,"ENGINEERING APPLICATIONS OF NEURAL NETWORKS, EANN 2016",2016,UK,Cognitive empathy,Facial expression,Machine learning; Visual,Machine learning,R,Image Dataset,Visual,HRI ; Computer Vision,Machine learning
Real-time convolutional neural networks for emotion and gender classification,"Emotion and gender recognition from facial features are important properties of human empathy. Robots should also have these capabilities. For this purpose we have designed special convolutional modules that allow a model to recognize emotions and gender with a considerable lower number of parameters, enabling real-time evaluation on a constrained platform. We report accuracies of 96% in the IMDB gender dataset and 66% in the FER-2013 emotion dataset, while requiring a computation time of less than 0.008 seconds on a Core i7 CPU. All our code, demos and pre-trained architectures have been released under an open-source license in our repository at https://github.com/oarriaga/face classification. © 2019 ESANN (i6doc.com). All rights reserved.",Image emotion classification,Machine learning,161,conferencePaper,"ESANN 2019 - Proceedings, 27th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning",2019,DE,Emotional empathy,Facial expression,Machine learning,Machine learning,R,Images,Visual,Computer Vision,Machine learning
Examining young children's perception toward augmented reality-infused dramatic play,"Amid the increasing interest in applying augmented reality (AR) in educational settings, this study explores the design and enactment of an AR-infused robot system to enhance children's satisfaction and sensory engagement with dramatic play activities. In particular, we conducted an exploratory study to empirically examine children's perceptions toward the computer- and robot-mediated AR systems designed to make dramatic play activities interactive and participatory. A multi-disciplinary expert group consisting of early childhood education experts, preschool teachers, AR specialists, and robot engineers collaborated to develop a learning scenario and technological systems for dramatic play. The experiment was conducted in a kindergarten setting in Korea, with 81 children (aged 5-6 years old). The participants were placed either in the computer-mediated AR condition (n = 40) or the robot-mediated AR condition (n = 41). We administered an instrument to measure children's perceived levels of the following variables: (a) satisfaction (i.e., interest in dramatic play & user-friendliness), (b) sensory immersion (i.e., self-engagement, environment-engagement & interaction-engagement), and (c) media recognition (i.e., collaboration with media, media function & empathy with media). Data analysis indicates that children in the robot-mediated condition showed significantly higher perceptions than those in the computer-mediated condition regarding the following aspects: interest in dramatic play (satisfaction), interactive engagement (sensory immersion), and empathy with media (media recognition). Furthermore, it was found that the younger-aged children and girls, in particular, perceived AR-infused dramatic play more positively than the older-aged children and boys, respectively. The contribution of this study is to provide empirical evidence about the affordances of robots and AR-based learning systems for young children. This remains a relatively unexplored area of research in the field of learning technologies. Implications of the current study and future research directions are also discussed.",Education,Robot / AI development,120,journalArticle,ETR&D-EDUCATIONAL TECHNOLOGY RESEARCH AND DEVELOPMENT,2015,KR,Social intelligence,Language,Visual,Interview,h,iROBIQ Robot,Robot,HRI ; Human relations,GOFAI
Implementation and Evaluation of a Grip Behavior Model to Express Emotions for an Android Robot.,"In this study, we implemented a model with which a robot expressed such complex emotions as heartwarming (e.g., happy and sad) or horror (fear and surprise) by its touches and experimentally investigated the effectiveness of the modeled touch behaviors. Robots that can express emotions through touching behaviors increase their interaction capabilities with humans. Although past studies achieved ways to express emotions through a robot's touch, such studies focused on expressing such basic emotions as happiness and sadness and downplayed these complex emotions. Such studies only proposed a model that expresses these emotions by touch behaviors without evaluations. Therefore, we conducted the experiment to evaluate the model with participants. In the experiment, they evaluated the perceived emotions and empathies from a robot's touch while they watched a video stimulus with the robot. Our results showed that the touch timing before the climax received higher evaluations than touch timing after for both the scary and heartwarming videos.",Embodiment,Robot / AI development,0,journalArticle,Front Robot AI Frontiers in robotics and AI,2021,JP,Embodiement,Embodied cues,Social Interaction,Interview,h,android robot,Robot,HRI,GOFAI
"Weight Shift Movements of a Social Mediator Robot Make It Being Recognized as Serious and Suppress Anger, Revenge and Avoidance Motivation of the User.","Humans can become aggressive during text messaging. To maintain a healthy interpersonal relationship through text messaging, our negative mental states, such as anger, have to be well-controlled. This paper discusses the use of a handheld social robot deployed as a mediator in text messaging between humans. The robot is equipped with a movable weight inside its body. By controlling the movement of the internal weight during the time when the robot speaks out messages received from a human sender, we hypothesize that the psychological state of a receiver who holds the robot can be affected (for example, he/she will listen to the messages more seriously). In a controlled study (n = 94), in which participants were manipulated to be frustrated by using a context scenario, we studied the effect of three dialogue scripts with/without weight shifts. Results showed that introducing weight shifts together with the robot speech suppressed on average 23% of the user's anger. However, only 3.5% of the anger was suppressed when the weight shifts were not applied. Additionally, in cases where the robot showed empathy to the user in words with weight shifts, the user's revenge urge was successfully reduced by 22%. There was almost no effect confirmed when the weight shifts were not applied. A similar effect was also found in avoidance motivation: 15% of the avoidance motivation was reduced if weight shifts were applied. The reductions in revenge and avoidance motivation are considered important factors for human forgiveness. Therefore, our findings provide experimental evidence that weight shifts can be an effective expression modality for mediator robots, from the perspective of not only suppressing the user's anger but also by inducing forgiveness during messaging.",Embodiment,Robot / AI development,1,journalArticle,Front Robot AI Frontiers in robotics and AI,2022,JP,Emotional empathy,"Embodied cues, Language",Social interaction,Interview,h,OMOY,Robot,HRI,GOFAI
Can a humanoid face be expressive? A psychophysiological investigation,"Non-verbal signals expressed through body language play a crucial role in multi-modal human communication during social relations. Indeed, in all cultures, facial expressions are the most universal and direct signs to express innate emotional cues. A human face conveys important information in Social interactions and helps us to better understand our social partners and establish empathic links. Latest researches show that humanoid and social robots are becoming increasingly similar to humans, both esthetically and expressively. However, their visual expressiveness is a crucial issue that must be improved to make these robots more realistic and intuitively perceivable by humans as not different from them. This study concerns the capability of a humanoid robot to exhibit emotions through facial expressions. More specifically, emotional signs performed by a humanoid robot have been compared with corresponding human facial expressions in terms of recognition rate and response time. The set of stimuli included standardized human expressions taken from an Ekman-based database and the same facial expressions performed by the robot. Furthermore, participants' psychophysiological responses have been explored to investigate whether there could be differences induced by interpreting robot or human emotional stimuli. Preliminary results show a trend to better recognize expressions performed by the robot than 2D photos or 3D models. Moreover, no significant differences in the subjects' psychophysiological state have been found during the discrimination of facial expressions performed by the robot in comparison with the same task performed with 2D photos and 3D models.",Embodiment,Robot / AI development,28,journalArticle,FRONTIERS IN BIOENGINEERING AND BIOTECHNOLOGY,2015,EU,Emotional empathy,Facial expression; Embodied cues,Behaviour model,Physiological behaviour,h,FACE Robot,Robot,HRI,Not specified
Emotional Empathy as a Mechanism of Synchronisation in Child-Robot Interaction,"Simulating emotional experience, Emotional empathy is the fundamental ingredient of interpersonal communication. In the speaker-listener scenario, the speaker is always a child, the listener is a human or a toy robot. Two groups of neurotypical children aged 6 years on average composed the population: one Japanese (n = 20) and one French (n = 20). Revealing potential similarities in communicative exchanges in both groups when in contact with a human or a toy robot, the results might signify that Emotional empathy requires the implication of an automatic identification. In this sense, Emotional empathy might be considered a broad idiosyncrasy, a kind of synchronisation, offering the mind a peculiar form of communication. Our findings seem to be consistent with the assumption that children's brains would be constructed to simulate the feelings of others in order to ensure interpersonal synchronisation.",Human-robot interaction,Human-robot interaction,8,journalArticle,FRONTIERS IN PSYCHOLOGY,2018,FR;JP,Emotional empathy,Embodied cues; Language,Social interaction,"Physiological behaviour, Interview",h,Pekoppa Robot,Robot,HRI,GOFAI;Not specified
"You Look Human, But Act Like a Machine: Agent Appearance and Behavior Modulate Different Aspects of Human-Robot Interaction","Gaze following occurs automatically in Social interactions, but the degree to which gaze is followed depends on whether an agent is perceived to have a mind, making its behavior socially more relevant for the interaction. Mind perception also modulates the attitudes we have toward others, and determines the degree of empathy, Prosociality, and morality invested in Social interactions. Seeing mind in others is not exclusive to human agents, but mind can also be ascribed to non-human agents like robots, as long as their appearance and/or behavior allows them to be perceived as intentional beings. Previous studies have shown that human appearance and reliable behavior induce mind perception to robot agents, and positively affect attitudes and performance in human-robot interaction. What has not been investigated so far is whether different triggers of mind perception have an independent or interactive effect on attitudes and performance in human-robot interaction. We examine this question by manipulating agent appearance (human vs. robot) and behavior (reliable vs. random) within the same paradigm and examine how congruent (human/reliable vs. robot/random) versus incongruent (human/random vs. robot/reliable) combinations of these triggers affect performance (i.e., gaze following) and attitudes (i.e., agent ratings) in human-robot interaction. The results show that both appearance and behavior affect human-robot interaction but that the two triggers seem to operate in isolation, with appearance more strongly impacting attitudes, and behavior more strongly affecting performance. The implications of these findings for human-robot interaction are discussed.",Embodiment,Robot / AI development,49,journalArticle,FRONTIERS IN PSYCHOLOGY,2017,US,Emotional empathy;Embodiment;Imitation,Embodied cues,Social interaction,Physiological behaviour,h,Images,Visual,HRI,Not specified
“Hit the Robot on the Head With This Mallet” - Making a Case for Including More Open Questions in HRI Research,"Researchers continue to devise creative ways to explore the extent to which people perceive robots as social agents, as opposed to objects. One such approach involves asking participants to inflict `harm' on a robot. Researchers are interested in the length of time between the experimenter issuing the instruction and the participant complying, and propose that relatively long periods of hesitation might reflect empathy for the robot, and perhaps even attribution of human-like qualities, such as agency and sentience. In a recent experiment, we adapted the so-called `hesitance to hit' paradigm, in which participants were instructed to hit a humanoid robot on the head with a mallet. After standing up to do so (signaling intent to hit the robot), participants were stopped, and then took part in a semi-structured interview to probe their thoughts and feelings during the period of hesitation. Thematic analysis of the responses indicate that hesitation not only reflects perceived socialness, but also other factors including (but not limited to) concerns about cost, mallet disbelief, processing of the task instruction, and the influence of authority. The open-ended, free responses participants provided also offer rich insights into individual differences with regards to anthropomorphism, perceived power imbalances, and feelings of connection toward the robot. In addition to aiding understanding of this measurement technique and related topics regarding socialness attribution to robots, we argue that greater use of open questions can lead to exciting new research questions and interdisciplinary collaborations in the domain of social robotics.",Abuse study,Perception of robot,0,journalArticle,FRONTIERS IN ROBOTICS AND AI,2021,EU;UK,Emotional empathy,Social behavior,Social interaction,Interview,h,Pepper Robot,Robot,HRI,GOFAI
Development and Testing of Psychological Conflict Resolution Strategies for Assertive Robots to Resolve Human-Robot Goal Conflict,"As service robots become increasingly autonomous and follow their own task-related goals, human-robot conflicts seem inevitable, especially in shared spaces. Goal conflicts can arise from simple trajectory planning to complex task prioritization. For successful human-robot goal-conflict resolution, humans and robots need to negotiate their goals and priorities. For this, the robot might be equipped with effective conflict resolution strategies to be assertive and effective but similarly accepted by the user. In this paper, conflict resolution strategies for service robots (public cleaning robot, home assistant robot) are developed by transferring psychological concepts (e.g., negotiation, cooperation) to HRI. Altogether, fifteen strategies were grouped by the expected affective outcome (positive, neutral, negative). In two online experiments, the acceptability of and compliance with these conflict resolution strategies were tested with humanoid and mechanic robots in two application contexts (public: n(1) = 61; private: n(2) = 93). To obtain a comparative value, the strategies were also applied by a human. As additional outcomes trust, fear, arousal, and valence, as well as perceived politeness of the agent were assessed. The positive/neutral strategies were found to be more acceptable and effective than negative strategies. Some negative strategies (i.e., threat, command) even led to reactance and fear. Some strategies were only positively evaluated and effective for certain agents (human or robot) or only acceptable in one of the two application contexts (i.e., approach, empathy). Influences on strategy acceptance and compliance in the public context could be found: acceptance was predicted by politeness and trust. Compliance was predicted by interpersonal power. Taken together, psychological conflict resolution strategies can be applied in HRI to enhance robot task effectiveness. If applied robot-specifically and context-sensitively they are accepted by the user. The contribution of this paper is twofold: conflict resolution strategies based on Human Factors and Social Psychology are introduced and empirically evaluated in two online studies for two application contexts. Influencing factors and requirements for the acceptance and effectiveness of robot assertiveness are discussed.",Service,Robot / AI development,0,journalArticle,FRONTIERS IN ROBOTICS AND AI,2021,DE,Emotional empathy;Social intelligence,Language;Embodied cues,Behaviour model,Interview,h,Videos,Visual,HRI,Not specified
Investigation of Methods to Create Future Multimodal Emotional Data for Robot Interactions in Patients with Schizophrenia: A Case Study.,"Rapid progress in humanoid robot investigations offers possibilities for improving the competencies of people with social disorders, although this improvement of humanoid robots remains unexplored for schizophrenic people. Methods for creating future multimodal emotional data for robot interactions were studied in this case study of a 40-year-old male patient with disorganized schizophrenia without comorbidities. The qualitative data included heart rate variability (HRV), video-audio recordings, and field notes. HRV, Haar cascade classifier (HCC), and Empath APIï¿½ï¿½ were evaluated during conversations between the patient and robot. Two expert nurses and one psychiatrist evaluated facial expressions. The research hypothesis questioned whether HRV, HCC, and Empath APIï¿½ï¿½ are useful for creating future multimodal emotional data about robot-patient interactions. The HRV analysis showed persistent sympathetic dominance, matching the human-robot conversational situation. The result of HCC was in agreement with that of human observation, in the case of rough consensus. In the case of observed results disagreed upon by experts, the HCC result was also different. However, emotional assessments by experts using Empath APIï¿½ï¿½ were also found to be inconsistent. We believe that with further investigation, a clearer identification of methods for multimodal emotional data for robot interactions can be achieved for patients with schizophrenia.",Health,Robot / AI development,0,journalArticle,"Healthcare (Basel) Healthcare (Basel, Switzerland)",2022,JP;US,Social intelligence,Facial expression; Emobied Cues; Language,Social Interaction,Machine learning,h,Pepper robot,Robot,HRI,Machine learning
Group-based Emotions in Teams of Humans and Robots,"Providing social robots an internal model of emotions can help them guide their behaviour in a more humane manner by simulating the ability to feel empathy towards others. Furthermore, the growing interest in creating robots that are capable of collaborating with other humans in team settings provides an opportunity to explore another side of human emotion, namely, group-based emotions. This paper contributes with the first model on group-based emotions in social robotic partners. We defined a model of group-based emotions for social robots that allowed us to create two distinct robotic characters that express either individual or group-based emotions. This paper also contributes with a user study where two autonomous robots embedded the previous characters, and formed two human-robot teams to play a competitive game. Our results showed that participants perceived the robot that expresses group-based emotions as more likeable and attributed higher levels of group identification and group trust towards their teams, when compared to the robotic partner that expresses individual-based emotions.",Perception of robot,Perception of robot,36,conferencePaper,HRI `18: PROCEEDINGS OF THE 2018 ACM/IEEE INTERNATIONAL CONFERENCE ON HUMAN-ROBOT INTERACTION,2018,PT,Emotional empathy,Social behavior,Game,Interview,h,EMYS robot,Robot,HRI,Not specified
Inducing Bystander Interventions During Robot Abuse with Social Mechanisms,"We explored whether a robot can leverage social influences to motivate nearby bystanders to intervene and defend them from human abuse. We designed a between-subjects study where 48 participants took part in a memorization task and observed a confederate mistreating a robot both verbally and physically. The robot was either empathetic towards the participant's performance in the task or indifferent. When the robot was mistreated, it ignored the abuse, shut down in response to it, or reacted emotionally. We found that the majority of the participants intervened to help the robot after it was abused. Interventions happened for a wide range of reasons. Interestingly, the empathetic robot increased the proportion of participants that self-reported intervening in comparison to the indifferent robot, but more participants moved the robot as a response to abuse in the latter case. The participants also perceived the robot being verbally mistreated more and reported higher levels of personal distress when the robot briefly shut down after abuse in comparison to when it reacted emotionally or did not react at all.",Abuse study,Perception of robot,27,conferencePaper,HRI `18: PROCEEDINGS OF THE 2018 ACM/IEEE INTERNATIONAL CONFERENCE ON HUMAN-ROBOT INTERACTION,2018,US,Emotional empathy,Social behavior,Social Interaction,Interview; Human behaviour,h,Cozmo Robot,Robot,HRI,GOFAI
A Computation Model for Learning Programming and Emotional Intelligence,"Introducing coding in early education improves the logical and computational thinking in kids. However, cognitive skills are not sufficient for a successful life. Understanding and managing the emotions of oneself is another crucial factor in success. The current state of the art teaching methods educates the kids about programming and emotional intelligence independently. In our opinion, it is advantageous to teach kids emotional intelligence, along with the programming concepts. However, the literature lacks the studies that make students emotionally aware while teaching them programming. This research aims to prepare students to be cognitively healthy as well as emotionally intelligent with the hypothesis that a kid's emotional intelligence can be enhanced while teaching them cognitive skills. We proposed a computational model that teaches programming and emotional intelligence side by side to students. The model provides a curriculum and related tools. For evaluations, five hundred students of a public school were involved in different activities to find the effectiveness of the proposed model. These students were divided into five groups (A, B, C, D, and E), each having a mean age of 4, 5, 6, 7, and 8 years, respectively. Students performed multiple adaptive scenarios of path-finding that were based on self-awareness, social-awareness, sharing, and empathy emotions. Students provide the programming instructions such as sequencing, conditional statements, and looping to a robot. The children have successfully improved in both fundamental programming constructs and emotional intelligence skills. The research also successfully reduced screen time problem by providing a screen-free student interface.",Education,Robot / AI development,0,journalArticle,IEEE ACCESS,2020,PK,Emotional empathy;Cognitive empathy;Social intelligence,Social behavior,Social interaction,Cognitive test; Interview,h,Cozmo Robot,Robot,HRI ; Human relations,Not specified
Multi-modality Sentiment Analysis in Social Internet of Things based on Hierarchical Attentions and CSATTCN with MBM Network,"Multi-modality sentiment analysis in the social internet of things is a developing field, which is basic to empathetic mechanisms, affective computing, and artificial intelligence. Current works in this domain do not explicitly consider the influence of contextual information fusion based on correlation coefficient and memory network with branch structure for sentiment analysis. Unlike present works, this paper presents a Hierarchical Self-attention Fusion (H-SATF) model for capturing contextual information better among utterances, a Contextual Self-attention Temporal Convolutional Network (CSAT-TCN) for the sentiment recognition in social internet of things, and a Multi Branches Memory (MBM) network that stores self-speaker and inter-speaker sentimental states into global memories. For the MOSI datasets, the hybrid H-SATF-CSAT-TCN-MBM model outperforms the state-of-art networks and shows 0.31 9.93% improvement. IEEE",Machine learning,Machine learning,9,journalArticle,IEEE Internet of Things Journal,2020,CN,Cognitive empathy,Language,Machine learning,Machine learning,R,None / Video datasets,Visual,Machine Learning,Machine learning
Recognition and expression of emotions by a symbiotic android head,"The creation of social empathie communication channels between social robots and humans has started to become reality. Nowadays, the development of empathie and affective agents is giving to scientists another way to explore the social dimension of human beings. In this work, we introduce the FACE humanoid project that aims at creating a social and emotional android. FACE is an android head with an articulated neck mounted on a passive body. In order to enable FACE to perceive and express emotions, two dedicated engines have been developed. A sensory apparatus able to perceive the 'social world', and a facial expressions generation engine that allows the robot to express its synthetic emotions. The system has been also integrated with an attention-based gaze generation component that allows the robot to autonomously follow a conversation between its partners. The developed framework has been implemented and tested in several standard human-robot interaction settings. Results demonstrated the promising social capabilities of the robot to perceive and convey emotions to humans through the generation of emotional perceivable facial expressions and socially aligned behaviour. © 2014 IEEE.",Embodiment,Robot / AI development,6,conferencePaper,IEEE-RAS International Conference on Humanoid Robots,2015,ES,Imitation,Facial expression,Machine learning,Machine learning,R,FACE Robot,Robot,HRI,Cognitive model
Consciousnes-based emotion and behavior of pet robot with brain-inspired method,"A personal robot becomes important to the future world where the robot facilitates our lives and be a friend. The understanding of emotional interaction is essential in the social behavior, including a natural behavior that is the needed functions for creature behavior-like robots. Our paper proposes the artificial topological consciousness based on a pet robot using a synthetic neurotransmitter and motivation including intelligent emotion. Since the significant factor of a companionable robot is the cross-communication system without conflict. This paper then focuses on three points: The first is the organization of the behavior and emotion model regarding the phylogenetic. The second, the method of the robot that can have empathy with user expression. The third, how the robot can perform the expression to the human with emotional intelligence us-ing a biologically inspired topological on-line method for encouragement or being delighted. We additionally demonstrate the performance of the artificial consciousness based on complexity level and the robot social expression to enhance the users affinity with the experiment. © 2017 International Information Institute.",Companion robot,Robot / AI development,0,journalArticle,Information (Japan),2017,JP,Emotional empathy;Social intelligence,Embodied cues;Facial expression,Machine learning;Behaviour model,"Machine learning, Physiological behaviour",R,Conbe Robot,Robot,HRI,Cognitive model;Machine learning
An Architecture for Telenoid Robot as Empathic Conversational Android Companion for Elderly People,"In Human-Humanoid Interaction (HHI), empathy is the crucial key in order to overcome the current limitations of social robots. In facts, a principal defining characteristic of human social behaviour is empathy. The present paper presents a robotic architecture for an android robot as a basis for natural empathic human-android interaction. We start from the hypothesis that the robots, in order to become personal companions need to know how to empathic interact with human beings. To validate our research, we have used the proposed system with the minimalistic humanoid robot Telenoid. We have conducted human-robot interactions test with elderly people with no prior interaction experience with robot. During the experiment, elderly persons engaged a stimulated conversation with the humanoid robot. Our goal is to overcome the state of loneliness of elderly people using this minimalistic humanoid robot capable to exhibit a dialogue similar to what usually happens in real life between human beings. The experimental results have shown a humanoid robotic system capable to exhibit a natural and empathic interaction and conversation with a human user.",Companion robot,Robot / AI development,9,conferencePaper,IN℡LIGENT AUTONOMOUS SYSTEMS 13,2016,IT;JP,Cognitive empathy,Language,Social interaction,Machine learning,h,Telenoid,Robot,HRI,GOFAI;Machine learning
Evaluation of Classifiers for Emotion Detection While Performing Physical and Visual Tasks: Tower of Hanoi and IAPS,"With the advancement in robot technology, smart human-robot interaction is of increasing importance for allowing the more excellent use of robots integrated into human environments and activities. If a robot can identify emotions and intentions of a human interacting with it, interactions with humans can potentially become more natural and effective. However, mechanisms of perception and empathy used by humans to achieve this understanding may not be suitable or adequate for use within robots. Electroencephalography (EEG) can be used for recording signals revealing emotions and motivations from a human brain. This study aimed to evaluate different Machine learning techniques to classify EEG data associated with specific affective/emotional states. For experimental purposes, we used visual (IAPS) and physical (Tower of Hanoi) tasks to record human emotional states in the form of EEG data. The obtained EEG data processed, formatted and evaluated using various Machine learning techniques to find out which method can most accurately classify EEG data according to associated affective/emotional states. The experiment confirms the choice of a method for improving the accuracy of results. According to the results, Support Vector Machine was the first, and Regression Tree was the second best method for classifying EEG data associated with specific affective/emotional states with accuracies up to 70.00% and 60.00%, respectively. In both tasks, SVM was better in performance than RT.",Machine learning,Machine learning,0,conferencePaper,"IN℡LIGENT SYSTEMS AND APPLICATIONS, VOL 1",2019,TH;SE;SA;PK;AU,Emotional empathy,Embodied cues,Machine learning,"Physiological behaviour, Machine learning","h, R",None,None,HRI,Machine learning
Do We Need Emotionally Intelligent Artificial Agents? First Results of Human Perceptions of Emotional Intelligence in Humans Compared to Robots,"Humans are very apt at reading emotional signals in other humans and even artificial agents, which raises the question of whether artificial agents need to be emotionally intelligent to ensure effective Social interactions. For artificial agents without emotional intelligence might generate behavior that is misinterpreted, unexpected, and confusing to humans, violating human expectations and possibly causing emotional harm. Surprisingly, there is a dearth of investigations aimed at understanding the extent to which artificial agents need emotional intelligence for successful interactions. Here, we present the first study in the perception of emotional intelligence (EI) in robots vs. humans. The objective was to determine whether people viewed robots as more or less emotionally intelligent when exhibiting similar behaviors as humans, and to investigate which verbal and nonverbal communication methods were most crucial for human observational judgments. Study participants were shown a scene in which either a robot or a human behaved with either high or low empathy, and then they were asked to evaluate the agent's emotional intelligence and trustworthiness. The results showed that participants could consistently distinguish the high EI condition from the low EI condition regardless of the variations in which communication methods were observed, and that whether the agent was a robot or human had no effect on the perception. We also found that relative to low EI high EI conditions led to greater trust in the agent, which implies that we must design robots to be emotionally intelligent if we wish for users to trust them.",Perception of robot,Perception of robot,18,conferencePaper,"IN℡LIGENT VIRTUAL AGENTS, IVA 2017",2017,US,Cognitive empathy,Embodied cues,Social interaction;Behaviour model,Interview,h,None / Videos of interactions with the PR2 robot,Visual,HRI,Not specified
More than advice The influence of adding references to prior discourse and signals of empathy on the persuasiveness of an advice-giving robot,"Persuasive social robots can influence human behavior through giving advice. The current study investigates whether references to prior discourse and signals of empathy make an advice-giving robot an even more effective persuader and whether participants follow the robot's advice and drink even more water when the robot additionally uses these strategies. We recruited students and university staff for a lab-study in which three different robot personalities on the same robot type presented health-related information. In one condition, the robot gave advice and referred to something mentioned earlier in the conversation (i.e., to dialog history), in another condition, the robot gave advice and used empathic signals, and in the third condition, the robot gave advice only. Our results show that participants drank significantly more when the advice-giving robot also used the persuasive strategies of empathy and references to dialog history than when the robot only gave advice. This study shows that both strategies increase the persuasiveness of the robot and makes it more influential.",Robot / AI development,Robot / AI development,5,journalArticle,Interaction Studies,2021,DK;NL,Embodiment,Embodiment,Social Interaction,Physiological behaviour,h,Socibot Mini,Robot,HRI,GOFAI
Impact of human-robot interaction on user satisfaction with humanoid-based healthcare,"Background/Objectives: The advent of self-service technology (SST) (e.g.,kiosks and Automatic Response System), has made it possible for service providersto make use of non-face-to-face channels to meet users'needs and decrease users'costs and time. On the other hand, however, more complex technology and/or services inhibit users' satisfaction and,consequently,the intention to adopt SST, because such SST can instill fear in users. Nevertheless, at present, patients and other people who are interested in their own health and well-being are paying great attention to healthcare robots (as a form of SST)and,consequently, it has become crucial to investigate how these healthcare robots can positively influence users' satisfaction with them. Hence, this study aims to empirically investigate the factors that affect users' satisfaction with healthcare robots, especially in regard to human-robot interaction (HRI). Methods/Statistical analysis: We focused on the theory of heterophily and applied a series of factors identified in previous robot-adoption studies.Uniquely, this study focuses on users' heterophily with healthcare robots, examining heterophily through three fundamental ele-ments, empathy, professionalism, and personality, which we considered to be suitable fordetermining user satisfaction with HRI-based communication.To prove the validity of our hypotheses, we conducted an empirical testthat involved participants receiving a short health assessment from a robot. Findings: The findings of our empirical test supported our hypothesis that the lower the difference in empathy between a user and robot, the higher the level of user satisfaction with the humanoid-style healthcare service. Further, our results also suggest that heterogeneity between a user and healthcare robot is positively associated with user satisfaction. Improvements/Applications: First, to increase user satisfaction,robots must be provided with the ability to somehow recognizea user's personality and adjust their own accordingly before beginning the robot-based healthcare service. Secondly, users' behavior patterns should be analyzed by the healthcare robot. Overall, our study empirically shows the importance of ensuring thatprofessionalism is present in healthcare-domain-related HRI. © 2018 Ohbyung Kwon at.al.",Health,Robot / AI development,0,journalArticle,International Journal of Engineering and Technology(UAE),2018,KR,Emotional empathy,Language,Social interaction,Interview,h,NAO Robot,Robot,HRI ; Human health,GOFAI
Proactivity or passivity? An investigation of the effect of service robots’ proactive behaviour on customer co-creation intention,"The implementation of robots in tourism services is transforming how companies interact with their customers and create value. Although corporate initiatives are beginning to focus on proactive robot services, little is known about the impact of this behavioural characteristic of service robots on customer value co-creation. Through two experiments and one field survey, this research demonstrates that customers’ co-creation intention is higher when a robot’s service proactivity is high (vs. low), and that this relationship is mediated by perceived robotic empathy. Furthermore, customer task orientation negatively moderates the mediating effect of empathy perception that underlies the relationship between service proactivity and customers’ co-creation intention. These findings provide new insights into the effect of service robots’ behavioural characteristics on customer–robot interactions.",Service,Robot / AI development,,journalArticle,International Journal of Hospitality Management,2022,CN,Emotional empathy;Social intelligence,Social behavior,Social Interaction,Interview,h,Text dataset,Text,HRI ; Customer Service,Cognitive model
Emotional Storytelling Using Virtual and Robotic Agents,"In order to create effective storytelling agents three fundamental questions must be answered: first, is a physically embodied agent preferable to a virtual agent or a voice-only narration? Second, does a human voice have an advantage over a synthesised voice? Third, how should the emotional trajectory of the different characters in a story be related to a storyteller's facial expressions during storytelling time, and how does this correlate with the apparent emotions on the faces of the listeners? The results of two specially designed studies indicate that the physically embodied robot produces more attention to the listener as compared to a virtual Embodiment, that a human voice is preferable over the current state of the art of text-to-speech, and that there is a complex yet interesting relation between the emotion lines of the story, the facial expressions of the narrating agent, and the emotions of the listener, and that the empathising of the listener is evident through its facial expressions. This work constitutes an important step towards emotional storytelling robots that can observe their listeners and adapt their style in order to maximise their effectiveness.",Embodiment,Robot / AI development,28,journalArticle,INTERNATIONAL JOURNAL OF HUMANOID ROBOTICS,2018,PT;KR,Emotional empathy,Facial expression; Language,Behaviour model,Machine learning; Interview,h,Virtual Agent / Ibn Sina Robot,GUI,HRI,GOFAI;Prerecorded
Seeing the mind of robots: Harm augments mind perception but benevolent intentions reduce dehumanisation of artificial entities in visual vignettes,"According to moral typecasting theory, good- and evil-doers (agents) interact with the recipients of their actions (patients) in a moral dyad. When this dyad is completed, mind attribution towards intentionally harmed liminal minds is enhanced. However, from a dehumanisation view, malevolent actions may instead result in a denial of humanness. To contrast both accounts, a visual vignette experiment (N = 253) depicted either malevolent or benevolent intentions towards robotic or human avatars. Additionally, we examined the role of harm-salience by showing patients as either harmed, or still unharmed. The results revealed significantly increased mind attribution towards visibly harmed patients, mediated by perceived pain and expressed empathy. Benevolent and malevolent intentions were evaluated respectively as morally right or wrong, but their impact on the patient was diminished for the robotic avatar. Contrary to dehumanisation predictions, our manipulation of intentions failed to affect mind perception. Nonetheless, benevolent intentions reduced dehumanisation of the patients. Moreover, when pain and empathy were statistically controlled, the effect of intentions on mind perception was mediated by dehumanisation. These findings suggest that perceived intentions might only be indirectly tied to mind perception, and that their role may be better understood when additionally accounting for empathy and dehumanisation.",Abuse study,Perception of robot,5,journalArticle,INTERNATIONAL JOURNAL OF PSYCHOLOGY,2021,DE;PL,Emotional empathy,Social behavior,,Interview,h,Images,Visual,HRI,Not specified
Deep learning-based facial expression recognition and analysis for filipino gamers,"This paper presents a computer vision based emotion recognition system for the identification of six basic emotions among Filipino Gamers using deep learning techniques. In particular, the proposed system utilized deep learning through the Inception Network and Long-Short Term Memory (LSTM). The researchers gathered a database for Filipino Facial Expressions consisting of 74 gamers for the training data and 4 gamer subjects for the testing data. The system was able to produce a maximum categorical validation accuracy of.9983 and a test accuracy of.9940 for the six basic emotions using the Filipino database. The cross-database analysis results using the well-known Cohn-Kanade+ database showed that the proposed Inception-LSTM system has accuracy on a par with the current existing systems. The results demonstrated the feasibility of the proposed system and showed sample computations of empathy and engagement based on the six basic emotions as a proof of concept. © BEIESP.",Image emotion classification,Machine learning,0,journalArticle,International Journal of Recent Technology and Engineering,2019,PH,Emotional empathy,facial expression,Visual,Machine learning,h,Videos,Visual,Computer Vision,Machine learning
"A Recipe for Empathy Integrating the Mirror System, Insula, Somatosensory Cortex and Motherese","Could a robot feel authentic empathy? What exactly is empathy, and why do most humans have it? We present a model which suggests that empathy is an emergent behavior with four main elements: a mirror neuron system, somatosensory cortices, an insula, and infant-directed “baby talk” or motherese. To test our hypothesis, we implemented a robot called MEI (multimodal emotional intelligence) with these functions, and allowed it to interact with human caregivers using comfort and approval motherese, the first kinds of vocalizations heard by infants at 3 and 6 months of age. The robot synchronized in real-time to the humans through voice and movement dynamics, while training statistical models associated with its low level gut feeling (”flourishing” or “distress”, based on battery or temperature). Experiments show that the post-interaction robot associates novel happy voices with physical flourishing 90 % of the time, sad voices with distress 84 % of the time. Our results also show that a robot trained with infant-directed “attention bids” can recognize adult fear voices. Importantly, this is the first emotion system to recognize adult emotional voices after training only with motherese, suggesting that this specific parental behavior may help build emotional intelligence.",Mirroring,Perception of robot,27,journalArticle,INTERNATIONAL JOURNAL OF SOCIAL ROBOTICS,2015,JP,cognitive empathy,Language,Social interaction,Machine learning,R,NAO Robot,Robot,Machine learning,GOFAI;Machine learning
A Reinforcement Learning Based Cognitive Empathy Framework for Social Robots,"Robots that express human's social norms, like empathy, are perceived as more friendly, understanding, and caring. However, appropriate human-like empathic behaviors cannot be defined in advance, instead, they must be learned through daily interaction with humans in different situations. Additionally, to learn and apply the correct behaviors, robots must be able to perceive and understand the affective states of humans. This study presents a framework to enable cognitive empathy in social robots, which uses facial emotion recognition to perceive and understand the affective states of human users. The perceived affective state is then provided to a reinforcement learning model to enable a robot to learn the most appropriate empathic behaviors for different states. The proposed framework has been evaluated through an experiment between 28 individual humans and the humanoid robot Pepper. The results show that by applying empathic behaviors selected by the employed learning model, the robot is able to provide participants comfort and confidence and help them enjoy and feel better.",Machine learning,Machine learning,0,journalArticle,INTERNATIONAL JOURNAL OF SOCIAL ROBOTICS,2020,BE,Cognitive empathy,Language;Facial expression,Machine learning;Social interaction,Interview,h,Pepper Robot,Robot,HRI,Machine learning
Can You Read My Face? A Methodological Variation for Assessing Facial Expressions of Robotic Heads,"Our paper reports about an online study on robot facial expressions. On the one hand, we performed this study to assess the quality of the current facial expressions of two robot heads. On the other hand, we aimed at developing a simple, easy-to-use methodological variation to evaluate facial expressions of robotic heads. Short movie clips of two different robot heads showing a happy, sad, surprised, and neutral facial expression were compiled into an online survey, to examine how people interpret these expressions. Additionally, we added a control condition with a human face showing the same four emotions. The results showed that the facial expressions could be recognized well for both heads. Even the blender emotion surprised was recognized, although it resulted in positive and negative connotations. These results underline the importance of the situational context to correctly interpret emotional facial expressions. Besides the expected finding that the human is perceived significantly more anthropomorphic and animate than both robot heads, the more human-like designed robot head was rated significantly higher with respect to anthropomorphism than the robot head using animal-like features. In terms of the validation procedure, we could provide evidence for a feasible two-step procedure. By assessing the participants' dispositional empathy with a Questionnaire it can be ensured that they are in general able to decode facial expressions into the corresponding emotion. In subsequence, robot facial expressions can be validated with a closed-question approach.",Embodiment,Robot / AI development,13,journalArticle,INTERNATIONAL JOURNAL OF SOCIAL ROBOTICS,2015,EU,Emotional empathy,Facial expression,Visual,Interview,R,Videos EDDIE & IURO,Visual,HRI,GOFAI
Empathetic Speech Synthesis and Testing for Healthcare Robots,"One of the major factors that affect the acceptance of robots in Human-Robot Interaction applications is the type of voice with which they interact with humans. The robot's voice can be used to express empathy, which is an affective response of the robot to the human user. In this study, the aim is to find out if social robots with empathetic voice are acceptable for users in healthcare applications. A pilot study using an empathetic voice spoken by a voice actor was conducted. Only prosody in speech is used to express empathy here, without any visual cues. Also, the emotions needed for an empathetic voice are identified. It was found that the emotions needed are not only the stronger primary emotions, but also the nuanced secondary emotions. These emotions are then synthesised using prosody modelling. A second study, replicating the pilot test is conducted using the synthesised voices to investigate if empathy is perceived from the synthetic voice as well. This paper reports the modelling and synthesises of an empathetic voice, and experimentally shows that people prefer empathetic voice for healthcare robots. The results can be further used to develop empathetic social robots, that can improve people's acceptance of social robots.",Embodiment,Robot / AI development,0,journalArticle,INTERNATIONAL JOURNAL OF SOCIAL ROBOTICS,2020,NZ,Emotional empathy,Language,Visual,Interview,R,iROBIQ Robot,Robot,HRI ; Human health,GOFAI
"Multimodal Integration of Emotional Signals from Voice, Body, and Context: Effects of (In)Congruence on Emotion Recognition and Attitudes Towards Robots","Humanoid social robots have an increasingly prominent place in today's world. Their acceptance in social and emotional human-robot interaction (HRI) scenarios depends on their ability to convey well recognized and believable emotional expressions to their human users. In this article, we incorporate recent findings from psychology, neuroscience, human-computer interaction, and HRI, to examine how people recognize and respond to emotions displayed by the body and voice of humanoid robots, with a particular emphasis on the effects of incongruence. In a social HRI laboratory experiment, we investigated contextual incongruence (i.e., the conflict situation where a robot's reaction is incongrous with the socio-emotional context of the interaction) and cross-modal incongruence (i.e., the conflict situation where an observer receives incongruous emotional information across the auditory (vocal prosody) and visual (whole-body expressions) modalities). Results showed that both contextual incongruence and cross-modal incongruence confused observers and decreased the likelihood that they accurately recognized the emotional expressions of the robot. This, in turn, gives the impression that the robot is unintelligent or unable to express “empathic” behaviour and leads to profoundly harmful effects on likability and believability. Our findings reinforce the need of proper design of emotional expressions for robots that use several channels to communicate their emotional states in a clear and effective way. We offer recommendations regarding design choices and discuss future research areas in the direction of multimodal HRI.",Mirroring,Perception of robot,27,journalArticle,INTERNATIONAL JOURNAL OF SOCIAL ROBOTICS,2019,CH,Emotional empathy,Embodied cues; Language,Social interaction,Interview,R,Pepper Robot,Robot,HRI,Prerecorded
Testing Empathy with Robots: A Model in Four Dimensions and Sixteen Items,"The four-dimensional model of empathy presented in this paper addresses human-human, human-avatar and human-robot interaction, and aims at better understanding the specificities of the empathy that humans might develop towards robots. Its first dimension is auto-empathy and refers to an empathetic relationship with oneself: how can a human directing a robot expand the various components of empathy he feels for himself to this robot? The second is direct empathy: what does a human attribute to a robot in terms of thoughts, emotions, action potentials or even altruism, on the model of what he imagines and attributes to himself? The third dimension is reciprocal empathy that consists of thinking that a robot is able to identify with me, feel or guess my emotions and thoughts, anticipate my actions and wear me assistance if necessary. Finally, the fourth dimension, intersubjective empathy, is about thinking and imagining that a robot can inform me of things - emotions, thoughts that I am likely to experience- that I do not know about myself. Each of these four dimensions includes four different components: (1) Action (empathy of action), (2) Emotion (Emotional empathy), (3) Cognition (cognitive empathy) and (4) Assistance (empathy of assistance). This theoretical model of empathy in four dimensions and four components defines sixteen items whose relevance will be tested in the near future through comparative experimental research involving human-human and human-robot interaction.",Model,Perception of robot,12,journalArticle,INTERNATIONAL JOURNAL OF SOCIAL ROBOTICS,2015,FR,Emotional empathy; Cognitive empathy,Social behavior,Social Interaction,Interview,h,Questionaire,Human behaviour,HRI ; Human health,Not specified
Empathy and Schadenfreude in Human-Robot Teams.,"Intergroup dynamics shape the ways in which we interact with other people. We feel more empathy towards ingroup members compared to outgroup members, and can even feel pleasure when an outgroup member experiences misfortune, known as schadenfreude. Here, we test the extent to which these intergroup biases emerge during interactions with robots. We measured trial-by-trial fluctuations in emotional reactivity to the outcome of a competitive reaction time game to assess both empathy and schadenfreude in arbitrary human-human and human-robot teams. Across four experiments (total n = 361), we observed a consistent empathy and schadenfreude bias driven by team membership. People felt more empathy towards ingroup members than outgroup members and more schadenfreude towards outgroup members. The existence of an intergroup bias did not depend on the nature of the agent: the same effects were observed for human-human and human-robot teams. People reported similar levels of empathy and schadenfreude towards a human and robot player. The human likeness of the robot did not consistently influence this intergroup bias. In other words, similar empathy and schadenfreude biases were observed for both humanoid and mechanoid robots. For all teams, this bias was influenced by the level of team identification; individuals who identified more with their team showed stronger intergroup empathy and schadenfreude bias. Together, we show that similar intergroup dynamics that shape our interactions with people can also shape interactions with robots. Our results highlight the importance of taking intergroup biases into account when examining social dynamics of human-robot interactions.",Abuse study,Perception of robot,4,journalArticle,J Cogn Journal of cognition,2021,EU,Emotional empathy; Prosociality,Social behavior,Game,Interview,h,video,Visual,HRI,Cognitive model
A pupil response system using hemispherical displays for enhancing affective conveyance,"In human interaction and communication, not only verbal messages but also nonverbal behaviors such as facial expressions, body movements, gazes and pupil responses play an important role in expressions of talker's affect. These expressions encourage to read the emotional cues and to cause the sharing of Embodiment and empathy. We focused on the pupil response which is closely related to human affect, and developed an embodied communication system in which an interactive CG character generates the pupil response as well as communicative actions and movements such as nodding and body movements by speech input. In addition, it was confirmed that the pupil response is effective for supporting the embodied interaction and communication using the developed system. In this paper, in order to realize the smooth interaction between human and robot, we developed a pupil response system using hemispherical displays for enhancing affective conveyance. This system looks like robot's eyeballs and expresses vivid pupil response by speech input. We carried out a sensory evaluation experiment under the condition that the developed system speaks. The results demonstrated that the system effectively enhances affective conveyance.",Embodiment,Robot / AI development,2,journalArticle,JOURNAL OF ADVANCED MECHANICAL DESIGN SYSTEMS AND MANUFACTURING,2019,JP,Emotional empathy,Embodied cues,Social Interaction,Interview,h,Robotic Eye,Robot,HRI,GOFAI;Not specified
Experiences of a Motivational Interview Delivered by a Robot: Qualitative Study,"Background: Motivational interviewing is an effective intervention for supporting behavior change but traditionally depends on face-to-face dialogue with a human counselor. This study addressed a key challenge for the goal of developing social robotic motivational interviewers: creating an interview protocol, within the constraints of current artificial intelligence, which participants will find engaging and helpful. Objective: The aim of this study was to explore participants' qualitative experiences of a motivational interview delivered by a social robot, including their evaluation of usability of the robot during the interaction and its impact on their motivation. Methods: NAO robots are humanoid, child-sized social robots. We programmed a NAO robot with Choregraphe software to deliver a scripted motivational interview focused on increasing physical activity. The interview was designed to be comprehensible even without an empathetic response from the robot. Robot breathing and face-tracking functions were used to give an impression of attentiveness. A total of 20 participants took part in the robot-delivered motivational interview and evaluated it after 1 week by responding to a series of written open-ended questions. Each participant was left alone to speak aloud with the robot, advancing through a series of questions by tapping the robot's head sensor. Evaluations were content-analyzed utilizing Boyatzis' steps: (1) sampling and design, (2) developing themes and codes, and (3) validating and applying the codes. Results: Themes focused on interaction with the robot, motivation, change in physical activity, and overall evaluation of the intervention. Participants found the instructions clear and the navigation easy to use. Most enjoyed the interaction but also found it was restricted by the lack of individualized response from the robot. Many positively appraised the nonjudgmental aspect of the interview and how it gave space to articulate their motivation for change. Some participants felt that the intervention increased their physical activity levels. Conclusions: Social robots can achieve a fundamental objective of motivational interviewing, encouraging participants to articulate their goals and dilemmas aloud. Because they are perceived as nonjudgmental, robots may have advantages over more humanoid avatars for delivering virtual support for behavioral change.",Therapy,Robot / AI development,23,journalArticle,JOURNAL OF MEDICAL INTERNET RESEARCH,2018,PT,Embodiment,Embodied cues; Language,Social interaction,Interview,h,NAO Robot,Robot,HRI,GOFAI
The Effect of Robot Attentional Behaviors on User Perceptions and Behaviors in a Simulated Health Care Interaction: Randomized Controlled Trial,"Background: For robots to be effectively used in health applications, they need to display appropriate social behaviors. A fundamental requirement in all Social interactions is the ability to engage, maintain, and demonstrate attention. Attentional behaviors include leaning forward, self-disclosure, and changes in voice pitch. Objective: This study aimed to examine the effect of robot attentional behaviors on user perceptions and behaviors in a simulated health care interaction. Methods: A parallel randomized controlled trial with a 1:1:1 allocation ration was conducted. We randomized participants to 1 of 4 experimental conditions before engaging in a scripted face-to-face interaction with a fully automated medical receptionist robot. Experimental conditions included a self-disclosure condition, voice pitch change condition, forward lean condition, and neutral condition. Participants completed paper-based postinteraction measures relating to engagement, perceived robot attention, and perceived robot empathy. We video recorded interactions and coded for participant attentional behaviors. Results: A total of 181 participants were recruited from the University of Auckland. Participants who interacted with the robot in the forward lean and self-disclosure conditions found the robot to be significantly more stimulating than those who interacted with the robot in the voice pitch or neutral conditions (P=.03). Participants in the forward lean, self-disclosure, and neutral conditions found the robot to be significantly more interesting than those in the voice pitch condition (P<.001). Participants in the forward lean and self-disclosure conditions spent significantly more time looking at the robot than participants in the neutral condition (P<.001). Significantly, more participants in the self-disclosure condition laughed during the interaction (P=.01), whereas significantly more participants in the forward lean condition leant toward the robot during the interaction (P<.001). Conclusions: The use of self-disclosure and forward lean by a health care robot can increase human engagement and attentional behaviors. Voice pitch changes did not increase attention or engagement. The small effects with regard to participant perceptions are potentially because of the limitations in self-report measures or a lack of comparison for most participants who had never interacted with a robot before. Further research could explore the use of self-disclosure and forward lean using a within-subjects design and in real health care settings.",Embodiment,Robot / AI development,8,journalArticle,JOURNAL OF MEDICAL INTERNET RESEARCH,2019,KR,Social intelligence; Emotional empathy,Embodied cues; Social behavior,Social interaction,Interview; Human behaviour,h,NAO Robot,Robot,HRI ; Human health,GOFAI
Embodiment matters: toward culture-specific robotized counselling,"Abstract: In this paper, we propose adding the traditional Japanese nodding behavior to the repertoire of social movements to be used in the context of human–robot interaction. Our approach is motivated by the notion that in many cultures, trust-building can be boosted by small body gestures. We discuss the integration of a robot capable of such movements within CRECA, our context-respectful counseling agent. The frequent nodding called “unazuki” in Japan, often accompanying the “un-un” sound (meaning “I agree”) of Japanese onomatopoeia, underlines empathy and embodies unconditioned approval. We argue that “unazuki” creates more empathy and promotes longer conversation between the robotic counsellor and people. We set up an experiment involving ten subjects to verify these effects. Our quantitative evaluation is based on the classic metrics of utterance, adapted to support the Japanese language. Interactions featuring “unazuki” showed higher value of this metrics. Moreover, subjects assessed the counselling robot’s trustworthiness and kindness as “very high” (Likert scale: 5.5 versus 3—4.5) showing the effect of social gestures in promoting empathetic dialogue to general people including the younger generation. Our findings support the importance of social movements when using robotized agents as a therapeutic tool aimed at improving emotional state and Social interactions, with unambiguous evidence that Embodiment can have a positive impact that warrants further exploration. The 3D printable design of our robot supports creating culture-specific libraries of social movements, adapting the gestural repertoire to different human cultures.",Mirroring,Perception of robot,0,journalArticle,Journal of Reliable Intelligent Environments,2020,JP,Emotional empathy,Embodied cues,Social Interaction,Interview,h,Custom Robot,Robot,HRI ; Human health,GOFAI;Machine learning
Gesture design attribute and level value of social robot: A user experience based study,"This study was to verify the attributes of the social robot's gesture design factors that has a significant difference in the user experience and to establish the level values of the attributes. To do so, the attributes and the level value standards for the gesture interface's key design factors have been organized and a user experience survey was conducted through researches on the existing literature and case studies. For the emotional gesture attributes, the level values were categorized as 'pleasure at low arousal', 'pleasure at high arousal', 'displeasure at low arousal', and 'displeasure at high arousal'. Among the communicative expression gesture attributes, the level values were categorized as ‘idling, conversation induction and concentration, and empathy’. Lastly, the derived attributes and the level values for the ‘emotional gesture’ and ‘communicative gesture’ have been integrated with the ones for the ‘functional/semantic gesture' derived on the previous studies; they have been presented as the robot's gesture interface design factors available in the aspect of the user experience. © 2020, Success Culture Press. All rights reserved.",Embodiment,Robot / AI development,,journalArticle,Journal of System and Management Sciences,2020,KR,Emotional empathy;Cognitive empathy;Social intelligence,Embodied cues,Social interaction,Interview,h,Images,Visual,HRI,Not specified
Emotion Synchronization Method for Robot Facial Expression,"Nowadays, communication robots are becoming popular since they are actively used in both commercially and personally. Increasing empathy between human-robot can effectively enhance the positive impression. Empathy can be created by syncing human emotion with the robot expression. Emotion estimation can be done by analyzing controllable expressions like facial expression, or uncontrollable expression like biological signals. In this work, we propose the comparison of robot expression synchronization with estimated emotion based on either facial expression or biological signal. In order to find out which of the proposed methods yield the best impression, subjective impression rating is used in the experiment. From the result of the impression evaluation, we found that the robot’s facial expression synchronization using the synchronization based on periodical emotion value performs the best and best suitable for emotion estimated both from facial expression and biological signal. © 2020, Springer Nature Switzerland AG.",Mirroring,Perception of robot,0,conferencePaper,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2020,JP,Imitation;Emotional empathy,Facial expression;Embodied cues,Behaviour model,"Interview, Human behaviour, Physiological behaviour",h,ROMAN,Robot,HRI,GOFAI
Expression of Grounded Affect: How Much Emotion Can Arousal Convey?,"In this paper we consider how non-humanoid robots can communicate their affective state via bodily forms of communication (kinesics), and the extent to which this influences how humans respond to them. We propose a simple model of grounded affect and kinesic expression before presenting the qualitative findings of an exploratory study (N = 9), during which participants were interviewed after watching expressive and non-expressive hexapod robots perform different ‘scenes’. A summary of these interviews is presented and a number of emerging themes are identified and discussed. Whilst our findings suggest that the expressive robot did not evoke significantly greater empathy or altruistic intent in humans than the control robot, the expressive robot stimulated greater desire for interaction and was also more likely to be attributed with emotion. © 2020, Springer Nature Switzerland AG.",Model,Perception of robot,0,conferencePaper,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2020,UK,Emotional empathy,Embodied cues,Behaviour model,Interview,h,Hexapod Robot,Robot,HRI,GOFAI
User Experience Study: The Service Expectation of Hotel Guests to the Utilization of AI-Based Service Robot in Full-Service Hotels,"With the dramatic development of AI technology, the concept of robotic hotel is entering the public’s awareness. Although AI application brings in high efficiency, low labor cost and novelty, practical operation of robotic hotels still faces with challenges. This quantitative research aims at understanding the current user expectation level of AI robotic hotel and robot appliance. Based on that, it tries to make the user classification by demographic, behavioral and attitude factors. By using the refined SERVQUAL model, it gathers the expectation from five dimensions involving tangibles, reliability, responsiveness, assurance and empathy. These research objectives were realized by using survey-designed Questionnaires and distributed by a snowball sampling method conducted in Beijing. After validity and reliability test, data collected from the field were analyzed by a variety of inspections. It is found that education, attitude and income level have a significant effect on the expectation to stay in the robotic hotel, which provided the basis of market position for robotic hotel operators. Through regression analysis, the model was established to identify what factors played an important part and how they worked. It is found that tangibles and responsiveness expectation significantly and positively contributed to increases in general user expectation to robotic hotels. This thesis drew up several conclusions, which would help industry players including hoteliers, AI robot suppliers better understand details of the user group in their decision-making process, as well as academic side to formulate a tailored model to evaluate the interaction between AI robots and hotel guests. © 2019, Springer Nature Switzerland AG.",Service,Robot / AI development,5,conferencePaper,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2019,CN,Emotional empathy,Social behavior,Social interaction,Interview,h,Robotic Hotel / Smart Environment,Robot,Customer Service,Not specified
"Degrees of Empathy: Humans’ Empathy Toward Humans, Animals, Robots and Objects","The aim of this paper is to present an experiment in which we compare the degree of empathy that a convenience sample of students expressed with humans, animals, robots and objects. The present study broadens the spectrum of the elements eliciting empathy that previous research has so far explored separately. Our research questions are: does the continuum represented by this set of elements elicit empathy? Is it possible to observe a linear decrease of empathy according to different features of the selected elements? More broadly, does empathy, as a construct, resist in front of the diversification of the element eliciting it? Results show that participants expressed empathy differently when exposed to three clusters of social actors being mistreated: they felt more sad, sorry, aroused and out of control for animals than for humans, but showed little to no empathy for objects. Interestingly, robots that looked more human-like evoked emotions similar to those evoked by humans, while robots that looked more animal-like evoked emotions half-way between those evoked by humans and objects. Implications are discussed. © 2019, Springer Nature Switzerland AG.",Abuse study,Perception of robot,0,conferencePaper,Lecture Notes in Electrical Engineering,2019,IT,Embodiment;Emotional empathy,Embodied cues,Social interaction,Interview,h,Videos,Visual,HRI,Not specified
Artificial Empathy for Clinical Companion Robots with Privacy-By-Design,"We present a prototype whereby we enabled a humanoid robot to be used to assist mental health patients and their families. Our approach removes the need for Cloud-based automatic speech recognition systems to address healthcare privacy expectations. Furthermore, we describe how the robot could be used in a mental health facility by giving directions from patient selection to metrics for evaluation. Our overarching goal is to make the robot interaction as natural as possible to the point where the robot can develop artificial empathy for the human companion through the interpretation of vocals and facial expressions to infer emotions. © 2021, ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering.",Therapy,Robot / AI development,0,conferencePaper,"Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering, LNICST",2021,"CA,MX",Cognitive empathy,Language,Machine learning,Machine learning,R,ASUS Zenbo Robot,Robot,HRI ; Human health,Not specified
A composite framework for supporting user emotion detection based on intelligent taxonomy handling,"One of the most relevant issues of a social robot is its capability of catching the attention of a new acquaintance and empathize with her. The first steps towards a system which can be used by a social robot in order to be empathetic are illustrated in this paper. The system can analyze the Twitter ID of the new acquaintance, trying to detect the IAB (Interactive Advertising Bureau) Tier 1 categories that possibly can let arise in him/her a joyful feeling. Furthermore, it can retrieve news about that category and report them to the user, hopefully increasing his/her curiosity towards the system, improving the naturalness of the interaction. Moreover, the system is capable of querying Wikipedia in order to clarify any doubts that may arise in the user. A sample of a possible interaction is reported at the end of the paper.",Chatbot,Robot / AI development,0,journalArticle,LOGIC JOURNAL OF THE IGPL,2021,IT,Cognitive empathy,Language,Machine learning,Machine learning,R,None / Text,Text,Machine learning,GOFAI;Machine learning
Incorporating politeness across languages in customer care responses: Towards building a multi-lingual empathetic dialogue agent,"Customer satisfaction is an essential aspect of customer care systems. It is imperative for such systems to be polite while handling the customer requests or demands. In this paper, we present a large multi-lingual conversational dataset for English and Hindi. We choose data from Twitter having both generic and courteous responses between customer care agents and aggrieved users. We also propose strong baselines that can induce courteous behaviour in generic customer care response in a multi-lingual scenario. We build a deep learning framework that can simultaneously handle different languages and incorporate polite behaviour in the customer care agent's responses. Our system is competent in generating responses in different languages (here, English and Hindi) depending on the customer's preference and also is able to converse with humans in an empathetic manner to ensure customer satisfaction and retention. Experimental results show that our proposed models can converse in both the languages and the information shared between the languages helps in improving the performance of the overall system. Qualitative and quantitative analysis show that the proposed method can converse in an empathetic manner by incorporating courteousness in the responses and hence increasing customer satisfaction. © European Language Resources Association (ELRA), licensed under CC-BY-NC",Service,Robot / AI development,0,conferencePaper,"LREC 2020 - 12th International Conference on Language Resources and Evaluation, Conference Proceedings",2020,IN,Cognitive empathy,Language,Machine learning,Machine learning,R,Text dataset,Text,HRI ; Human relations,Machine learning
Emotional Empathy Model for Robot Partners Using Recurrent Spiking Neural Network Model with Hebbian-LMS Learning,"This paper discusses the development of an emotion model for robot partner system. In our previous studies, we have focused only on the robot's emotional state. However, the emotional state of the other party is also an important factor for smooth conversation in human society. Therefore, the robot partner has two emotional structures for human: empathy and robot emotion. First, human empathy uses a perceptual based emotion model to know the human's emotional state based on the sensory information. Next, we propose a recurrent simple spike response model to improve the robot's emotional model, and we apply “Hebbian-LMS” learning to modify the weights in the spiking neural network. The robot's emotional state is calculated by using the human's emotional information, internal and external information. The robot partner can use the emotional results to control the facial and gesture expression. The utterance style is also changed by the robot's emotional state. As a result, the robot partner can interact emotionally and naturally with human. First, we explain the related works and the development of the robot partner “iPhonoid-C”. Next, we define the architecture of the emotional model to realize Emotional empathy towards human. Then, we discuss the algorithms and the methods for developing the emotional model. Finally, we show experimental results of the proposed method, and discuss the effectiveness of the proposed structure.",Machine learning,Machine learning,12,journalArticle,MALAYSIAN JOURNAL OF COMPUTER SCIENCE,2017,JP,Emotional empathy,Facial expression; Embodied cues,Social Interaction,Machine learning,h,iPhonoid-C,Robot,HRI,GOFAI;Machine learning
Simulating empathic behavior in a social assistive robot,"When used as an interface in the context of Ambient Assisted Living (AAL), a social robot should not just provide a task-oriented support. It should also try to establish a social empathic relation with the user. To this aim, it is crucial to endow the robot with the capability of recognizing the user's affective state and reason on it for triggering the most appropriate communicative behavior. In this paper we describe how such an affective reasoning has been implemented in the NAO robot for simulating empathic behaviors in the context of AAL. In particular, the robot is able to recognize the emotion of the user by analyzing communicative signals extracted from speech and facial expressions. The recognized emotion allows triggering the robot's affective state and, consequently, the most appropriate empathic behavior. The robot's empathic behaviors have been evaluated both by experts in communication and through a user study aimed at assessing the perception and interpretation of empathy by elderly users. Results are quite satisfactory and encourage us to further extend the social and affective capabilities of the robot.",Machine learning,Machine learning,18,journalArticle,MULTIMEDIA TOOLS AND APPLICATIONS,2017,IT,Emotional empathy,Language;Facial expression,Machine learning;Behaviour model;Social interaction,Interview,"h, R",NAO Robot,Robot,HRI,Machine learning
EEG based functional connectivity analysis of human pain empathy towards humans and robots,"Humans can show emotional reactions toward humanoid robots, such as empathy. Previous neuroimaging studies have indicated that neural responses of empathy for others' pain are modulated by an early automatic emotional sharing and a late controlled cognitive evaluation process. Recent studies about pain empathy for robots found humans present similar empathy process towards humanoid robots under painful stimuli as well as to humans. However, the whole-brain functional connectivity and the spatial dynamics of neural activities underlying empathic processes are still unknown. In the present study, the functional connectivity was investigated for ERPs recorded from 18 healthy adults who were presented with pictures of human hand and robot hand under painful and non-painful situations. Functional brain networks for both early and late empathy responses were constructed and a new parameter, empathy index (EI), was proposed to represent the empathy ability of humans quantitatively. We found that the mutual dependences between early ERP components was significantly decreased, but for the late components, there were no significant changes. The mutual dependences for human hand stimuli were larger than to robot hand stimuli for early components, but not for late components. The connectivity weights for early components were larger than late components. EI value shows significant difference between painful and non-painful stimuli, indicating it is a good indicator to represent the empathy of humans. This study enriches our understanding of the neurological mechanisms implicated in human empathy, and provides evidence of functional connectivity for both early and late responses of pain empathy towards humans and robots.",Cognitive neuroscience,Perception of robot,0,journalArticle,NEUROPSYCHOLOGIA,2021,CN,Embodiment,Embodied cues,Pain,Physiological behaviour,h,Images,Visual,HRI,Machine learning
Towards the synthetic self: making others perceive me as an other,"Future applications of robotic technologies will involve interactions with non-expert humans as machines will assume the role of companions, teachers or healthcare assistants. In all those tasks social behavior is a key ability that needs to be systematically investigated and modelled at the lowest level, as even a minor inconsistency of the robot’s behavior can greatly affect the way humans will perceive it and react to it. Here we propose an integrated architecture for generating a socially competent robot.We validate our architecture using a humanoid robot, demonstrating that gaze, eye contact and utilitarian emotions play an essential role in the psychological validity or social salience of Human-Robot Interaction (HRI). We show that this social salience affects both the empathic bonding between the human and a humanoid robot and, to a certain extent, the attribution of a Theory of Mind (ToM). More specifically, we investigate whether these social cues affect other utilitarian aspects of the interaction such as knowledge transfer within a teaching context.",Embodiment,Robot / AI development,21,journalArticle,"Paladyn, Journal of Behavioral Robotics",2015,EU,Emotional empathy,Language; Facial expression; Embodied cues,Social interaction,Interview; Machine learning,R,iCub / iKart,Robot,Human relations,Cognitive model;GOFAI
Deep learning for emotion driven user experiences,"Deep knowledge about user characteristics and behaviors opens new and promising landscapes to the User Experience. Thanks to emerging Machine learning techniques, a new form of communication channel among users and applications may be exploited for customizing and finely tuning the dynamic behavior of applications to the peculiarities of their users. This work investigates the empathic improvement of the User Experiences and exploits inferences on user expressions for activating gaming and entertainment events, that are adopted in a cinematic way to create dynamic application behaviors. The presented approach is applied to a third/first-person horror adventure and a classic table game. As already verified in a preliminary phase of the research, user impressions, collected as subjective evaluation in a controlled experiment, are positive and encourage further steps, like the evaluation of other inferences on users.",Robot / AI development,Robot / AI development,0,journalArticle,Pattern Recognition Letters,2021,IT,Emotional empathy ,Facial expression,Game,Machine learning,h,Virtual agent ,GUI,HRI ; Game,Machine learning
Avatars in Pain: Visible Harm Enhances Mind Perception in Humans and Robots,"Previous research has shown that when people read vignettes about the infliction of harm upon an entity appearing to have no more than a liminal mind, their attributions of mind to that entity increased. Currently, we investigated if the presence of a facial wound enhanced the perception of mental capacities (experience and agency) in response to images of robotic and human-like avatars, compared with unharmed avatars. The results revealed that harmed versions of both robotic and human-like avatars were imbued with mind to a higher degree, irrespective of the baseline level of mind attributed to their unharmed counterparts. Perceptions of capacity for pain mediated attributions of experience, while both pain and empathy mediated attributions of abilities linked to agency. The findings suggest that harm, even when it appears to have been inflicted unintentionally, may augment mind perception for robotic as well as for nearly human entities, at least as long as it is perceived to elicit pain.",Abuse study,Perception of robot,8,journalArticle,PERCEPTION,2018,PL,Emotional empathy,Embodied cues,Visual,Interview,h,Images,Visual,HRI ; Human health,Not specified
A neurocognitive investigation of the impact of socializing with a robot on empathy for pain,"To what extent can humans form social relationships with robots? In the present study, we combined functional neuroimaging with a robot socializing intervention to probe the flexibility of empathy, a core component of social relationships, towards robots. Twenty-six individuals underwent identical fMRI sessions before and after being issued a social robot to take home and interact with over the course of a week. While undergoing fMRI, participants observed videos of a human actor or a robot experiencing pain or pleasure in response to electrical stimulation. Repetition suppression of activity in the pain network, a collection of brain regions associated with empathy and emotional responding, was measured to test whether socializing with a social robot leads to greater overlap in neural mechanisms when observing human and robotic agents experiencing pain or pleasure. In contrast to our hypothesis, functional region-of-interest analyses revealed no change in neural overlap for agents after the socializing intervention. Similarly, no increase in activation when observing a robot experiencing pain emerged post-socializing. Whole-brain analysis showed that, before the socializing intervention, superior parietal and early visual regions are sensitive to novel agents, while after socializing, medial temporal regions show agent sensitivity. A region of the inferior parietal lobule was sensitive to novel emotions, but only during the pre-socializing scan session. Together, these findings suggest that a short socialization intervention with a social robot does not lead to discernible differences in empathy towards the robot, as measured by behavioural or brain responses. We discuss the extent to which long-term socialization with robots might shape social cognitive processes and ultimately our relationships with these machines. This article is part of the theme issue `From social brains to social robots: applying neurocognitive insights to human-robot interaction'.",Cognitive neuroscience,Perception of robot,20,journalArticle,PHILOSOPHICAL TRANSACTIONS OF THE ROYAL SOCIETY B-BIOLOGICAL SCIENCES,2019,EU,Emotional empathy;Embodiment;Social intelligence,Embodied cues,Social interaction;Pain,Physiological behaviour,h,Images / Images Cozmo Robot,Visual,HRI,Not specified
Affective Embodied Agents and Their Effect on Decision Making,"Embodied agents, such as avatars and social robots, are increasingly incorporating a capacity to enact affective states and recognize the mood of their interlocutor. This influences how users perceive these technologies and how they interact with them. We report on an experiment aimed at assessing perceived empathy and fairness among individuals interacting with avatars and robots when compared to playing against a computer or a fellow human being. Twenty-one individuals were asked to play the ultimatum game, playing the role of a responder against another person, a computer, an avatar and a robot for a total of 32 games (8 per condition). We hypothesize that affective expressions by avatars and robots influence the emotional state of the users, leading them to irrational behavior by rejecting unfair proposals. We monitored galvanic skin response and heart rate of the players in the period when the offer was made by the proposer until the decision was announced by the responder. Our results show that most fair offers were accepted while most unfair offers were rejected. However, participants rejected more very unfair offers made by people and computers than by the avatars or robots.",Companion robot,Robot / AI development,0,journalArticle,Proceedings,2019,MX,Emotional empathy;Embodiment,Embodied cues,Game,"Physiological behaviour, Cognitive test",h,Eva Robot / Avatar,Robot,HRI,Machine learning
An Emotion-Based Interaction Strategy to Improve Human-Robot Interaction,"Emotion and empathy are key subjects on human-robot interaction, especially regarding social robots. Several studies have investigated emotional reactions of humans toward robots, while others deal with development of actual systems to analyze how affective feedback may influence this kind of interaction. This paper presents an emotion-aware interaction strategy applied to an embodied virtual agent, implemented as an Android application. The system assigns two distinct paradigms to the virtual character, according to the user's emotion, inferred through facial expressions analysis. Within subject user experiments have been performed, in order to evaluate if the proposed strategy improves empathy and pleasantness.",Chatbot,Robot / AI development,4,conferencePaper,PROCEEDINGS OF 13TH LATIN AMERICAN ROBOTICS SYMPOSIUM AND 4TH BRAZILIAN SYMPOSIUM ON ROBOTICS - LARS/SBR 2016,2016,BR,Emotional empathy,Facial expression,Game;Social interaction,Interview,h,Smartphone,GUI,HRI,Machine learning
Development of a Cloud-based Computational Framework for an Empathetic Robot,"This article presents the development and preliminary evaluation of an empathy controlled robot. Such a robot is one step forward towards industry 5.0, as it provides a theoretical framework to enable the performance of the robot to be customized to suit the needs of both the task as well as the operator. An inventive step is taken through the separation of computational resources based on whether the algorithms are addressing functional or experiential needs. The paper therefore addresses the requirement for new approaches that can be employed in the design of mobile robots to reduce cost, power consumption, and computational burden of the system. We propose that tasks requiring real-time and safety critical control are processed using dedicated on-board computers, whereas functionality dedicated to system optimization, Machine learning and customization are handled through use a cloud-based platform. In this paper, key components of the architecture are defined, and the development and preliminary evaluation of an exemplar robot capable of changing its behavior in accordance with the perceived emotional state of an operator's voice is presented.",Companion robot,Robot / AI development,1,conferencePaper,PROCEEDINGS OF 2019 11TH INTERNATIONAL CONFERENCE ON COMPUTER AND AUTOMATION ENGINEERING (ICCAE 2019),2019,AU,Emotional empathy,Language;Facial expression,Social interaction;Machine learning,Machine learning,R,Turtlebot3 robot,Robot,HRI,Machine learning
Modeling and Simulating Empathic Behavior in Social Assistive Robots,"Several studies report successful results on how social assistive robots can be employed as interface in the assisted living domain. In our opinion, to plan their response and interact successfully with people, it is crucial to recognize human emotions. To this aim, features of the prosody of the speech together with facial expressions and gestures may be used to recognize the emotional state of the user. The information gained from these different sources may be fused in order to endow the robot with the capability to reason on the user's affective state. In this paper we describe how this capability has been implemented in the NAO robot and how this allows simulating empathic behaviors in the context of Ambient Assisted Living.",Companion robot,Robot / AI development,4,conferencePaper,Proceedings of the 11th Biannual Conference on Italian SIGCHI Chapter,2015,IT,Cognitive empathy,Facial expression; Embodied cues; Language,Machine learning,Machine learning,R,NAO Robot,Robot,HRI,Machine learning
Design and Evaluation of a Peripheral Robotic Conversation Companion,"We present the design, implementation, and evaluation of a peripheral empathy-evoking robotic conversation companion, Kip1. The robot's function is to increase people's awareness to the effect of their behavior towards others, potentially leading to behavior change. Specifically, Kip1 is designed to promote non-aggressive conversation between people. It monitors the conversation's nonverbal aspects and maintains an emotional model of its reaction to the conversation. If the conversation seems calm, Kip1 responds by a gesture designed to communicate curious interest. If the conversation seems aggressive, Kip1 responds by a gesture designed to communicate fear. We describe the design process of Kip1, guided by the principles of peripheral and evocative. We detail its hardware and software systems, and a study evaluating the effects of the robot's autonomous behavior on couples' conversations. We find support for our design goals. A conversation companion reacting to the conversation led to more gaze attention, but not more verbal distraction, compared to a robot that moves but does not react to the conversation. This suggests that robotic devices could be designed as companions to human-human interaction without compromising the natural communication flow between people. Participants also rated the reacting robot as having significantly more social human character traits and as being significantly more similar to them. This points to the robot's potential to elicit people's empathy.",Companion robot,Robot / AI development,78,conferencePaper,PROCEEDINGS OF THE 2015 ACM/IEEE INTERNATIONAL CONFERENCE ON HUMAN-ROBOT INTERACTION (HRI'15),2015,IL,Emotional empathy,Language;Embodied cues,Social interaction,Physiological behaviour,h,Kip1 robot (lamp-like),Robot,HRI,GOFAI;Not specified
When Children Teach a Robot to Write: An Autonomous Teachable Humanoid Which Uses Simulated Handwriting,"This article presents a novel robotic partner which children can teach handwriting. The system relies on the learning by teaching paradigm to build an interaction, so as to stimulate meta-cognition, empathy and increased self-esteem in the child user. We hypothesise that use of a humanoid robot in such a system could not just engage an unmotivated student, but could also present the opportunity for children to experience physically-induced bene fits encountered during human-led handwriting interventions, such as motor mimicry. By leveraging simulated handwriting on a synchronised tablet display, anao humanoid robot with limited fine motor capabilities has been con figured as a suitably embodied handwriting partner. Statistical shape models derived from principal component analysis of a dataset of adult-written letter trajectories allow the robot to draw purposefully deformed letters. By incorporating feedback from user demonstrations, the system is then able to learn the optimal parameters for the appropriate shape models. Preliminary in situ studies have been conducted with primary school classes to obtain insight into children's use of the novel system. Children aged 6-8 successfully engaged with the robot and improved its writing to a level which they were satisfied with. The validation of the interaction represents a significant step towards an innovative use for robotics which addresses a widespread and socially meaningful challenge in education.",Education,Robot / AI development,145,conferencePaper,PROCEEDINGS OF THE 2015 ACM/IEEE INTERNATIONAL CONFERENCE ON HUMAN-ROBOT INTERACTION (HRI'15),2015,CH,Cognitive empathy;Social intelligence,Social behavior,Social interaction,Machine learning,h,NAO Robot,Robot,HRI,GOFAI;Machine learning
I Remember You! Interaction with Memory for an Empathic Virtual Robotic Tutor,"We present a study that investigates the effect of incorporating memory in the interaction for a virtual robotic tutor in terms of helping children achieve a pedagogical goal and the perceived likeability and empathy of the tutor. The domain is a virtual robotic tutor who is guiding and helping learners through a mobile Treasure Hunt exercise that tests their map reading skills. The contribution described in this paper is the discovery that incorporating 'memory' through utterances that recall events from previous interactions significantly increases the learner's ability to perform a pedagogical task. However, the virtual tutor with memory was perceived as less likeable and the instructions given as harder to follow than with a virtual tutor without memory. In addition, there was a significant drop in perceived empathy. This work has a large potential influence in the field of interaction design for agents as one cannot blindly add in human-like features, such as, memory that improve task performance without considering the potential detrimental effects to the perceived empathy and likeability.",Education,Robot / AI development,24,conferencePaper,Proceedings of the 2016 International Conference on Autonomous Agents &amp; Multiagent Systems,2016,EU,Cognitive empathy,Social behavior,Social interaction,Interview,h,Virtual Agent / EMYS Robot,GUI,HRI,Cognitive model
A New Chatbot for Customer Service on Social Media,"Users are rapidly turning to social media to request and receive customer service; however, a majority of these requests were not addressed timely or even not addressed at all. To overcome the problem, we create a new conversational system to automatically generate responses for users requests on social media. Our system is integrated with state-of-the-art deep learning techniques and is trained by nearly 1M Twitter conversations between users and agents from over 60 brands. The evaluation reveals that over 40% of the requests are emotional, and the system is about as good as human agents in showing empathy to help users cope with emotional situations. Results also show our system outperforms information retrieval system based on both human judgments and an automatic evaluation metric.",Service,Robot / AI development,299,conferencePaper,PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17),2017,US,Emotional empathy;Cognitive empathy,Language,Social interaction,Machine learning,R,Virtual Agent,GUI,Customer Service,Machine learning
Touch Your Heart: A Tone-aware Chatbot for Customer Care on Social Media,"Chatbot has become an important solution to rapidly increasing customer care demands on social media in recent years. However, current work on chatbot for customer care ignores a key to impact user experience - tones. In this work, we create a novel tone-aware chatbot that generates toned responses to user requests on social media. We first conduct a formative research, in which the effects of tones are studied. Significant and various influences of different tones on user experience are uncovered in the study. With the knowledge of effects of tones, we design a deep learning based chatbot that takes tone information into account. We train our system on over 1.5 million real customer care conversations collected from Twitter. The evaluation reveals that our tone -aware chatbot generates as appropriate responses to user requests as human agents. More importantly, our chatbot is perceived to be even more empathetic than human agents.",Chatbot,Robot / AI development,56,conferencePaper,PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018),2018,US,Cognitive empathy,Language,Social interaction; Machine learning,Interview; Machine learning,R,Text Dataset,Text,HRI ; Customer Service,Machine learning
Prompting Prosocial Human Interventions in Response to Robot Mistreatment,"Inspired by the benefits of human prosocial behavior, we explore whether prosocial behavior can be extended to a Human-Robot Interaction (HRI) context. More specifically, we study whether robots can induce prosocial behavior in humans through a 1x2 between-subjects user study (N = 30) in which a confederate abused a robot. Through this study, we investigated whether the emotional reactions of a group of bystander robots could motivate a human to intervene in response to robot abuse. Our results show that participants were more likely to prosocially intervene when the bystander robots expressed sadness in response to the abuse as opposed to when they ignored these events, despite participants reporting similar perception of robot mistreatment and levels of empathy for the abused robot. Our findings demonstrate possible effects of group social influence through emotional cues by robots in human-robot interaction. They reveal a need for further research regarding human prosocial behavior within HRI.",Abuse study,Perception of robot,6,conferencePaper,PROCEEDINGS OF THE 2020 ACM/IEEE INTERNATIONAL CONFERENCE ON HUMAN-ROBOT INTERACTION (HRI `20),2020,US,Social intelligence;Embodiment,Embodied cues,Social interaction,"Machine learning, Physiological behaviour",h,NICO Robot,Robot,HRI,Not specified
Drone in Love: Emotional Perception of Facial Expressions on Flying Robots,"Drones are rapidly populating human spaces, yet little is known about how these flying robots are perceived and understood by humans. Recent works suggested that their acceptance is predicated upon their sociability. This paper explores the use of facial expressions to represent emotions on social drones. We leveraged design practices from ground robotics and created a set of rendered robotic faces that convey basic emotions. We evaluated individuals’ response to these emotional facial expressions on drones in two empirical studies (N = 98, N = 98). Our results demonstrate that individuals accurately recognize five drone emotional expressions, as well as make sense of intensities within emotion categories. We describe how participants were emotionally affected by the drone, showed empathy towards it, and created narratives to interpret its emotions. As a consequence, we formulate design recommendations for social drones and discuss methodological insights on the use of static versus dynamic stimuli in affective robotics studies.",Embodiment,Robot / AI development,0,conferencePaper,Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems,2021,IL;DE;CA,Emotional empathy,Facial expression,Social interaction,Interview,h,Images,Visual,HRI,Not specified
Zara Returns: Improved Personality Induction and Adaptation by an Empathetic Virtual Agent,"Virtual agents need to adapt their personality to the user in order to become more empathetic. To this end, we developed Zara the Supergirl, an interactive empathetic agent, using a modular approach. In this paper, we describe the enhanced personality module with improved recognition from speech and text using deep learning frameworks. From raw audio, an average F-score of 69.6 was obtained from real-time personality assessment using a Convolutional Neural Network (CNN) model. From text, we improved personality recognition results with a CNN model on top of pre-trained word embeddings and obtained an average F-score of 71.0. Results from our Human-Agent Interaction study confirmed our assumption that people have different agent personality preferences. We use insights from this study to adapt our agent to user personality.",Machine learning,Machine learning,14,conferencePaper,PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017): SYSTEM DEMONSTRATIONS,2017,HK,Cognitive empathy,Facial expression; Language,Machine learning,Machine learning,R,Avatar,GUI,HRI,Machine learning
The Role of Animism Tendencies and Empathy in Adult Evaluations of Robot,"We investigated whether Japanese adults' beliefs about friendship and morality toward robots differing in appearance (i.e., humanoid, dog-like, and egg-shaped) related to their animism tendencies and empathy. University students responded to Questionnaires regarding three animism tendencies (i.e., general animism or a tendency to believe souls or gods in nonliving things, aliveness animism or a tendency to consider nonliving things as live entities, and agentic animisms or a tendency to attribute biological, artifactual, psychological, perceptual, and naming properties) and empathy. We found that friendship and morality were related to slightly different animism tendencies and empathy even though they shared some major factors. Aliveness animism, as well as a tendency to attribute perceptual and name properties toward robots, might be necessary for an individual to believe that robots could be social agents. Participants who responded that robots could be their friends showed a tendency to feel a soul in manmade objects and a strong self-oriented emotional reactivity, whereas participants who answered that robots were moral beings showed a tendency to exhibit strong emotional susceptibility. We discuss implications of these results and reasons why people feel that robots have a mind or consciousness.",Perception of robot,Perception of robot,4,conferencePaper,Proceedings of the 7th International Conference on Human-Agent Interaction,2019,JP,Embodiment,Embodied cues,Behaviour model,Interview,h,"Toyota’s KIROBO mini, Sony’s aibo and MJI’s Tapia",Robot,HRI,Not specified
DELEX: A DEep Learning Emotive EXperience: Investigating Empathic HCI,"Recent advances in Machine Learning have unveiled interesting possibilities for real-time investigating about user characteristics and expressions like, but not limited to, age, sex, body posture, emotions and moods. These new opportunities lay the foundations for new HCI tools for interactive applications that adopt user emotions as a communication channel.This paper presents an Emotion Controlled User Experience that changes according to user feelings and emotions analysed at runtime. Aiming at obtaining a preliminary evaluation of the proposed ecosystem, a controlled experiment has been performed in an engineering and software development company, where 60 people have been involved as volunteers. The subjective evaluation has been based on a standard Questionnaire commonly adopted for measuring user perceived sense of immersion in Virtual Environments. The results of the controlled experiment encourage further investigations strengthen by the analysis of objective performance measurements and user physiological parameters.",VR,Perception of robot,0,conferencePaper,Proceedings of the International Conference on Advanced Visual Interfaces,2020,IT,Cognitive empathy,Facial expression,Visual; Game,Interview; Machine learning,h,None / Avatars,GUI,VR,Machine learning
Evaluation of the Emotional Answer in HRI on a Game Situation,"This project has as purpose to propose an adequate method for the assessment of the emotional answer after an interaction with a social and emotional robot. A lottery game application has been developed for playing with the robot Nao, and through an experimental scenario the empathy towards a robot has been demonstrated. As a result, the Emocards are presented as a promising assessment method for the emotional answer of the users.",Perception of robot,Perception of robot,1,conferencePaper,Proceedings of the Latin American Conference on Human Computer Interaction,2015,MX,Emotional empathy,Embodied cues,Game,Physiological behaviour,h,NAO Robot,Robot,HRI,Not specified
He who hesitates is lost (...in thoughts over a robot),"In a team, the strong bonds that can form between teammates are often seen as critical for reaching peak performance. This perspective may need to be reconsidered, however, if some team members are autonomous robots since establishing bonds with fundamentally inanimate and expendable objects may prove counterproductive. Previous work has measured empathic responses towards robots as singular events at the conclusion of experimental sessions. As relationships extend over long periods of time, sustained empathic behavior towards robots would be of interest. In order to measure user actions that may vary over time and are affected by empathy towards a robot teammate, we created the TEAMMATE simulation system. Our findings suggest that inducing empathy through a back story narrative can significantly change participant decisions in actions that may have consequences for a robot companion over time. The results of our study can have strong implications for the overall performance of human machine teams.",Military,Robot / AI development,10,conferencePaper,"PROCEEDINGS OF THE TECHNOLOGY, MIND, AND SOCIETY CONFERENCE (TECHMINDSOCIETY'18)",2018,US,Social intelligence;Embodiment,Social behavior,Social interaction;Game,Physiological behaviour,h,Virtual Agent,GUI,HRI ; Game,GOFAI
"Attachment styles moderate customer responses to frontline service robots: Evidence from affective, attitudinal, and behavioral measures","Despite the growing application of interactive technologies like service robots in customer service, there is limited understanding about how customers respond to interactions with frontline service robots compared to those with frontline human employees. Moreover, it is unclear whether all customers respond to the interaction with frontline service robots in the same way. Our research looks at how individual differences in social behaviors, specifically in customers' attachment styles, influence three types of customer responses: affective responses (experienced pleasantness), attitudinal responses (perceived empathy, satisfaction), and behavioral responses (word-of-mouth). Three experimental studies reveal that customers with low (vs. high) scores on anxious attachment style (AAS) measures respond more negatively to frontline service robot (compared to a frontline human agent). We investigate alternative explanations for these findings, such as robots' level of anthropomorphism and we show that human-likeness features such as voice type and level of human-like physical appearance, cannot explain our findings. Our results indicate that for low-AAS customers replacing frontline human service agent with frontline robot undermines customer attitude and behavioral responses to service robots, leading to possible implications on customer segmentation, targeting, and marketing communication.",Service,Robot / AI development,0,journalArticle,PSYCHOLOGY & MARKETING,2021,IT;NL,Emotional empathy,Social behavior,Visual,Interview,h,Videos,Visual,HRI,Not specified
Emotion Differentiation based on Decision-Making in Emotion Model,"Having emotions is essential for robots in order for them to understand and sympathize with people's feelings. In addition, it may allow robots to be accepted in human society. The role of emotions in decision-making is another important perspective. In this paper, a model of emotions is proposed based on various neurological and psychological findings related to empathic communication between humans and robots. Subsequently, a decision-making mechanism based on affects using convolutional long short-term memory and deep deterministic policy gradient is examined. We set a 'facial expression' task simulating mother-child interactions and verified emotion differentiation during the task. © 2018 IEEE.",Machine learning,Machine learning,2,conferencePaper,RO-MAN 2018 - 27th IEEE International Symposium on Robot and Human Interactive Communication,2018,JP,Cognitive empathy,Facial expression,Social Interaction; Machine learning,Machine learning,h;R,Image Dataset,Visual,HRI,Machine learning
The Affective Loop: A Tool for Autonomous and Adaptive Emotional Human-Robot Interaction,"The paper presents an affective model for social robotics, where the robot is capable of behavior adaptation, in accordance with the needs and preferences of a particular user. The proposed approach differs from other studies in human-robot interaction as these usually have been using the `Wizard of Oz' technique, where a person remotely operates a robot. On the other side, simulated robots are not able of personalized behaviors and behave according to the preprogrammed set of rules. We provide a tool to personalize affective artificial behaviors in cooperative human-robot scenarios, where human emotion recognition, appropriate robotic behavior selection and expression of robotic emotions play a key role. The preliminary experiments show that the personalized affective robotic behavior can achieve better results in a scenario in which a robot motivates children in learning. We believe that human-robot interfaces which mimic how humans interact with one another in an empathic way could ultimately lead to robots being accepted in the wider domain.",Model,Perception of robot,6,conferencePaper,ROBOT IN℡LIGENCE TECHNOLOGY ANDAPPLICATIONS 3,2015,EU,Social intelligence,Embodied cues,Behaviour model,Interview; Interview,h,NAO Robot,Robot,HRI,Not specified
"Differential Facial Articulacy in Robots and Humans Elicit Different Levels of Responsiveness, Empathy, and Projected Feelings","Life-like humanoid robots are on the rise, aiming at communicative purposes that resemble humanlike conversation. In human Social interaction, the facial expression serves important communicative functions. We examined whether a robot's face is similarly important in human-robot communication. Based on emotion research and neuropsychological insights on the parallel processing of emotions, we argue that greater plasticity in the robot's face elicits higher affective responsivity, more closely resembling human-to-human responsiveness than a more static face. We conducted a between-subjects experiment of 3 (facial plasticity: human vs. facially flexible robot vs. facially static robot) x 2 (treatment: affectionate vs. maltreated). Participants (N = 265; M-age = 31.5) were measured for their emotional responsiveness, empathy, and attribution of feelings to the robot. Results showed empathically and emotionally less intensive responsivity toward the robots than toward the human but followed similar patterns. Significantly different intensities of feelings and attributions (e.g., pain upon maltreatment) followed facial articulacy. Theoretical implications for underlying processes in human-robot communication are discussed. We theorize that precedence of emotion and affect over cognitive reflection, which are processed in parallel, triggers the experience of `because I feel, I believe it's real,' despite being aware of communicating with a robot. By evoking emotional responsiveness, the cognitive awareness of `it is just a robot' fades into the background and appears not relevant anymore.",Embodiment,Robot / AI development,0,journalArticle,ROBOTICS,2020,NL,Emotional empathy,Facial expression,Social interaction,Interview,R,Videos NAO & Alice,Visual,HRI,GOFAI
The Role of Personality Factors and Empathy in the Acceptance and Performance of a Social Robot for Psychometric Evaluations,"Research and development in socially assistive robotics have produced several novel applications in the care of senior people. However, some are still unexplored such as their use as psychometric tools allowing for a quick and dependable evaluation of human users' intellectual capacity. To fully exploit the application of a social robot as a psychometric tool, it is necessary to account for the users' factors that might influence the interaction with a robot and the evaluation of user cognitive performance. To this end, we invited senior participants to use a prototype of a robot-led Cognitive test and analyzed the influence of personality traits and user's empathy on the cognitive performance and technology acceptance. Results show a positive influence of a personality trait, the “openness to experience”, on the human-robot interaction, and that other factors, such as anxiety, trust, and intention to use, are influencing technology acceptance and correlate the evaluation by psychometric tests.",Companion robot,Robot / AI development,2,journalArticle,ROBOTICS,2020,EU;IT,Cognitive empathy; Emotional empathy,Social behavior,Social interaction,Cognitive test; Interview,,Pepper Robot,Robot,HRI ; Human health,GOFAI;Not specified
Measuring empathy for human and robot hand pain using electroencephalography,"This study provides the first physiological evidence of humans' ability to empathize with robot pain and highlights the difference in empathy for humans and robots. We performed electroencephalography in 15 healthy adults who observed either human- or robot-hand pictures in painful or non-painful situations such as a finger cut by a knife. We found that the descending phase of the P3 component was larger for the painful stimuli than the non-painful stimuli, regardless of whether the hand belonged to a human or robot. In contrast, the ascending phase of the P3 component at the frontal-central electrodes was increased by painful human stimuli but not painful robot stimuli, though the interaction of ANOVA was not significant, but marginal. These results suggest that we empathize with humanoid robots in late top-down processing similarly to human others. However, the beginning of the top-down process of empathy is weaker for robots than for humans.",Cognitive neuroscience,Perception of robot,98,journalArticle,SCIENTIFIC REPORTS,2015,JP,Embodiment,Embodied cues,Pain,Physiological behaviour,h,Images,Visual,HRI ; Human health,Not specified
Ordered interpersonal synchronisation in ASD children via robots,"Children with autistic spectrum disorders (ASD) experience persistent disrupted coordination in interpersonal synchronisation that is thought to be associated with deficits in neural connectivity. Robotic interventions have been explored for use with ASD children worldwide revealing that robots encourage one-to-one social and emotional interactions. However, associations between interpersonal synchronisation and Emotional empathy have not yet been directly explored in French and Japanese ASD children when they interact with a human or a robot under analogous experimental conditions. Using the paradigm of actor-perceiver, where the child was the actor and the robot or the human the perceiver, we recorded the autonomic heart rate activation and reported emotional feelings of ASD children in both countries. Japanese and French ASD children showed different interpersonal synchronisation when they interacted with the human perceiver, even though the human was the same in both countries. However, they exhibited similar interpersonal synchronisation when the perceiver was the robot. The findings suggest that the mechanism combining interpersonal synchronisation and Emotional empathy might be weakened but not absent in ASD children and that both French and Japanese ASD children do spontaneously and unconsciously discern non verbal actions of non human partners through a direct matching process that occurs via automatic mapping.",Human-robot interaction,Human-robot interaction,0,journalArticle,SCIENTIFIC REPORTS,2020,AU;JP,Emotional empathy,Embodied cues; Language,Social interaction,"Physiological behaviour, Interview",h,Pekoppa Robot,Robot,HRI ; Human health,GOFAI;Not specified
Promises and trust in human-robot interaction.,"Understanding human trust in machine partners has become imperative due to the widespread use of intelligent machines in a variety of applications and contexts. The aim of this paper is to investigate whether human-beings trust a social robot-i.e. a human-like robot that embodies emotional states, empathy, and non-verbal communication-differently than other types of agents. To do so, we adapt the well-known economic trust-game proposed by Charness and Dufwenberg (2006) to assess whether receiving a promise from a robot increases human-trust in it. We find that receiving a promise from the robot increases the trust of the human in it, but only for individuals who perceive the robot very similar to a human-being. Importantly, we observe a similar pattern in choices when we replace the humanoid counterpart with a real human but not when it is replaced by a computer-box. Additionally, we investigate participants' psychophysiological reaction in terms of cardiovascular and electrodermal activity. Our results highlight an increased psychophysiological arousal when the game is played with the social robot compared to the computer-box. Taken all together, these results strongly support the development of technologies enhancing the humanity of robots.",Perception of robot,Perception of robot,9,journalArticle,Scientific reports,2021,IT,Emotional empathy,Language,Social interaction,Interview; Physiological behaviour,h,FACE robot,Robot,HRI,Cognitive model
A Preliminary Study on Realizing Human-Robot Mental Comforting Dialogue via Sharing Experience Emotionally.,"Mental health issues are receiving more and more attention in society. In this paper, we introduce a preliminary study on human-robot mental comforting conversation, to make an android robot (ERICA) present an understanding of the user's situation by sharing similar emotional experiences to enhance the perception of empathy. Specifically, we create the emotional speech for ERICA by using CycleGAN-based emotional voice conversion model, in which the pitch and spectrogram of the speech are converted according to the user's mental state. Then, we design dialogue scenarios for the user to talk about his/her predicament with ERICA. In the dialogue, ERICA shares other people's similar predicaments and adopts a low-spirit voice to express empathy to the interlocutor's situation. At the end of the dialogue, ERICA tries to encourage with a positive voice. Subsequently, Questionnaire-based evaluation experiments were conducted with the recorded conversation. In the Questionnaire, we use the Big Five scale to evaluate ERICA's personality. In addition, the perception of emotion, empathy, and encouragement in the dialogue are evaluated. The results show that the proposed emotional expression strategy helps the android robot better present low-spirit emotion, empathy, the personality of extroversion, while making the user better feel the encouragement.",Therapy,Robot / AI development,0,journalArticle,"Sensors (Basel) Sensors (Basel, Switzerland)",2022,JP,Emotional empathy,Embodiment,Social Interaction,Interview,h,android robot,Robot,HRI,Machine learning
LEMON: A Lightweight Facial Emotion Recognition System for Assistive Robotics Based on Dilated Residual Convolutional Neural Networks.,"The development of a Social Intelligence System based on artificial intelligence is one of the cutting edge technologies in Assistive Robotics. Such systems need to create an empathic interaction with the users; therefore, it os required to include an Emotion Recognition (ER) framework which has to run, in near real-time, together with several other intelligent services. Most of the low-cost commercial robots, however, although more accessible by users and healthcare facilities, have to balance costs and effectiveness, resulting in under-performing hardware in terms of memory and processing unit. This aspect makes the design of the systems challenging, requiring a trade-off between the accuracy and the complexity of the adopted models. This paper proposes a compact and robust service for Assistive Robotics, called Lightweight EMotion recognitiON (LEMON), which uses image processing, Computer Vision and Deep Learning (DL) algorithms to recognize facial expressions. Specifically, the proposed DL model is based on Residual Convolutional Neural Networks with the combination of Dilated and Standard Convolution Layers. The first remarkable result is the few numbers (i.e., 1.6 Million) of parameters characterizing our model. In addition, Dilated Convolutions expand receptive fields exponentially with preserving resolution, less computation and memory cost to recognize the distinction among facial expressions by capturing the displacement of the pixels. Finally, to reduce the dying ReLU problem and improve the stability of the model, we apply an Exponential Linear Unit (ELU) activation function in the initial layers of the model. We have performed training and evaluation (via one- and five-fold cross validation) of the model with five datasets available in the community and one mixed dataset created by taking samples from all of them. With respect to the other approaches, our model achieves comparable results with a significant reduction in terms of the number of parameters.",Image emotion classification,Machine learning,2,journalArticle,"Sensors (Basel) Sensors (Basel, Switzerland)",2022,IT,Emotional empathy,Facial expression,Machine learning,Machine learning,h,video,Visual,Computer vision,Machine learning
Influence of Upper Body Pose Mirroring in Human-Robot Interaction,"This paper explores the effect of upper body pose mirroring in human-robot interaction. A group of participants is used to evaluate how imitation by a robot affects people's perception of their conversation with it. A set of twelve questions about the participants' university experience serves as a backbone for the dialogue structure. In our experimental evaluation, the robot reacts in one of three ways to the human upper body pose: ignoring it, displaying its own upper body pose, and mirroring it. The manner in which the robot behaviour influences human appraisal is analysed using the standard Godspeed Questionnaire. Our results show that robot body mirroring/non-mirroring influences the perceived humanness of the robot. The results also indicate that body pose mirroring is an important factor in facilitating rapport and empathy in human Social interactions with robots.",Mirroring,Perception of robot,12,conferencePaper,SOCIAL ROBOTICS (ICSR 2015),2015,UK,Imitation,Embodied cues,Social interaction,Interview,h,NAO Robot,Robot,HRI,Not specified
Towards a Robot Computational Model to Preserve Dignity in Stigmatizing Patient-Caregiver Relationships,"Parkinson's disease (PD) patients with an expressive mask are particularly vulnerable to stigmatization during interactions with their caregivers due to their inability to express affect through nonverbal channels. Our approach to uphold PD patient dignity is through the use of an ethical robot that mediates patient shame when it recognizes norm violations in the patient-caregiver interaction. This paper presents the basis for a computational model tasked with computing patient shame and the empathetic response of a caregiver during “empathetic opportunities” in their interaction. A PD patient is liable to suffer indignity when there is a substantial difference between his experienced shame and the empathy shown by the caregiver. When this difference falls outside of acceptable set bounds (norms), the robotic agent will act using subtle, nonverbal cues to guide the relationship back within these bounds, preserving patient dignity.",Health,Robot / AI development,7,conferencePaper,SOCIAL ROBOTICS (ICSR 2015),2015,US,Emotional empathy,Language,Game,Machine learning,h,None,None,HRI; Human health,GOFAI;Machine learning
Designing for Perceived Robot Empathy for Children in Long-Term Care,"We describe a mixed-methods approach toward the design and evaluation of social robots that can offer emotional support for children in long-term care environments. Based on the results of a needfinding interview with a local expert, our specific aim was to design a robot that would be perceived as empathetic. An online human-subject study (n&nbsp;=&nbsp;26) provided preliminary support for a hypothesis that this design goal could be achieved by designing robots to maintain the flow of conversation and ask related followup questions to further understand interlocutors’ feelings.",Therapy,Robot / AI development,0,conferencePaper,"Social Robotics: 13th International Conference, ICSR 2021, Singapore, Singapore, November 10–13, 2021, Proceedings",2021,US,Social intelligence,Language,Social Interaction,Interview ,h,NAO Robot,Robot,HRI,Cognitive model
"Leveraging human-robot interaction in hospitality services: Incorporating the role of perceived value, empathy, and information sharing into visitors' intentions to use social robots","Social robots have become pervasive in the tourism and hospitality service environments. The empirical understanding of the drivers of visitors' intentions to use robots in such services has become an urgent necessity for their sustainable deployment. Certainly, using social androids within hospitality services requires organisations' attentive commitment to value creation and fulfilling service quality expectations. In this paper, via structural equation modelling (SEM) and semi-structured interviews with managers, we conceptualise and empirically test visitors' intentions to use social robots in hospitality services. With data collected in Singapore's hospitality settings, we found visitors' intentions to use social robots stem from the effects of technology acceptance variables, service quality dimensions leading to perceived value, and two further dimensions from human robot interaction (HRI): empathy and information sharing. Analysis of these dimensions' importance provides a deeper understanding of novel opportunities managers may take advantage of to position social robot-delivered services in tourism and hospitality strategies.",Service,Robot / AI development,29,journalArticle,TOURISM MANAGEMENT,2020,FR;UK;SG,Emotional empathy;Cognitive empathy,Language,Social interaction,"Interview, Interview",h,Images,Visual,HRI ; Customer Service,Not specified
Analyzing Human-Avatar Interaction with Neurotypical and not Neurotypical Users,"Assistive technologies have been used to improve the quality of life of people who have been diagnosed with health issues. In this case, we aim to use an assistive technology in the shape of an affective avatar to help people who have been diagnosed with different forms of Social Communications Disorders (SCD). The designed avatar presents a humanoid face that displays emotions with a subtlety akin to that of real life human emotions, with those emotions changing according to the interactions that the user chooses to perform on the avatar. We have used Blender for the design of the emotions, which are happiness, sadness, surprise, fear and anger, plus a neutral emotion, while Unity was used to dictate the behavior of the avatar when the interactions were performed, which could be positive (caress), negative (poke) or neutral (wait). The avatar has been evaluated by 48 people from different backgrounds and the results show the overall positive reception by the users, as well as the difference between neurotypical and non-neurotypical users in terms of emotion recognition and chosen interactions. A ground truth has been established in terms of prototypic empathic interactions by the users.",Therapy,Robot / AI development,1,conferencePaper,"UBIQUITOUS COMPUTING AND AMBIENT IN℡LIGENCE, UCAMI 2016, PT I",2016,EU,Emotional empathy,Facial expression,Social Interaction,Interview,h; R,Avatar,GUI,HRI ; Human health,Not specified
Personality traits assessment using P.A.D. Emotional space in human-robot interaction,"Cognitive social robotics is the field of research that is committed to building social robots that facilitate to draw parallels with human beings. Humans assess the behavior and personality of their counterparts to adapt their behavior and show empathy to flourish human-human interaction. Similarly, assessment of human personality is highly critical in realizing natural and intelligent human-robot interaction. Numerous personality traits assessment systems have been reported in the literature; however, most of them target the big five personality traits. From only visual information, this work proposes to use pleasure, arousal, and dominance emotional space for the assessment of personality traits based on the work of Mehrabian. To validate the system, three different scenarios have been developed to assess 12 different personality traits on a social humanoid robot. Experimental results show that the system can assess human personality traits with 84% accuracy in real-time and, hence, it can adapt its behavior according to the perceived personality of the interaction partner. Copyright © 2021 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved.",Machine learning,Machine learning,0,conferencePaper,"VISIGRAPP 2021 - Proceedings of the 16th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications",2021,DE,Social intelligence,Language,Social interaction,Machine learning,R,ROBIN robot,Robot,HRI ; Human health,GOFAI;Machine learning
Affect-Aware Student Models for Robot Tutors,"Computational tutoring systems, such as educational software or interactive robots, have the potential for great societal benefit. Such systems track and assess students' knowledge via inferential methods, such as the popular Bayesian Knowledge Tracing (BKT) algorithm. However, these methods do not typically draw on the affective signals that human teachers use to assess knowledge, such as indications of discomfort, engagement, or frustration. In this paper we present a novel extension to the BKT model that uses affective data, derived autonomously from video records of children playing an interactive story-telling game with a robot, to infer student knowledge of reading skills. We find that, compared to a control group of children who played the game with only a tablet, children who interacted with an embodied social robot generated stronger affective data signals of engagement and enjoyment during the interaction. We then show that incorporating this affective data into model training improves the quality of the learned knowledge inference models. These results suggest that physically embodied, affect-aware robot tutors can provide more effective and empathic educational experiences for children, and advance both algorithmic and human-centered motivations for further development of systems that tightly integrate affect understanding and complex models of inference with interactive, educational robots.",Education,Robot / AI development,41,conferencePaper,AAMAS'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS,2016,US,Emotional empathy,Social behavior,Game; Visual,Cognitive Test,h,Custom,Robot,HRI ; Human relations,Machine learning