"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"LGYTD4GC","conferencePaper","2020","Abate, Andrea F.; Castiglione, Aniello; Nappi, Michele; Passero, Ignazio","DELEX: A DEep Learning Emotive EXperience: Investigating Empathic HCI","Proceedings of the International Conference on Advanced Visual Interfaces","978-1-4503-7535-1","","10.1145/3399715.3399820","https://doi.org/10.1145/3399715.3399820","Recent advances in Machine Learning have unveiled interesting possibilities for real-time investigating about user characteristics and expressions like, but not limited to, age, sex, body posture, emotions and moods. These new opportunities lay the foundations for new HCI tools for interactive applications that adopt user emotions as a communication channel.This paper presents an Emotion Controlled User Experience that changes according to user feelings and emotions analysed at runtime. Aiming at obtaining a preliminary evaluation of the proposed ecosystem, a controlled experiment has been performed in an engineering and software development company, where 60 people have been involved as volunteers. The subjective evaluation has been based on a standard questionnaire commonly adopted for measuring user perceived sense of immersion in Virtual Environments. The results of the controlled experiment encourage further investigations strengthen by the analysis of objective performance measurements and user physiological parameters.","2020","2021-05-19 12:52:09","2021-05-19 12:52:09","","","","","","","","","AVI '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Salerno, Italy","","","","Computer Vision; Deep Learning; User Emotions; User Experience","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MJMNC7X8","conferencePaper","2020","Daher, Karl; Casas, Jacky; Khaled, Omar Abou; Mugellini, Elena","Empathic Chatbot Response for Medical Assistance","Proceedings of the 20th ACM International Conference on Intelligent Virtual Agents","978-1-4503-7586-3","","10.1145/3383652.3423864","https://doi.org/10.1145/3383652.3423864","Is it helpful for a medical physical health chatbot to show empathy? How can a chatbot show empathy only based on short-term text conversations? We have investigated these questions by building two different medical assistant chatbots with the goal of providing a diagnosis for physical health problem to the user based on a short conversation. One chatbot was advice-only and asked only the necessary questions for the diagnosis without responding to the user's emotions. Another chatbot, capable of showing empathy, responded in a more supportive manner by analyzing the user's emotions and generating appropriate responses with a high empathic accuracy. Using the RoPE scale questionnaire for empathy perception in a human-robot interaction, our empathic chatbot was rated significantly better in showing empathy and was preferred by a majority of the preliminary study participants (N=12).","2020","2021-05-19 12:52:09","2021-05-19 12:52:09","","","","","","","","","IVA '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Virtual Event, Scotland, UK","","","","empathy; conversational agent; emotion detection; healthcare computing; pattern matching","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LVKN5RHR","conferencePaper","2015","Ribeiro, Tiago; Alves-Oliveira, Patrícia; Di Tullio, Eugenio; Petisca, Sofia; Sequeira, Pedro; Deshmukh, Amol; Janarthanam, Srinivasan; Foster, Mary Ellen; Jones, Aidan; Corrigan, Lee J.; Papadopoulos, Fotios; Hastie, Helen; Aylett, Ruth; Castellano, Ginevra; Paiva, Ana","The Empathic Robotic Tutor: Featuring the NAO Robot","Proceedings of the Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction Extended Abstracts","978-1-4503-3318-4","","10.1145/2701973.2702100","https://doi.org/10.1145/2701973.2702100","We present an autonomous empathic robotic tutor to be used in classrooms as a peer in a virtual learning environment. The system merges a virtual agent design with HRI features, consisting of a robotic embodiment, a multimedia interactive learning application and perception sensors that are controlled by an artificial intelligence agent.","2015","2021-05-19 12:52:09","2021-05-19 12:52:09","","285","","","","","","","HRI'15 Extended Abstracts","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Portland, Oregon, USA","","","","educational robotics; empathic robot","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KL3SYL83","journalArticle","2020","Zhou, Li; Gao, Jianfeng; Li, Di; Shum, Heung-Yeung","The Design and Implementation of XiaoIce, an Empathetic Social Chatbot","Comput. Linguist.","","0891-2017","10.1162/coli_a_00368","https://doi.org/10.1162/coli_a_00368","This article describes the development of Microsoft XiaoIce, the most popular social chatbot in the world. XiaoIce is uniquely designed as an artifical intelligence companion with an emotional connection to satisfy the human need for communication, affection, and social belonging. We take into account both intelligent quotient and emotional quotient in system design, cast human–machine social chat as decision-making over Markov Decision Processes, and optimize XiaoIce for long-term user engagement, measured in expected Conversation-turns Per Session (CPS). We detail the system architecture and key components, including dialogue manager, core chat, skills, and an empathetic computing module. We show how XiaoIce dynamically recognizes human feelings and states, understands user intent, and responds to user needs throughout long conversations. Since the release in 2014, XiaoIce has communicated with over 660 million active users and succeeded in establishing long-term relationships with many of them. Analysis of large-scale online logs shows that XiaoIce has achieved an average CPS of 23, which is significantly higher than that of other chatbots and even human conversations.","2020-03","2021-05-19 12:52:09","2021-05-19 12:52:09","","53–93","","1","46","","","","","","","","","","","","","","","","","Place: Cambridge, MA, USA Publisher: MIT Press","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3ACIMUBE","conferencePaper","2019","Roth, Daniel; Bloch, Carola; Schmitt, Josephine; Frischlich, Lena; Latoschik, Marc Erich; Bente, Gary","Perceived Authenticity, Empathy, and Pro-Social Intentions Evoked through Avatar-Mediated Self-Disclosures","Proceedings of Mensch Und Computer 2019","978-1-4503-7198-8","","10.1145/3340764.3340797","https://doi.org/10.1145/3340764.3340797","Avatars are our digital embodied alter egos. Virtual embodiment by avatars allows social interaction with others using the full spectrum of verbal and non-verbal behaviour. Still, one's avatar appearances is elective. Hence, avatars make it possible for users to discuss and exchange sensible or even problematic personal topics potentially hiding their real identity and hence preserving anonymity and privacy. While previous works identified similarities how participants perceive avatars compared to human stimuli, there is a question as to whether avatar-mediated self-disclosure is authentic and results in similar social responses. In the present study, we created a comparable stimulus set to investigate this issue and conducted an online study (N=172) for comparison. Our results indicate that avatars can be perceived as authentic and that empathy is attributed in similar level than to a human stimulus. In an exploratory model, we found that for in the overall results, authenticity fostered emotional empathy which in turn fostered pro-social intentions. We argue that avatars may serve as a valuable supporting medium for HCI applications related to mental well-being, self-disclosure, and support.","2019","2021-05-19 12:52:09","2021-05-19 12:52:09","","21–30","","","","","","","MuC'19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Hamburg, Germany","","","","Empathy; Avatars; Social Perception; Virtual Characters","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RBQBP3C4","conferencePaper","2019","Salaken, Syed Moshfeq; Nahavandi, Saeid; McGinn, Conor; Hossny, Mohammed; Kelly, Kevin; Abobakr, Ahmed; Nahavandi, Darius; Iskander, Julie","Development of a Cloud-Based Computational Framework for an Empathetic Robot","Proceedings of the 2019 11th International Conference on Computer and Automation Engineering","978-1-4503-6287-0","","10.1145/3313991.3314018","https://doi.org/10.1145/3313991.3314018","This article presents the development and preliminary evaluation of an empathy controlled robot. Such a robot is one step forward towards industry 5.0, as it provides a theoretical framework to enable the performance of the robot to be customized to suit the needs of both the task as well as the operator. An inventive step is taken through the separation of computational resources based on whether the algorithms are addressing functional or experiential needs. The paper therefore addresses the requirement for new approaches that can be employed in the design of mobile robots to reduce cost, power consumption, and computational burden of the system. We propose that tasks requiring real-time and safety critical control are processed using dedicated on-board computers, whereas functionality dedicated to system optimization, machine learning and customization are handled through use a cloud-based platform. In this paper, key components of the architecture are defined, and the development and preliminary evaluation of an exemplar robot capable of changing its behavior in accordance with the perceived emotional state of an operator's voice is presented.","2019","2021-05-19 12:52:09","2021-05-19 12:52:09","","102–108","","","","","","","ICCAE 2019","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Perth, WN, Australia","","","","deep learning; robot; cloud control; emotion classification; intent perception","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UA62QC48","conferencePaper","2017","Murphy, Dooley","Building a Hybrid Virtual Agent for Testing User Empathy and Arousal in Response to Avatar (Micro-)Expressions","Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology","978-1-4503-5548-3","","10.1145/3139131.3141217","https://doi.org/10.1145/3139131.3141217","This poster paper describes a hybrid (i.e., film and CG) method for capturing and implementing facial expressions for/in VR. A video camera was used to capture an actor's performance. The actor's eyes and mouth were isolated, and footage was processed as movie textures to overlay a static 3D model of a head. Micro-expressions (subtle, rapid movements of muscles in and around the eyes and mouth in particular) are thus captured in a fine-grained, yet low- cost and low-tech alternative to established techniques. A future experiment will compare the emotive efficacy of the hybrid virtual agent with that of a conventional (fully CG) rigged avatar head in a 6DoF scenario that transitions from sympathetic (gauging empathy by self-report) to confrontational (gauging physiological arousal by heart-rate or GSR). The experiment's prospective design is discussed, as well as its significance for the study of the crucial intersection of social plausibility and perceptual realism in VR.","2017","2021-05-19 12:52:09","2021-05-19 12:52:09","","","","","","","","","VRST '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Gothenburg, Sweden","","","","virtual reality; social presence; avatar capture; social plausibility","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2HFN32JD","journalArticle","2019","Alves-Oliveira, Patrícia; Sequeira, Pedro; Melo, Francisco S.; Castellano, Ginevra; Paiva, Ana","Empathic Robot for Group Learning: A Field Study","J. Hum.-Robot Interact.","","","10.1145/3300188","https://doi.org/10.1145/3300188","This work explores a group learning scenario with an autonomous empathic robot. We address two research questions: (1) Can an autonomous robot designed with empathic competencies foster collaborative learning in a group context? (2) Can an empathic robot sustain positive educational outcomes in long-term collaborative learning interactions with groups of students? To answer these questions, we developed an autonomous robot with empathic competencies that is able to interact with a group of students in a learning activity about sustainable development. Two studies were conducted. The first study compares learning outcomes in children across three conditions: learning with an empathic robot; learning with a robot without empathic capabilities; and learning without a robot. The results show that the autonomous robot with empathy fosters meaningful discussions about sustainability, which is a learning outcome in sustainability education. The second study features groups of students who interact with the robot in a school classroom for 2 months. The long-term educational interaction did not seem to provide significant learning gains, although there was a change in game-actions to achieve more sustainability during game-play. This result reflects the need to perform more long-term research in the field of educational robots for group learning.","2019-03","2021-05-19 12:52:09","2021-05-19 12:52:09","","","","1","8","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","education; empathy; human-robot interaction; collaborative learning; group learning; learning gains; Social robotics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q5KE4RJT","conferencePaper","2019","Franzoni, Valentina; Milani, Alfredo; Biondi, Giulio; Micheli, Francesco","A Preliminary Work on Dog Emotion Recognition","IEEE/WIC/ACM International Conference on Web Intelligence - Companion Volume","978-1-4503-6988-6","","10.1145/3358695.3361750","https://doi.org/10.1145/3358695.3361750","Humans react to animal emotions, and animals react to human emotions because we share similar emotional and neurological mirroring systems. Mirror neurons fire both when an animal performs an action and when the animal observes the same action performed by another individual. This neurological system has been linked to social behaviors and abilities, from empathy to learning by imitation, both in intra-species and in inter-species communications.The aim of this paper is to study if a machine learning system can recognize animal emotions, starting from dogs’ basic emotions of joy and anger, and to investigate the opportunity of future applications concerning systems of prosthetic knowledge to help people without the proper experience or capability to understand animals aggressivity or friendliness, or for supportive systems in Artificial Intelligence.","2019","2021-05-19 12:52:09","2021-05-19 12:52:09","","91–96","","","","","","","WI '19 Companion","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Thessaloniki, Greece","","","","Affective Computing; Artificial Intelligence; Emotion Recognition; Neural Networks; Transfer Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H9JZBU7L","conferencePaper","2021","Billinghurst, Mark","Empathic Computing and Human Robot Interaction","Proceedings of the 2021 ACM/IEEE International Conference on Human-Robot Interaction","978-1-4503-8289-2","","10.1145/3434073.3444642","https://doi.org/10.1145/3434073.3444642","Empathic Computing is an emerging research field that aims to use technology to create deeper shared understanding or empathy between people [1]. The field sits at the junction of research in Natural Collaboration, Experience Capture and Implicit Understanding. Technologies such as Augmented Reality (AR) and Virtual Reality (VR), can be combined with the sensing of human physiological signals to create new types of collaborative experiences. For example, Empathy Glasses [2] use gaze- and face-tracking to share non-verbal communication cues and enhance remote collaborative. More complex tools, such as EEG, can measure brain activity synchronization and physiological states not normally perceived by humans [3].This talk explores how lessons learned from Empathic Computing can be applied to field of Human Robot Interaction. Previous research has shown how humans can develop empathy for robots [4], and how robots can be used as telepresence surrogates for real people [5]. This shows that there is great potential to create robot mediated Empathic Computing experiences to enhance face to face and remote collaboration.","2021","2021-05-19 12:52:09","2021-05-19 12:52:09","","5","","","","","","","HRI '21","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Boulder, CO, USA","","","","virtual reality; augmented reality; empathic computing; human robot interaction; tele-existence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2WVQ98JH","conferencePaper","2016","Hall, Lynne; Hume, Colette; Tazzyman, Sarah; Deshmukh, Amol; Janarthanam, Srinivasan; Hastie, Helen; Aylett, Ruth; Castellano, Ginevra; Papadopoulos, Fotis; Jones, Aidan; Corrigan, Lee J.; Paiva, Ana; Alves Oliveira, Patrícia; Ribeiro, Tiago; Barendregt, Wolmet; Serholt, Sofia; Kappas, Arvid","Map Reading with an Empathic Robot Tutor","The Eleventh ACM/IEEE International Conference on Human Robot Interaction","978-1-4673-8370-7","","","","In this video submission, we describe a scenario developed in the EMOTE project. The overall goal of the EMOTE project is to develop an empathic robot tutor for 11-13 year old school students in an educational setting. The pedagogical domain here is to assist students in learning and testing their map-reading skills typically learned as part of the geography curriculum in schools. We show this scenario with a NAO robot interacting with the students whilst performing map-reading tasks on a touch-screen device in this video.","2016","2021-05-19 12:52:09","2021-05-19 12:52:09","","567","","","","","","","HRI '16","","","","IEEE Press","","","","","","","","","event-place: Christchurch, New Zealand","","","","empathy; robot-child interaction; robotic tutor","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YU4SDZFA","conferencePaper","2015","Seo, Stela H.; Geiskkovitch, Denise; Nakane, Masayuki; King, Corey; Young, James E.","Poor Thing! Would You Feel Sorry for a Simulated Robot? A Comparison of Empathy toward a Physical and a Simulated Robot","Proceedings of the Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction","978-1-4503-2883-8","","10.1145/2696454.2696471","https://doi.org/10.1145/2696454.2696471","In designing and evaluating human-robot interactions and interfaces, researchers often use a simulated robot due to the high cost of robots and time required to program them. However, it is important to consider how interaction with a simulated robot differs from a real robot; that is, do simulated robots provide authentic interaction? We contribute to a growing body of work that explores this question and maps out simulated-versus-real differences, by explicitly investigating empathy: how people empathize with a physical or simulated robot when something bad happens to it. Our results suggest that people may empathize more with a physical robot than a simulated one, a finding that has important implications on the generalizability and applicability of simulated HRI work. Empathy is particularly relevant to social HRI and is integral to, for example, companion and care robots. Our contribution additionally includes an original and reproducible HRI experimental design to induce empathy toward robots in laboratory settings, and an experimentally validated empathy-measuring instrument from psychology for use with HRI.","2015","2021-05-19 12:52:09","2021-05-19 12:52:09","","125–132","","","","","","","HRI '15","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Portland, Oregon, USA","","","","empathy; human-robot interaction; robot embodiment; simulated interaction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N83EQZ6Y","book","2020","","HAI '20: Proceedings of the 8th International Conference on Human-Agent Interaction","","978-1-4503-8054-6","","","","It is our great pleasure to welcome you to the Eighth International Conference on Human-Agent Interaction HAI 2020 (Virtual Conference); hosted by the Western Sydney University (Australia) and supported by Chalmers University of Technology (Sweden).The conference is a venue with an interdisciplinary nature to discuss and disseminate state-ofthe- art research on topics related to human interactions with a range of agent systems, including physical robots and humanoids, virtual agents, socially interactive agents, and Artificially Intelligent (AI) agents. The topical areas of the conference include user studies, frameworks, simulations, technical developments and more within Human Agent and Robotic Interaction. The conference brings together a large variety of multidisciplinary research groups, companies, and researchers looking into the broader area of agents and robotics across Australia, Japan and the rest of the world.The theme for HAI 2020 is ""Artificial Intelligence + Experience Design."" The recent advent of AI has motivated researchers to focus on several algorithmic prospects in developing intelligent robotic agents and their interactions. Progressively, AI advances are leading to exciting outcomes in the HAI field and, at the same time, are opening up for a wide perspective on how to design intelligent robotic agents. For example, how to combine artificial intelligence and user experience design approaches in human-agent interaction. We are looking forward to sharing the latest research results of HAI that contribute a broad range of disciplines.Three keynote talks are featured. The first is titled ""We're in This Together: Social Robots in Group, Organizational, and Community Interactions"", by Associate Prof. Selma Šabanović, Indiana University Bloomington, USA. The second is titled ""What kind of human-centric robotics do we need? Investigations from human-robot interactions in socially assistive scenarios"", by Prof. Ginevra Castellano, Uppsala University, Sweden. The third is an industry talk titled ""The rapid rise in drone technology"", by Sebastian Robertson, CEO of BIRDI, Australia. Their keynote talks will provide cross-disciplinary examples of novel HAI research and applications that are highly inspiring for the HAI audience and research community.This year's submissions have come from more than 25 countries and cover leading-edge topics including human and machine learning, conversational agents, empathy and trust of social robots, social drones, social presence, robot applications, virtual agent applications and novel perspectives of HAI. With an acceptance rate of 38% (25 papers out of 65 submissions), the program committee has again set a high quality standard. In addition, 26 out of the 35 latebreaking poster papers submissions were accepted.","2020","2021-05-19 12:52:09","2021-05-19 12:52:09","","","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5C99JYP7","conferencePaper","2021","Burns, Rachael Bevill; Seifi, Hasti; Lee, Hyosang; Kuchenbecker, Katherine J.","A Haptic Empathetic Robot Animal for Children with Autism","Companion of the 2021 ACM/IEEE International Conference on Human-Robot Interaction","978-1-4503-8290-8","","10.1145/3434074.3446352","https://doi.org/10.1145/3434074.3446352","Children with autism and their families could greatly benefit from increased support resources. While robots are already being introduced into autism therapy and care, we propose that these robots could better understand the child's needs and provide enriched interaction if they utilize touch. We present our plans, both completed and ongoing, for a touch-perceiving robot companion for children with autism. We established and validated touch-perception requirements for an ideal robot companion through interviews with 11 autism specialists. Currently, we are evaluating custom fabric-based tactile sensors that enable the robot to detect and identify various touch communication gestures. Finally, our robot companion will react to the child's touches through an emotion response system that will be customizable by a therapist or caretaker.","2021","2021-05-19 12:52:09","2021-05-19 12:52:09","","583–585","","","","","","","HRI '21 Companion","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Boulder, CO, USA","","","","socially assistive robotics; robot-assisted therapy; tactile sensors","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CNJGJPUS","conferencePaper","2019","Okanda, Mako; Taniguchi, Kosuke; Itakura, Shoji","The Role of Animism Tendencies and Empathy in Adult Evaluations of Robot","Proceedings of the 7th International Conference on Human-Agent Interaction","978-1-4503-6922-0","","10.1145/3349537.3351891","https://doi.org/10.1145/3349537.3351891","We investigated whether Japanese adults' beliefs about friendship and morality toward robots differing in appearance (i.e., humanoid, dog-like, and egg-shaped) related to their animism tendencies and empathy. University students responded to questionnaires regarding three animism tendencies (i.e., general animism or a tendency to believe souls or gods in nonliving things, aliveness animism or a tendency to consider nonliving things as live entities, and agentic animisms or a tendency to attribute biological, artifactual, psychological, perceptual, and naming properties) and empathy. We found that friendship and morality were related to slightly different animism tendencies and empathy even though they shared some major factors. Aliveness animism, as well as a tendency to attribute perceptual and name properties toward robots, might be necessary for an individual to believe that robots could be social agents. Participants who responded that robots could be their friends showed a tendency to feel a soul in manmade objects and a strong self-oriented emotional reactivity, whereas participants who answered that robots were moral beings showed a tendency to exhibit strong emotional susceptibility. We discuss implications of these results and reasons why people feel that robots have a mind or consciousness.","2019","2021-05-19 12:52:09","2021-05-19 12:52:09","","51–58","","","","","","","HAI '19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Kyoto, Japan","","","","empathy; human-robot interaction; animism; robots' perception","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZXJ8TCRC","conferencePaper","2020","Toxtli, Carlos; Richmond-Fuller, Angela; Savage, Saiph","Reputation Agent: Prompting Fair Reviews in Gig Markets","Proceedings of The Web Conference 2020","978-1-4503-7023-3","","10.1145/3366423.3380199","https://doi.org/10.1145/3366423.3380199","Our study presents a new tool, Reputation Agent, to promote fairer reviews from requesters (employers or customers) on gig markets. Unfair reviews, created when requesters consider factors outside of a worker’s control, are known to plague gig workers and can result in lost job opportunities and even termination from the marketplace. Our tool leverages machine learning to implement an intelligent interface that: (1) uses deep learning to automatically detect when an individual has included unfair factors into her review (factors outside the worker’s control per the policies of the market); and (2) prompts the individual to reconsider her review if she has incorporated unfair factors. To study the effectiveness of Reputation Agent, we conducted a controlled experiment over different gig markets. Our experiment illustrates that across markets, Reputation Agent, in contrast with traditional approaches, motivates requesters to review gig workers’ performance more fairly. We discuss how tools that bring more transparency to employers about the policies of a gig market can help build empathy thus resulting in reasoned discussions around potential injustices towards workers generated by these interfaces. Our vision is that with tools that promote truth and transparency we can bring fairer treatment to gig workers.","2020","2021-05-19 12:52:09","2021-05-19 12:52:09","","1228–1240","","","","","","","WWW '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Taipei, Taiwan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XIFDV5L4","conferencePaper","2018","Hu, Tianran; Xu, Anbang; Liu, Zhe; You, Quanzeng; Guo, Yufan; Sinha, Vibha; Luo, Jiebo; Akkiraju, Rama","Touch Your Heart: A Tone-Aware Chatbot for Customer Care on Social Media","Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems","978-1-4503-5620-6","","10.1145/3173574.3173989","https://doi.org/10.1145/3173574.3173989","Chatbot has become an important solution to rapidly increasing customer care demands on social media in recent years. However, current work on chatbot for customer care ignores a key to impact user experience - tones. In this work, we create a novel tone-aware chatbot that generates toned responses to user requests on social media. We first conduct a formative research, in which the effects of tones are studied. Significant and various influences of different tones on user experience are uncovered in the study. With the knowledge of effects of tones, we design a deep learning based chatbot that takes tone information into account. We train our system on over 1.5 million real customer care conversations collected from Twitter. The evaluation reveals that our tone-aware chatbot generates as appropriate responses to user requests as human agents. More importantly, our chatbot is perceived to be even more empathetic than human agents.","2018","2021-05-19 12:52:09","2021-05-19 12:52:09","","1–12","","","","","","","CHI '18","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Montreal QC, Canada","","","","social media; deep learning; chatbot; customer care","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4C52JT4C","conferencePaper","2020","Harilal, Nidhin; Shah, Rushil; Sharma, Saumitra; Bhutani, Vedanta","CARO: An Empathetic Health Conversational Chatbot for People with Major Depression","Proceedings of the 7th ACM IKDD CoDS and 25th COMAD","978-1-4503-7738-6","","10.1145/3371158.3371220","https://doi.org/10.1145/3371158.3371220","There has been a rise in the number of patients suffering from major depression over the past decade. Most of the patients are reluctant and do not open up for councelling services. Conversational applications such as chatbots have been found efficient in overcoming alcohol addiction. Effective treatments can tackle depression, but only 10% of affected patients are able to avail such treatments mainly due to lack of resources and social stigma associated with mental disorders. We propose CARO, a chatbot app, which is capable of performing empathetic conversations and providing medical advice for people with major depression. CARO will be able to sense the conversational context, its intent and the associated emotions.","2020","2021-05-19 12:52:10","2021-05-19 12:52:10","","349–350","","","","","","","CoDS COMAD 2020","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Hyderabad, India","","","","Chatbot; Depression; Empathetic Response; Medical Advice","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NRPTBT8M","conferencePaper","2016","Liu, Xin; London, Kati","T.A.I: A Tangible AI Interface to Enhance Human-Artificial Intelligence (AI) Communication Beyond the Screen","Proceedings of the 2016 ACM Conference on Designing Interactive Systems","978-1-4503-4031-1","","10.1145/2901790.2901896","https://doi.org/10.1145/2901790.2901896","Social and emotional intelligence of computer systems is increasingly important in human-AI (Artificial Intelligence) interactions. This paper presents a tangible AI interface, T.A.I, that enhances physical engagement in digital communication between users and a conversational AI agent. We describe a compact, pneumatically shape-changing hardware design with a rich set of physical gestures that actuate on mobile devices during real-time conversations. Our user study suggests that the physical presence provided by T.A.I increased users' empathy for, and social connection with the virtual intelligent system, leading to an improved Human-AI communication experience.","2016","2021-05-19 12:52:10","2021-05-19 12:52:10","","281–285","","","","","","","DIS '16","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Brisbane, QLD, Australia","","","","affective communication; shape-changing interface; social agent; tangible interface","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZISLTLJ5","conferencePaper","2015","Deshmukh, Amol; Jones, Aidan; Janarthanam, Srinivasan; Foster, Mary Ellen; Ribeiro, Tiago; Corrigan, Lee Joseph; Aylett, Ruth; Paiva, Ana; Papadopoulos, Fotios; Castellano, Ginevra","Empathic Robotic Tutors: Map Guide","Proceedings of the Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction Extended Abstracts","978-1-4503-3318-4","","10.1145/2701973.2702693","https://doi.org/10.1145/2701973.2702693","In this demonstration we describe a scenario developed in the EMOTE project. The overall goal of the project is to develop an empathic robot tutor for 11-13 year old school students in an educational setting. We are aiming to develop an empathic robot tutor to teach map reading skills with this scenario on a touch-screen device.","2015","2021-05-19 12:52:10","2021-05-19 12:52:10","","311","","","","","","","HRI'15 Extended Abstracts","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Portland, Oregon, USA","","","","empathy; human-robot interaction; robotic tutors","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CZRFWA4S","conferencePaper","2019","Jean-Charles, Norah; Haas, Gregor; Drennan, Alex","Using Machine Learning to Convey Emotions During Requirements Elicitation Interviews","Proceedings of the 2019 ACM Southeast Conference","978-1-4503-6251-1","","10.1145/3299815.3314484","https://doi.org/10.1145/3299815.3314484","In an effort to assist in an ongoing research project where stakeholders have been interviewed using voice recording platforms and the Empatica E4 wristband to gather biofeedback data, the purpose of this research project in relation to the aforementioned research is to be able to determine the stakeholder's emotional range during a requirements elicitation interview. In doing so, the requirements analyst is supported during the requirements elicitation interview because conveying the emotional range of the stakeholder can help eliminate any miscommunication or misunderstandings between the requirements analyst and the stakeholder due to ambiguity in questions, statements, and responses. Therefore, knowing the range of the emotional state of the stakeholder, in real-time, can allow the requirements analyst to recover or make adjustments to questions (i.e. sensitive topics) during the requirements elicitation interview. The main questions are: (1) What machine learning technique(s) would be most efficient in conveying the emotional range of the stakeholder through the voice recording data and biofeedback data? (2) What features would result in optimal performance from the chosen machine learning technique(s)? The objective is to use supervised machine learning techniques in order to convey an emotional range from the retrieved dataset. To accomplish this, exploring the most efficient machine learning technique for emotion detection for voice recordings and biofeedback data and finding a way to construct or utilize the techniques in an effective manner will be necessary. In conclusion, the machine learning technique(s) chosen will convey a range of emotion from the stakeholder based on the retrieved data. The machine learning technique(s) will be used to support a requirements analyst during requirements elicitation interviews and will help the analyst identify problems or concerns in the communication to better assess and engage the stakeholder.","2019","2021-05-19 12:52:10","2021-05-19 12:52:10","","266–267","","","","","","","ACM SE '19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Kennesaw, GA, USA","","","","Neural Networks; Machine Learning; Biofeedback Data; Datasets; Regression; Voice Recording Data","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4TY29WYT","conferencePaper","2017","Thompson, Jeff","I Touch You and You Touch Me","SIGGRAPH Asia 2017 Art Gallery","978-1-4503-5401-1","","10.1145/3143748.3143753","https://doi.org/10.1145/3143748.3143753","A robotic arm plays back hallucinated gestures from a machine learning system trained on my interactions with my phone, exploring issues of human/machine empathy and agency.","2017","2021-05-19 12:52:10","2021-05-19 12:52:10","","","","","","","","","SA '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Bangkok, Thailand","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HKR88ETR","conferencePaper","2019","Shvo, Maayan","Towards Empathetic Planning and Plan Recognition","Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society","978-1-4503-6324-2","","10.1145/3306618.3314307","https://doi.org/10.1145/3306618.3314307","Every compassionate and functioning society requires its members to have a capacity to adopt others' perspectives. As Artificial Intelligence (AI) systems are given increasingly sensitive and impactful roles in society, it is important to enable AI to wield empathy as a tool to benefit those it interacts with. In this paper, we work towards this goal by bringing together a number of important concepts: empathy, AI planning, and plan recognition (i.e., the problem of inferring an actor's plan and goal given observations about its behavior). We formalize the notions of Empathetic Planning and Empathetic Plan Recognition which are informed by the beliefs and affective state of the actor, and propose AI planning-based computational approaches. We illustrate the benefits of our approach by conducting a study with human participants.","2019","2021-05-19 12:52:10","2021-05-19 12:52:10","","525–526","","","","","","","AIES '19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Honolulu, HI, USA","","","","AI planning; plan recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6JC7UN9R","conferencePaper","2015","De Carolis, Berardina; Ferilli, Stefano; Palestra, Giuseppe; Carofiglio, Valeria","Modeling and Simulating Empathic Behavior in Social Assistive Robots","Proceedings of the 11th Biannual Conference on Italian SIGCHI Chapter","978-1-4503-3684-0","","10.1145/2808435.2808445","https://doi.org/10.1145/2808435.2808445","Several studies report successful results on how social assistive robots can be employed as interface in the assisted living domain. In our opinion, to plan their response and interact successfully with people, it is crucial to recognize human emotions. To this aim, features of the prosody of the speech together with facial expressions and gestures may be used to recognize the emotional state of the user. The information gained from these different sources may be fused in order to endow the robot with the capability to reason on the user's affective state. In this paper we describe how this capability has been implemented in the NAO robot and how this allows simulating empathic behaviors in the context of Ambient Assisted Living.","2015","2021-05-19 12:52:10","2021-05-19 12:52:10","","110–117","","","","","","","CHItaly 2015","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Rome, Italy","","","","Affective Computing; Social Assistive Robots","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WRWUFXT6","journalArticle","2015","Leite, Iolanda","Long-Term Interactions with Empathic Social Robots","AI Matters","","","10.1145/2735392.2735397","https://doi.org/10.1145/2735392.2735397","We investigated the effects of an adaptive empathic model in repeated interactions between users and social robots. The proposed model includes an online learning decision-making mechanism that allows the robot to select the most appropriate supportive behaviors based on the impact that similar behaviors had in keeping the user in a positive affective state.","2015-03","2021-05-19 12:52:10","2021-05-19 12:52:10","","13–15","","3","1","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9VHMJUZ4","conferencePaper","2019","Antle, Alissa N.; Sadka, Ofir; Radu, Iulian; Gong, Boxiao; Cheung, Victor; Baishya, Uddipana","EmotoTent: Reducing School Violence through Embodied Empathy Games","Proceedings of the 18th ACM International Conference on Interaction Design and Children","978-1-4503-6690-8","","10.1145/3311927.3326596","https://doi.org/10.1145/3311927.3326596","EmotoTent is an interactive socio-emotional learning system developed in response to escalating levels of violence, inequality and marginalization in schools seen in the early 21st Century. The system is inspired by advances in biosensing wearables, tattoo displays, brain sensors, robotic agents, artificial intelligence (AI), gestural interaction and 3D holographic displays. By 2030, technological advances will enable us to prototype and investigate questions related to experiential and embodied emotional learning; emotion-based human-computer interaction, affective biosensing, empathetic AI agents, and 3D interactive holographic environments. We envision EmotoTent as a modular, emotion-sensing Holodeck. In the EmotoTent program children learn and practice emotion regulation and empathy with peers, pets and a robotic dog agent in ways that are experiential, embodied and playful. We propose EmotoTent as a core element of a K-6 socio-emotional learning curriculum designed to improve school culture through the enhancement of children's ability to regulate emotions and interact with human and non-human species with empathy and compassion. Enhancing these qualities has been shown to lead to reductions in violence and bullying, racism, gender inequality and other forms of marginalization. We predict that the EmotoTent socio-emotional learning program will improve school cultures and create a foundation for children's lifelong well-being.","2019","2021-05-19 12:52:10","2021-05-19 12:52:10","","755–760","","","","","","","IDC '19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Boise, ID, USA","","","","biosensing; children; embodied learning; Empathy; experiential learning; holographic environments; mind-body interaction; robotic agents","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TMB9JQ9T","conferencePaper","2020","Patro, Jasabanta; Rathore, Pushpendra Singh","A Sociolinguistic Route to the Characterization and Detection of the Credibility of Events on Twitter","Proceedings of the 31st ACM Conference on Hypertext and Social Media","978-1-4503-7098-1","","10.1145/3372923.3404795","https://doi.org/10.1145/3372923.3404795","Although Twitter constitutes as one of the primary sources of real-time news with users acting as the sensors updating the content from all across the globe, yet the spread of rumours via Twitter is becoming an increasingly alarming issue and is known to have caused significant damage already. We propose a credibility analysis approach based on the linguistic structure of the tweets. We not only characterize the Twitter events but also predict their perceived credibility of them by a novel deep learning architecture. We use the huge CREDBANK data to conduct our experiments. Some of our exciting findings are that standard LIWC categories like 'negate', 'discrep', 'cogmech', 'swear' and the Empath categories like 'hate', 'poor', 'government', 'worship' and 'swearing-terms' correlate negatively with the credibility of events. While some of our results resonate with the earlier literature others represent novel insights of the fake and legitimate twitter events. Using the above observations and the current deep learning architecture we predict the credibility of an event (a four-class classification problem in our case) with an accuracy of 0.54 that improves the best-known state-of-the-art (current accuracy 0.43) by 26%. A fascinating observation is that even by looking at the first few tweets of an event, it is possible to make the prediction almost as accurate as in the case where the entire volume of tweets is observed.","2020","2021-05-19 12:52:10","2021-05-19 12:52:10","","241–250","","","","","","","HT '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Virtual Event, USA","","","","credibility detection; event credibility; sociolinguistic approach","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4ZGDR4UB","conferencePaper","2015","Deshmukh, Amol; Jones, Aidan; Janarthanam, Srinivasan; Hastie, Helen; Ribeiro, Tiago; Aylett, Ruth; Paiva, Ana; Castellano, Ginevra; Ellen Foster, Mary; Corrigan, Lee J.; Papadopoulos, Fotios; Di Tullio, Eugenio; Sequeira, Pedro","An Empathic Robotic Tutor in a Map Application","Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems","978-1-4503-3413-6","","","","In this demonstration, we describe a scenario developed in the EMOTE project [2]. The overall goal of the EMOTE project is to develop an empathic robot tutor for 11-13 year old school students in an educational setting. The pedagogical domain we demonstrate here is to assist students in learning and testing their map-reading skills typically learned as part of the geography curriculum in schools. We demonstrate this scenario with a NAO robot interacting with the students whilst performing map-reading tasks in the form of a game on a touch-screen device.","2015","2021-05-19 12:52:10","2021-05-19 12:52:10","","1923–1924","","","","","","","AAMAS '15","","","","International Foundation for Autonomous Agents and Multiagent Systems","Richland, SC","","","","","","","","event-place: Istanbul, Turkey","","","","empathy; human-robot interaction; robotic tutors","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P49Z2K6Q","conferencePaper","2019","Aubergé, Véronique","The Socio-Affective Robot: Aimed to Understand Human Links?","Proceedings of the 9th International on Audio/Visual Emotion Challenge and Workshop","978-1-4503-6913-8","","10.1145/3347320.3357687","https://doi.org/10.1145/3347320.3357687","Is the social robot a product of artificial intelligence or is it a perception product by our natural intelligence, revealing some crucial aspects of social and cultural human processing? Among the smart objects, the social robot cannot be distinguished by precise and well defined technical or morphological cues. Even though no serious and discriminative attributes can be given by any science knowledge – even the movement attribute, and the ""autonomous” cognitive attribute are not clearly defined – in order to understand how an object becomes, perceptively, a subject (social robot), it is a fact that the automatons and the talking artefacts are now named robot, which is particularly attractive for general public, for scientists and engineers. However, is it a socio-cultural desire or a technical need to add the augmentation of the social space to the ""augmented self” (self body and self environment abilities)?In this talk we will explore some social space perturbations in ecological conditions, such as elderly people suffering from isolation and interacting with a robot that can emit solely non-verbal speech primitives. Long term interactions were collected and analysed using the concepts of the Dynamic Affective Network for Social Entities (D.A.N.S.E.) theory. We will try to show that non-verbal speech primitives, organised in the D.A.N.S.E.'s ""glue” paradigm, permit to predict the relations with the robot perceived by elderly as oriented inside or outside dominance, but also to explore in particular an empathic dimension. Through a Living Lab method, evaluation of these hypotheses and building of an empathic socio-affective HRI were conducted together, within strong ethical constraints. In particular the development of frail robots for frail people will be proposed as possible ethical perspective.","2019","2021-05-19 12:52:10","2021-05-19 12:52:10","","1","","","","","","","AVEC '19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Nice, France","","","","engagement; frail person; hri; living lab; social robot; socio-affective speech","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QWWWX6R3","conferencePaper","2019","Charrier, Laurianne; Rieger, Alisa; Galdeano, Alexandre; Cordier, Amélie; Lefort, Mathieu; Hassas, Salima","The RoPE Scale: A Measure of How Empathic a Robot is Perceived","Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction","978-1-5386-8555-6","","","","To be accepted in our everyday life and to be valuable interaction partners, robots should be able to display emotional and empathic behaviors. That is why there has been a great focus on developing empathy in robots in recent years. However, there is no consensus on how to measure how much a robot is considered to be empathic. In this context, we decided to construct a questionnaire which specifically measures the perception of a robot's empathy in human-robot interaction (HRI). Therefore we conducted pretests to generate items. These were validated by experts and will be further validated in an experimental setting.","2019","2021-05-19 12:52:10","2021-05-19 12:52:10","","656–657","","","","","","","HRI '19","","","","IEEE Press","","","","","","","","","event-place: Daegu, Republic of Korea","","","","human-robot interaction; social robots; perceived empathy; psychometrics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7ZJG3IWU","conferencePaper","2017","Xu, Anbang; Liu, Zhe; Guo, Yufan; Sinha, Vibha; Akkiraju, Rama","A New Chatbot for Customer Service on Social Media","Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems","978-1-4503-4655-9","","10.1145/3025453.3025496","https://doi.org/10.1145/3025453.3025496","Users are rapidly turning to social media to request and receive customer service; however, a majority of these requests were not addressed timely or even not addressed at all. To overcome the problem, we create a new conversational system to automatically generate responses for users requests on social media. Our system is integrated with state-of-the-art deep learning techniques and is trained by nearly 1M Twitter conversations between users and agents from over 60 brands. The evaluation reveals that over 40% of the requests are emotional, and the system is about as good as human agents in showing empathy to help users cope with emotional situations. Results also show our system outperforms information retrieval system based on both human judgments and an automatic evaluation metric.","2017","2021-05-19 12:52:10","2021-05-19 12:52:10","","3506–3510","","","","","","","CHI '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Denver, Colorado, USA","","","","social media; deep learning; chatbot; customer service","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KXIZ5YRM","conferencePaper","2016","Fast, Ethan; Chen, Binbin; Bernstein, Michael S.","Empath: Understanding Topic Signals in Large-Scale Text","Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems","978-1-4503-3362-7","","10.1145/2858036.2858535","https://doi.org/10.1145/2858036.2858535","Human language is colored by a broad range of topics, but existing text analysis tools only focus on a small number of them. We present Empath, a tool that can generate and validate new lexical categories on demand from a small set of seed terms (like ""bleed"" and ""punch"" to generate the category violence). Empath draws connotations between words and phrases by deep learning a neural embedding across more than 1.8 billion words of modern fiction. Given a small set of seed words that characterize a category, Empath uses its neural embedding to discover new related terms, then validates the category with a crowd-powered filter. Empath also analyzes text across 200 built-in, pre-validated categories we have generated from common topics in our web dataset, like neglect, government, and social media. We show that Empath's data-driven, human validated categories are highly correlated (r=0.906) with similar categories in LIWC.","2016","2021-05-19 12:52:10","2021-05-19 12:52:10","","4647–4657","","","","","","","CHI '16","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: San Jose, California, USA","","","","NLP; fiction; social computing; computational social science","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ANNV7P8B","journalArticle","2017","Paiva, Ana; Leite, Iolanda; Boukricha, Hana; Wachsmuth, Ipke","Empathy in Virtual Agents and Robots: A Survey","ACM Trans. Interact. Intell. Syst.","","2160-6455","10.1145/2912150","https://doi.org/10.1145/2912150","This article surveys the area of computational empathy, analysing different ways by which artificial agents can simulate and trigger empathy in their interactions with humans. Empathic agents can be seen as agents that have the capacity to place themselves into the position of a user’s or another agent’s emotional situation and respond appropriately. We also survey artificial agents that, by their design and behaviour, can lead users to respond emotionally as if they were experiencing the agent’s situation. In the course of this survey, we present the research conducted to date on empathic agents in light of the principles and mechanisms of empathy found in humans. We end by discussing some of the main challenges that this exciting area will be facing in the future.","2017-09","2021-05-19 12:52:10","2021-05-19 12:52:10","","","","3","7","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","virtual agents; empathy; affective computing; human-computer interaction; human-robot interaction; social robots","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I879DM46","conferencePaper","2018","Roberts, Jasmine","Using Affective Computing for Proxemic Interactions in Mixed-Reality","Proceedings of the Symposium on Spatial User Interaction","978-1-4503-5708-1","","10.1145/3267782.3274692","https://doi.org/10.1145/3267782.3274692","Immersive technologies have been touted as empathetic mediums. This capability has yet to be fully explored through machine learning integration. Our demo seeks to explore proxemics in mixed-reality (MR) human-human interactions.The author developed a system, where spatial features can be manipulated in real time by identifying emotions corresponding to unique combinations of facial micro-expressions and tonal analysis. The Magic Leap One is used as the interactive interface, the first commercial spatial computing head mounted (virtual retinal) display (HUD).A novel spatial user interface visualization element is prototyped that leverages the affordances of mixed-reality by introducing both a spatial and affective component to interfaces.","2018","2021-05-19 12:52:10","2021-05-19 12:52:10","","176","","","","","","","SUI '18","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Berlin, Germany","","","","affective computing; augmented reality; mixed reality; Proxemics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5U8ML444","conferencePaper","2020","Sohrab, Fahad; Raitoharju, Jenni; Gabbouj, Moncef","Facial Expression Based Satisfaction Index for Empathic Buildings","Adjunct Proceedings of the 2020 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2020 ACM International Symposium on Wearable Computers","978-1-4503-8076-8","","10.1145/3410530.3414443","https://doi.org/10.1145/3410530.3414443","In this work, we examine the suitability of automatic facial expression recognition to be used for satisfaction analysis in an Empathic Building environment. We use machine learning based facial expression recognition on the working stations to integrate an online satisfaction index into Empathic Building platform. To analyze the suitability of facial expression recognition to reflect longer-term satisfaction, we examine the changes and trends in the happiness curves of our test users. We also correlate the happiness curve with temperature, humidity, and light intensity of the test users' local city (Tampere Finland). The results indicate that the proposed analysis indeed shows some trends that may be used for long-term satisfaction analysis in different kinds of intelligent buildings.","2020","2021-05-19 12:52:10","2021-05-19 12:52:10","","704–707","","","","","","","UbiComp-ISWC '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Virtual Event, Mexico","","","","machine learning; empathic building; facial expressions; satisfaction index","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AU4CZQ6U","conferencePaper","2020","Heljakka, Katriina Irja; Ihamäki, Pirita Johanna; Lamminen, Anu Inkeri","Playing with the Opposite of Uncanny: Empathic Responses to Learning with a Companion-Technology Robot Dog vs. Real Dog","Extended Abstracts of the 2020 Annual Symposium on Computer-Human Interaction in Play","978-1-4503-7587-0","","10.1145/3383668.3419900","https://doi.org/10.1145/3383668.3419900","Social robots are becoming increasingly common in the contexts of education and healthcare. This paper reports on the findings of the first stage of an exploratory study conducted with (n=16) Finnish preschoolers aged 5-7 years. The multidisciplinary study intertwining the areas of early education pedagogics, smart toys and interactive technologies, employed both a commercial robot dog and a real dog to study the potential of these artificial and living entities to support and facilitate social-emotional learning (SEL) through a guided playful learning approach. We performed a research intervention including facilitation, observation and video- recordings of three play sessions organized in March-May 2020. The preliminary findings indicate how guided playing with the robot dog supported SEL through conversation about human relationships, while interaction with the real dog facilitated empathic responses through spontaneous reactions on the animal's behavior. The contribution of our research is an understanding of that a robotic dog more than a living dog may assist in simulating human interaction more than human- animal interaction and is in this way suitable to support playful learning of social-emotional competencies.","2020","2021-05-19 12:52:10","2021-05-19 12:52:10","","262–266","","","","","","","CHI PLAY '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Virtual Event, Canada","","","","emotional intelligence; human-computer interaction; child-robot interaction; human-animal interaction; playful learning; robot toys; social robotics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CX5G53QN","conferencePaper","2020","Chromik, Michael; Lachner, Florian; Butz, Andreas","ML for UX? - An Inventory and Predictions on the Use of Machine Learning Techniques for UX Research","Proceedings of the 11th Nordic Conference on Human-Computer Interaction: Shaping Experiences, Shaping Society","978-1-4503-7579-5","","10.1145/3419249.3420163","https://doi.org/10.1145/3419249.3420163","Machine learning (ML) techniques have successfully been applied to many complex domains. Yet, applying it to UX research (UXR) received little academic attention so far. To better understand how UX practitioners envision the synergies between empathy-focused UX work and data-driven ML techniques, we surveyed 49 practitioners experienced in UX, ML, or both and conducted 13 semi-structured interviews with UX experts. We derived an inventory of ML’s impact on current UXR activities and practitioners’ predictions about its potentials. We learned that ML methods may help to automate mundane tasks, complement decisions with data-driven insights, and enrich UXR with insights from users’ emotional worlds. Challenges may arise from a potential obligation to utilize data and a more restrictive access to user data. We embed our insights into recent academic work on ML for UXR and discuss automated UX evaluation as a promising use case for future research.","2020","2021-05-19 12:52:10","2021-05-19 12:52:10","","","","","","","","","NordiCHI '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Tallinn, Estonia","","","","machine learning; User experience research; UX research","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YBVRBUUI","conferencePaper","2019","Zhou, Michelle X.","Getting Virtually Personal: Making Responsible and Empathetic ""Her"" for Everyone","Proceedings of the 24th International Conference on Intelligent User Interfaces","978-1-4503-6272-6","","10.1145/3301275.3308445","https://doi.org/10.1145/3301275.3308445","Have you watched the movie Her? Have you ever wondered or wished to have your own AI companion just like Samantha, who could understand you better than you know about yourself, and could tell you what you really are, whom your best partner may be, and which career path would be best for you? In this talk, I will present a computational framework for building responsible and empathetic Artificial Intelligent (AI) agents who can deeply understand their users as unique individuals and responsibly guide their behavior in both virtual and real world.Starting with a live demo of showing how an AI interviewer chats with a user to automatically derive his/her personality characteristics and provide personalized recommendations, I will highlight the technical advances of the framework in two aspects. First, I will present a computational, evidence-based approach to Big 5 personality inference, which enables an AI agent to deeply understand a user's unique characteristics by analyzing the user's chat text on the fly. Second, I will describe a topic-based conversation engine that couples deep learning with rules to support a natural conversation and rapid customization of a conversational agent.I will describe the initial applications of our AI agents in the real world, from talent selection to student teaming to user experience research. Finally, I will discuss the wider implications of our work on building hyper-personalized systems and their impact on our lives.","2019","2021-05-19 12:52:10","2021-05-19 12:52:10","","i","","","","","","","IUI '19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Marina del Ray, California","","","","computational psychology; AI interviewer; chatbot; conversational agent; empathetic AI; hyper-personalization; personality inference; responsible AI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FK9Y5DKZ","conferencePaper","2017","Hieida, Chie; Nagai, Takayuki","A Model of Emotion for Empathic Communication","Proceedings of the Companion of the 2017 ACM/IEEE International Conference on Human-Robot Interaction","978-1-4503-4885-0","","10.1145/3029798.3038299","https://doi.org/10.1145/3029798.3038299","Most people believe that robots have no emotions, and nor do they need them. However, we strongly believe that having emotions is essential for robots to understand and sympathize with the feelings of people, thereby allowing them to be accepted into the human society. In this paper, we propose a model of emotion based on some neurological and psychological findings concerning empathic communication between humans and robots. Then, we examine a method for generating affect for given visual stimuli using a recurrent neural network as a first step.","2017","2021-05-19 12:52:10","2021-05-19 12:52:10","","133–134","","","","","","","HRI '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","empathic human-robot interaction; model of emotion; recurrent-attention model","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IMVQ8MG4","conferencePaper","2020","Svoren, Henrik; Thambawita, Vajira; Halvorsen, Pål; Jakobsen, Petter; Garcia-Ceja, Enrique; Noori, Farzan Majeed; Hammer, Hugo L.; Lux, Mathias; Riegler, Michael Alexander; Hicks, Steven Alexander","Toadstool: A Dataset for Training Emotional Intelligent Machines Playing Super Mario Bros","Proceedings of the 11th ACM Multimedia Systems Conference","978-1-4503-6845-2","","10.1145/3339825.3394939","https://doi.org/10.1145/3339825.3394939","Games are often defined as engines of experience, and they are heavily relying on emotions, they arouse in players. In this paper, we present a dataset called Toadstool as well as a reproducible methodology to extend on the dataset. The dataset consists of video, sensor, and demographic data collected from ten participants playing Super Mario Bros, an iconic and famous video game. The sensor data is collected through an Empatica E4 wristband, which provides high-quality measurements and is graded as a medical device. In addition to the dataset and the methodology for data collection, we present a set of baseline experiments which show that we can use video game frames together with the facial expressions to predict the blood volume pulse of the person playing Super Mario Bros. With the dataset and the collection methodology we aim to contribute to research on emotionally aware machine learning algorithms, focusing on reinforcement learning and multimodal data fusion. We believe that the presented dataset can be interesting for a manifold of researchers to explore exciting new interdisciplinary questions.","2020","2021-05-19 12:52:10","2021-05-19 12:52:10","","309–314","","","","","","","MMSys '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Istanbul, Turkey","","","","machine learning; neural networks; emotional machines; multimedia datasets","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FVLZ42QE","journalArticle","2020","McDonald, Nora; Pan, Shimei","Intersectional AI: A Study of How Information Science Students Think about Ethics and Their Impact","Proc. ACM Hum.-Comput. Interact.","","","10.1145/3415218","https://doi.org/10.1145/3415218","Recent literature has demonstrated the limited and, in some instances, waning role of ethical training in computing classes in the US. The capacity for artificial intelligence (AI) to be inequitable or harmful is well documented, yet it's an issue that continues to lack apparent urgency or effective mitigation. The question we raise in this paper is how to prepare future generations to recognize and grapple with the ethical concerns of a range of issues plaguing AI, particularly when they are combined with surveillance technologies in ways that have grave implications for social participation and restriction?from risk assessment and bail assignment in criminal justice, to public benefits distribution and access to housing and other critical resources that enable security and success within society. The US is a mecca of information and computer science (IS and CS) learning for Asian students whose experiences as minorities renders them familiar with, and vulnerable to, the societal bias that feeds AI bias. Our goal was to better understand how students who are being educated to design AI systems think about these issues, and in particular, their sensitivity to intersectional considerations that heighten risk for vulnerable groups. In this paper we report on findings from qualitative interviews with 20 graduate students, 11 from an AI class and 9 from a Data Mining class. We find that students are not predisposed to think deeply about the implications of AI design for the privacy and well-being of others unless explicitly encouraged to do so. When they do, their thinking is focused through the lens of personal identity and experience, but their reflections tend to center on bias, an intrinsic feature of design, rather than on fairness, an outcome that requires them to imagine the consequences of AI. While they are, in fact, equipped to think about fairness when prompted by discussion and by design exercises that explicitly invite consideration of intersectionality and structural inequalities, many need help to do this empathy 'work.' Notably, the students who more frequently reflect on intersectional problems related to bias and fairness are also more likely to consider the connection between model attributes and bias, and the interaction with context. Our findings suggest that experience with identity-based vulnerability promotes more analytically complex thinking about AI, lending further support to the argument that identity-related ethics should be integrated into IS and CS curriculums, rather than positioned as a stand-alone course.","2020-10","2021-05-19 12:52:11","2021-05-19 12:52:11","","","","CSCW2","4","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","artificial intelligence; ethics; algorithm bias; education; intersectionality","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3V7S9YTF","conferencePaper","2017","Frueh, Christian; Sud, Avneesh; Kwatra, Vivek","Headset Removal for Virtual and Mixed Reality","ACM SIGGRAPH 2017 Talks","978-1-4503-5008-2","","10.1145/3084363.3085083","https://doi.org/10.1145/3084363.3085083","Virtual Reality (VR) has advanced significantly in recent years and allows users to explore novel environments (both real and imaginary), play games, and engage with media in a way that is unprecedentedly immersive. However, compared to physical reality, sharing these experiences is difficult because the user's virtual environment is not easily observable from the outside and the user's face is partly occluded by the VR headset. Mixed Reality (MR) is a medium that alleviates some of this disconnect by sharing the virtual context of a VR user in a flat video format that can be consumed by an audience to get a feel for the user's experience.Even though MR allows audiences to connect actions of the VR user with their virtual environment, empathizing with them is difficult because their face is hidden by the headset. We present a solution to address this problem by virtually removing the headset and revealing the face underneath it using a combination of 3D vision, machine learning and graphics techniques. We have integrated our headset removal approach with Mixed Reality, and demonstrate results on several VR games and experiences.","2017","2021-05-19 12:52:11","2021-05-19 12:52:11","","","","","","","","","SIGGRAPH '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Los Angeles, California","","","","virtual reality; mixed reality; facial synthesis; headset removal","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UL2TSSUG","conferencePaper","2015","Zuckerman, Oren; Hoffman, Guy","Empathy Objects: Robotic Devices as Conversation Companions","Proceedings of the Ninth International Conference on Tangible, Embedded, and Embodied Interaction","978-1-4503-3305-4","","10.1145/2677199.2688805","https://doi.org/10.1145/2677199.2688805","We present the notion of Empathy Objects, ambient robotic devices accompanying human-human interaction. Empathy Objects respond to human behavior using physical gestures as nonverbal expressions of their ""emotional states"". The goal is to increase people's self-awareness to the emotional state of others, leading to behavior change. We demonstrate an Empathy Object prototype, Kip1, a conversation companion designed to promote non-aggressive conversation between people.","2015","2021-05-19 12:52:11","2021-05-19 12:52:11","","593–598","","","","","","","TEI '15","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Stanford, California, USA","","","","social robots; tangible interfaces; ambient devices; behavior change; companion devices","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MQTYRXW6","conferencePaper","2015","Encinas, Enrique","Cyrafour: How Two Human Avatars Communicate With Each Other","Proceedings of the 33rd Annual ACM Conference Extended Abstracts on Human Factors in Computing Systems","978-1-4503-3146-3","","10.1145/2702613.2726962","https://doi.org/10.1145/2702613.2726962","Human avatars or physical surrogates are becoming increasingly present in leisure, artistic and business activities that seek to augment the sensory richness available to telepresent participants. While a number of studies have focused on how human avatars relate to other humans, little attention has been paid to the particularities of human avatar to human avatar interaction. This paper examines characteristic features of such interaction through Cyrafour, a playful embodied identity game in which two human avatars clone various conversations generated elsewhere. Such cloning, or speech shadowing, seems to allow for an empathic embodiment of the meaning transmitted and appears to create a frame for further discussion on the topics raised. This project contributes to the study of telepresence with new insights applicable to the design and research of human computer and human robot interfaces.","2015","2021-05-19 12:52:11","2021-05-19 12:52:11","","109–114","","","","","","","CHI EA '15","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Seoul, Republic of Korea","","","","embodied cognition; serious games; copresence; cyranoids; human avatars; personal surrogates; telepresence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HUMRAHIN","conferencePaper","2020","Soleymani, Mohammad","Machine Understanding of Emotion and Sentiment","Companion Publication of the 2020 International Conference on Multimodal Interaction","978-1-4503-8002-7","","10.1145/3395035.3425321","https://doi.org/10.1145/3395035.3425321","Emotions are subjective experiences involving perceptual and con-textual factors [4]. There is no objective tool for precise measurement of emotions. However, we can anticipate an emotion's emergence through the knowledge of common responses to events in similar situations. We can also measure proxies of emotions by recognizing emotional expressions [3]. Studying emotional response to multimedia allows identifying expected emotions in users consuming the content. For example,abrupt loud voices are novel and unsettling which result in surprise and higher experience of arousal [2,6]. For a particular type of con-tent such as music, mid-level attributes such as rhythmic stability or melodiousness have strong association with expected emotions[1]. Given that such mid-level attributes are more related to the con-tent, their machine-perception is more straightforward. Moreover,their perception in combination with user models enables building person-specific emotion anticipation models.In addition to studying expected emotions, we can also observe users emotional reactions to understand emotion in multimedia.Typical methods of emotion recognition include recognizing emotions from facial or vocal expressions. Recognition of emotional expressions requires large amount of labeled data, expensive to produce. Hence, the most recent advances in machine-based emotion perception include methods that can leverage unlabeled data through self-supervised and semi-supervised learning [3, 5]. In this talk, I review the field and showcase methods for automatic modeling and recognition of emotions and sentiment indifferent contexts [3,8]. I show how we can identify underlying factors contributing to the construction of subjective experience of emotions [1,7]. Identification of these factors allows us to use them as mid-level attributes to build machine learning models for emotion and sentiment understanding. I also show how emotions and sentiment can be recognized from expressions with the goal of building empathetic autonomous agents [8].","2020","2021-05-19 12:52:11","2021-05-19 12:52:11","","206–207","","","","","","","ICMI '20 Companion","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Virtual Event, Netherlands","","","","machine learning; emotion; affective computing; sentiment","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NJJNWXCU","conferencePaper","2020","Chen, Zhifa; Lu, Yichen; Nieminen, Mika P.; Lucero, Andrés","Creating a Chatbot for and with Migrants: Chatbot Personality Drives Co-Design Activities","Proceedings of the 2020 ACM Designing Interactive Systems Conference","978-1-4503-6974-9","","10.1145/3357236.3395495","https://doi.org/10.1145/3357236.3395495","Information portals are usually created to support the integration of migrants into a host country. However, the information-seeking process can be exhausting, cumbersome and even confusing for migrants as they must cope with time-consuming information overload while searching desired information from lists of documents. Chatbots are easy-to-use, natural, and intuitive, and thus could support information-seeking. There is a lack of research that engages and empowers migrants and other stakeholders as co-design participants in chatbot development. We explored how migrants can be empowered in designing a chatbot that supports their social integration. Using a co-design approach, we conducted a series of activities with migrants and other stakeholders (i.e., online questionnaires, empathy probes, surveys, and co-design workshops) to first understand their expectations regarding chatbots, and then co-design a personality-driven chatbot. We found that chatbot personality can drive co-designing a chatbot as design goals, design directions, and design criteria.","2020","2021-05-19 12:52:11","2021-05-19 12:52:11","","219–230","","","","","","","DIS '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Eindhoven, Netherlands","","","","chatbot; personality; avatar; co-design; conversation design; generative toolkit; migrants; probes; social integration","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A2KIB5W2","journalArticle","2021","Mitchell, Elliot G.; Maimone, Rosa; Cassells, Andrea; Tobin, Jonathan N.; Davidson, Patricia; Smaldone, Arlene M.; Mamykina, Lena","Automated vs. Human Health Coaching: Exploring Participant and Practitioner Experiences","Proc. ACM Hum.-Comput. Interact.","","","10.1145/3449173","https://doi.org/10.1145/3449173","Health coaching can be an effective intervention to support self-management of chronic conditions like diabetes, but there are not enough coaching practitioners to reach the growing population in need of support. Conversational technology, like chatbots, presents an opportunity to extend health coaching support to broader and more diverse populations. However, some have suggested that the human element is essential to health coaching and cannot be replicated with technology. In this research, we examine automated health coaching using a theory-grounded, wizard-of-oz chatbot, in comparison with text-based virtual coaching from human practitioners who start with the same protocol as the chatbot but have the freedom to embellish and adjust as needed. We found that even a scripted chatbot can create a coach-like experience for participants. While human coaches displayed advantages expressing empathy and using probing questions to tailor their support, they also encountered tremendous barriers and frustrations adapting to text-based virtual coaching. The chatbot coach had advantages in being persistent, as well as more consistently giving choices and options to foster client autonomy. We discuss implications for the design of virtual health coaching interventions.","2021-04","2021-05-19 12:52:11","2021-05-19 12:52:11","","","","CSCW1","5","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","conversational agents; chatbots; self-management; health coaching; text messaging; type 2 diabetes; wizard-of-oz","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5A4W5LX7","conferencePaper","2020","De Nieva, Johan Oswin; Joaquin, Jose Andres; Tan, Chaste Bernard; Marc Te, Ruzel Khyvin; Ong, Ethel","Investigating Students’ Use of a Mental Health Chatbot to Alleviate Academic Stress","6th International ACM In-Cooperation HCI and UX Conference","978-1-4503-8829-0","","10.1145/3431656.3431657","https://doi.org/10.1145/3431656.3431657","The amount of academic workload in schools can cause students to experience stress and become more susceptible to mental health problems. However, because of fear of societal stigma, students may find it more difficult to approach others about the stress they experience. A chatbot can provide an alternative avenue for students to freely share the stressful situations they are experiencing. In this study, we investigated the use of Woebot as a mechanism to help senior high school students alleviate stress from academic workload. 25 participants who engaged in daily conversations with Woebot for a two-week period rated the chatbot’s likeness to a human with a mean score of 5.56 out of 8, while its ability to understand the feelings of the participants and empathize with them had a mean score of 5.61. An analysis of the chat logs showed that the participants valued Woebot’s lessons and stories while they faced challenges in cases when the chatbot generated inappropriate responses. We discuss our findings and provide design suggestions that could make conversational agents like Woebot be more useful in helping the general student population cope with stress.","2020","2021-05-19 12:52:11","2021-05-19 12:52:11","","1–10","","","","","","","CHIuXiD '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Jakarta &amp; Bandung, Indonesia","","","","Academic stress; Alleviating stress; Mental health chatbot","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NTKA7SKI","conferencePaper","2020","Ahn, Yuna; Zhang, Yilin; Park, Yujin; Lee, Joonhwan","A Chatbot Solution to Chat App Problems: Envisioning a Chatbot Counseling System for Teenage Victims of Online Sexual Exploitation","Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems","978-1-4503-6819-3","","10.1145/3334480.3383070","https://doi.org/10.1145/3334480.3383070","In recent years, online sexual exploitation targeting teenagers has been on the rise. Given teenagers' growing reluctance toward face-to-face communication, using a counseling chatbot could be a more effective way to provide teenage victims with necessary information and emotional support. There is a small number of counseling chatbots for victims of sexual crime, but none targeting teenagers specifically. This research suggests design guidelines for building a counseling chatbot for teenage victims of online sexual exploitation with a focus on establishing rapport by empathizing with their stories and providing them with the proper information. We conducted in-depth interviews with peer counselors at the Teenage Women's Human Rights Center, who have been consulting teenage victims in their age group using online messengers. The four key findings from our research suggested using open-ended questions, using teenager-friendly language, helping teenagers understand that they are victims and offering age-relevant information.","2020","2021-05-19 12:52:11","2021-05-19 12:52:11","","1–7","","","","","","","CHI EA '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Honolulu, HI, USA","","","","rapport; counseling chatbot; online sexual exploitation; teenager","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J5845GCE","conferencePaper","2016","Ishii, Yutaka; Watanabe, Tomio; Sejima, Yoshihiro","Development of an Embodied Avatar System Using Avatar-Shadow's Color Expressions with an Interaction-Activated Communication Model","Proceedings of the Fourth International Conference on Human Agent Interaction","978-1-4503-4508-8","","10.1145/2974804.2980487","https://doi.org/10.1145/2974804.2980487","In reality, shadows are usually natural and unintentional. In virtual reality, however, they play an important role in three-dimensional effects and the perceived reality of the virtual space. An avatar's shadow can have interactive effects with the avatar itself in the virtual space. In this study, we develop an embodied avatar system using avatar-shadow color expressions with an interaction-activated communication model. This model is based on the heat conduction equation in heat-transfer engineering, and has been developed to enhance empathy during embodied interaction in avatar-mediated communication. A communication experiment is performed with 12 pairs of participants to confirm the effectiveness of the system. The results of the sensory evaluation show that interaction activation is visualized by changing avatar-shadow color.","2016","2021-05-19 12:52:11","2021-05-19 12:52:11","","337–340","","","","","","","HAI '16","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Biopolis, Singapore","","","","avatar-mediated communication; embodied interaction; avatar's shadow; color expression.; virtual face-to-face communication","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"652PX5HH","conferencePaper","2020","Xiao, Ziang; Zhou, Michelle X.; Chen, Wenxi; Yang, Huahai; Chi, Changyan","If I Hear You Correctly: Building and Evaluating Interview Chatbots with Active Listening Skills","Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems","978-1-4503-6708-0","","10.1145/3313831.3376131","https://doi.org/10.1145/3313831.3376131","Interview chatbots engage users in a text-based conversation to draw out their views and opinions. It is, however, challenging to build effective interview chatbots that can handle user free-text responses to open-ended questions and deliver engaging user experience. As the first step, we are investigating the feasibility and effectiveness of using publicly available, practical AI technologies to build effective interview chatbots. To demonstrate feasibility, we built a prototype scoped to enable interview chatbots with a subset of active listening skills-the abilities to comprehend a user's input and respond properly. To evaluate the effectiveness of our prototype, we compared the performance of interview chatbots with or without active listening skills on four common interview topics in a live evaluation with 206 users. Our work presents practical design implications for building effective interview chatbots, hybrid chatbot platforms, and empathetic chatbots beyond interview tasks.","2020","2021-05-19 12:52:55","2021-05-19 12:52:55","","1–14","","","","","","","CHI '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Honolulu, HI, USA","","","","deep learning; conversational agents; active listening; ai chatbot; chatbot platform; interview chatbot","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5BUM8XP4","conferencePaper","2021","Svikhnushina, Ekaterina; Pu, Pearl","Key Qualities of Conversational Chatbots – the PEACE Model","26th International Conference on Intelligent User Interfaces","978-1-4503-8017-1","","10.1145/3397481.3450643","https://doi.org/10.1145/3397481.3450643","Open-domain chatbots engage in natural conversations with the user to socialize and establish bonds. However, designing and developing an effective open-domain chatbot is challenging. It is unclear what qualities of such chatbots most correspond to users’ expectations. Even though existing work has considered a wide range of aspects, some key components are still missing. More importantly, the consistency and validity of the combined criteria have not been tested. In this paper, we describe a large-scale survey using a consolidated model to elicit users’ preferences, expectations, and concerns. We apply structural equation modeling methods to further validate the data collected from the user survey. The outcome supports the consistency, validity, and reliability of the model, which we call PEACE (Politeness, Entertainment, Attentive Curiosity, and Empathy). PEACE, therefore, defines the key determinants most predictive of user acceptance. This has allowed us to develop a set of implications useful for the development of compelling open-domain chatbots.","2021","2021-05-19 12:52:55","2021-05-19 12:52:55","","520–530","","","","","","","IUI '21","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: College Station, TX, USA","","","","chatbots; adoption; user study; questionnaire; SEM","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4BHWKM33","conferencePaper","2019","Weisz, Justin D.; Jain, Mohit; Joshi, Narendra Nath; Johnson, James; Lange, Ingrid","BigBlueBot: Teaching Strategies for Successful Human-Agent Interactions","Proceedings of the 24th International Conference on Intelligent User Interfaces","978-1-4503-6272-6","","10.1145/3301275.3302290","https://doi.org/10.1145/3301275.3302290","Chatbots are becoming quite popular, with many brands developing conversational experiences using platforms such as IBM's Watson Assistant and Facebook Messenger. However, previous research reveals that users' expectations of what conversational agents can understand and do far outpace their actual technical capabilities. Our work seeks to bridge the gap between these expectations and reality by designing a fun learning experience with several goals: explaining how chatbots work by mapping utterances to a set of intents, teaching strategies for avoiding conversational breakdowns, and increasing desire to use chatbots by creating feelings of empathy toward them. Our experience, called BigBlueBot, consists of interactions with two chatbots in which breakdowns occur and the user (or chatbot) must recover using one or more repair strategies. In a Mechanical Turk evaluation (N=88), participants learned strategies for having successful human-agent interactions, reported feelings of empathy toward the chatbots, and expressed a desire to interact with chatbots in the future.","2019","2021-05-19 12:52:55","2021-05-19 12:52:55","","448–459","","","","","","","IUI '19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Marina del Ray, California","","","","conversational agents; explainable AI; mechanical turk","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AKSF7SZ8","conferencePaper","2016","Hastie, Helen; Lim, Mei Yii; Janarthanam, Srini; Deshmukh, Amol; Aylett, Ruth; Foster, Mary Ellen; Hall, Lynne","I Remember You! Interaction with Memory for an Empathic Virtual Robotic Tutor","Proceedings of the 2016 International Conference on Autonomous Agents &amp; Multiagent Systems","978-1-4503-4239-1","","","","We present a study that investigates the effect of incorporating memory in the interaction for a virtual robotic tutor in terms of helping children achieve a pedagogical goal and the perceived likeability and empathy of the tutor. The domain is a virtual robotic tutor who is guiding and helping learners through a mobile Treasure Hunt exercise that tests their map reading skills. The contribution described in this paper is the discovery that incorporating 'memory' through utterances that recall events from previous interactions significantly increases the learner's ability to perform a pedagogical task. However, the virtual tutor with memory was perceived as less likeable and the instructions given as harder to follow than with a virtual tutor without memory. In addition, there was a significant drop in perceived empathy. This work has a large potential influence in the field of interaction design for agents as one cannot blindly add in human-like features, such as, memory that improve task performance without considering the potential detrimental effects to the perceived empathy and likeability.","2016","2021-05-19 12:52:55","2021-05-19 12:52:55","","931–939","","","","","","","AAMAS '16","","","","International Foundation for Autonomous Agents and Multiagent Systems","Richland, SC","","","","","","","","event-place: Singapore, Singapore","","","","empathy; human-agent interaction; human-robot interaction; memory","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LRHEUBVH","conferencePaper","2019","Degraen, Donald; Kosmalla, Felix; Krüger, Antonio","Overgrown: Supporting Plant Growth with an Endoskeleton for Ambient Notifications","Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems","978-1-4503-5971-9","","10.1145/3290607.3312833","https://doi.org/10.1145/3290607.3312833","Ambient notifications are an essential element to support users in their daily activities. Designing effective and aesthetic notifications that balance the alert level while maintaining an unobtrusive dialog, require them to be seamlessly integrated into the user's environment. In an attempt to employ the living environment around us, we designed Overgrown, an actuated robotic structure capable of supporting a plant to grow over itself. As a plant endoskeleton, Overgrown aims to engage human empathy towards living creatures to increase effectiveness of ambient notifications while ensuring better integration with the environment. In a focus group, Overgrown was identified with having personality, showed potential as a user's ambient avatar, and was suited for social experiments.","2019","2021-05-19 12:52:55","2021-05-19 12:52:55","","1–6","","","","","","","CHI EA '19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Glasgow, Scotland Uk","","","","ambient notifications; empathic living media; focus group; ambient interfaces","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2JAZTUAF","conferencePaper","2020","Ravi, Akhilesh; Yadav, Amit Kumar Singh; Chauhan, Jainish; Dholakia, Jatin; Jain, Naman","SentEmoji: A Dataset to Generate Empathising Conversations","Proceedings of the 7th ACM IKDD CoDS and 25th COMAD","978-1-4503-7738-6","","10.1145/3371158.3371218","https://doi.org/10.1145/3371158.3371218","Emojis are gaining popularity in day-to-day computer-mediated conversations, resulting in more interactive conversations. On the other hand, traditional chatbots lack the ability to use emojis effectively for creating an engaging and empathising conversation even after recognising feelings of the conversation partner, an essential communicative skill. This inability is majorly due to the paucity of any such suitable publicly available datasets and framework for training and evaluation of chatbot. Prior work has either classified the emojis or generated empathy dialogue without the use of emojis. Through this work, we propose a new dataset SentEmoji, generated using public dataset EmpathyDialogues, and its mapping to relevant emojis using EmojiNet dataset. We present a novel approach to generate dialogue with emojis to express empathy. A study will be conducted to get user rating on three aspects - empathy/sympathy, relevance and fluency. The comparison of this user-study with prior studies will reflect the effectiveness of this approach.","2020","2021-05-19 12:52:56","2021-05-19 12:52:56","","345–346","","","","","","","CoDS COMAD 2020","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Hyderabad, India","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BC5AEAKM","conferencePaper","2017","Vieira, Suanny; Santos, Alexandre; Costa, Rostand; Maritan, Tiago; Aschoff, Manuella; Veríssimo, Vinícius","A Study on the Use of Multiple Avatars in 3D Sign Language Dictionaries","Proceedings of the 23rd Brazillian Symposium on Multimedia and the Web","978-1-4503-5096-9","","10.1145/3126858.3126865","https://doi.org/10.1145/3126858.3126865","Numerous platforms in the field of machine translation of oralized languages to sign language are available nowadays, and accessibility has been gaining more and more space. However, it is noticed that most platforms use only a unique 3D avatar, and this character is responsible for all the reproduction of signals, with no alternative of choice for users. Such a limitation may have an impact on the acceptance of automatic translation by the deaf community, since there must be empathy of the deaf with the animated agent. Having only one available avatar makes impossible a more precise choice, which may involve personal characteristics and affinities. One of the reasons for this is the great effort, human and technological, that is necessary for the construction of a sign dictionary, which can scale proportionally with the addition of new avatars. In view of such a scenario, the present study aims to investigate mechanisms that allow multiple avatars to be offered in sign dictionaries without necessarily needing to reshape them again and manually, one by one. The initial premise is to analyze the functioning of each signal in a particular avatar, in order to predict possible problems in the reproduction of the signals after the permutation to a new one (retargeting), such as improper collisions or mesh invasions. As main contributions of the work, techniques are proposed to facilitate the identification and automatic correction of nonconformities in the movement of the signals and also some practical recommendations for the modeling of new avatars in order to minimize the occurrence of errors.","2017","2021-05-19 12:52:56","2021-05-19 12:52:56","","325–332","","","","","","","WebMedia '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Gramado, RS, Brazil","","","","accessibility; avatar; machine translation; retargeting; sign language","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C9QUMAEL","conferencePaper","2019","Gu, Xiusen; Xu, Weiran; Zhang, Chao","Neural Emotional Response Generation via Adversarial Transfer Learning","Proceedings of the 2019 3rd International Conference on Innovation in Artificial Intelligence","978-1-4503-6128-6","","10.1145/3319921.3319933","https://doi.org/10.1145/3319921.3319933","Emotional response generation is a key step to build an empathetic chatbot. However, previous emotional chatting models mainly focus on single-turn conversation, and multi-turn context emotional response generation has not been explored. In this paper, we propose an adversarial transfer emotional chatting (ATEC) model for multi-turn conversation which is based on conditional variational autoencoders (CVAE). ATEC has two alternate training phases: supervised training and transfer training. In the supervised training stage, we train the CVAE model, a content discriminator and an emotional classifier based on ground truth corpus. And in the transfer training stage, we change the target emotion and use the content discriminator to force the model to transfer the multi-turn context information, while the emotional classifier regularizes the emotions expressed in the generated responses. Experiments show that the proposed approach achieves state of the art performance with diverse responses and accurate emotional expression.","2019","2021-05-19 12:52:56","2021-05-19 12:52:56","","106–110","","","","","","","ICIAI 2019","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Suzhou, China","","","","conditional variational autoencoders; emotion transfer; Emotional dialogue system","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PQI958GE","conferencePaper","2016","Sakurai, Sho; Ban, Yuki; Katsumura, Toki; Narumi, Takuji; Tanikawa, Tomohiro; Hirose, Michitaka","Sharing Emotion Described as Text on the Internet by Changing Self-Physiological Perception","Proceedings of the Fourth International Conference on Human Agent Interaction","978-1-4503-4508-8","","10.1145/2974804.2974825","https://doi.org/10.1145/2974804.2974825","Agents like human, such as humanoid robots or avatars can be felt as if they have and communicate and communicate due to manipulation of the bodily information. Meanwhile, as in the case of Internet bot, it is still difficult to communiate the emotion described as text, let alone empathizing due to degradation of information online. The current study proposes a method for experiencing emotion on the Internet by reproducing a mechanism of evoking emotion. This method evokes a number of emotions described on the Web, by changing of self-physiological perception with sensory stimuli. To investigate the feasibility of our method, we made a system named ""Communious Mouse."" This system rewrites the perception of self-skin temperature and pulse in a palm by presenting vibration and thermal stimulation through a mouse device for evoking emotion. The current paper discusses the feasibility of our method based on the obtained feedbacks through an exhibition of the system.","2016","2021-05-19 12:52:56","2021-05-19 12:52:56","","145–153","","","","","","","HAI '16","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Biopolis, Singapore","","","","emotion; theory of mind; a sense of ownership; online communication; physiological perception; self-perception","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"89CS87AN","conferencePaper","2015","Andersen, Josephine Soegaard; Schoenau-Fog, Henrik","Using Role-Taking and Behavioral Mimicking in Games to Increase Awareness on the Bystander Effect","Proceedings of the 19th International Academic Mindtrek Conference","978-1-4503-3948-3","","10.1145/2818187.2818290","https://doi.org/10.1145/2818187.2818290","This study presents a concept on how a serious game might raise awareness of the bystander effect by using elements of game theory as well as a few psychological terms. The paper summarizes the theories and concludes with the description of a concept, which is a third person role playing game with behavioral mimicking. The game concept should include a relatable (preferably player modifiable) avatar, so the player can relate and adhere to the empathy and intent to help. Since the bystander effect takes place in groups where deindividuation also is common, this should require a behavioral change of this particular group's norms. However, groups (especially of friends) can aid as support in case there is need for intervention as opposed to being passive bystanders.","2015","2021-05-19 12:52:56","2021-05-19 12:52:56","","69–72","","","","","","","AcademicMindTrek '15","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Tampere, Finland","","","","behavioral mimicking; bystander effect; mimetic activity; proteus effect; role playing; role-taking; serious game; simulations","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6JAQUWJL","conferencePaper","2017","Tong, Xin; Ulas, Servet; Jin, Weina; Gromala, Diane; Shaw, Chris","The Design and Evaluation of a Body-Sensing Video Game to Foster Empathy towards Chronic Pain Patients","Proceedings of the 11th EAI International Conference on Pervasive Computing Technologies for Healthcare","978-1-4503-6363-1","","10.1145/3154862.3154869","https://doi.org/10.1145/3154862.3154869","Chronic Pain (CP) has been identified as a complex medical condition, one that is difficult for sufferers to articulate and for others to discern. This may interfere with the ability of a patient's family, friends and healthcare practitioners to understand what it is like to live with CP, or to even believe it exists. A reluctance by or ability of others to believe a CP patient may in turn exacerbate pain and sequelae common in CP, such as depression, frustration, stigma or social isolation. The goal of this research is to help foster empathy of what CP patients experience by designing and evaluating a body-sensing video game titled AS IF. In this game, players ""inhabit"" a virtual body or avatar of a CP patient. The virtual body simulates physical limitations and displays red areas meant to indicate painful areas. A pilot study with 15 participants was conducted. Results show that while not every aspect of the game proved successful, players had a significant increase in their willingness to help patients. This research demonstrates an approach that may help foster empathy towards CP patients through an embodied game simulation, and has design implications for future research and gameplay explorations.","2017","2021-05-19 12:52:56","2021-05-19 12:52:56","","244–250","","","","","","","PervasiveHealth '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Barcelona, Spain","","","","empathy; serious games; body-sensing games; chronic pain; embodied simulation; gaming for a purpose","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VC4KGFGQ","conferencePaper","2019","Giakoumis, Dimitrios; Votis, Konstantinos; Altsitsiadis, Efthymios; Segkouli, Sofia; Paliokas, Ioannis; Tzovaras, Dimitrios","Smart, Personalized and Adaptive ICT Solutions for Active, Healthy and Productive Ageing with Enhanced Workability","Proceedings of the 12th ACM International Conference on PErvasive Technologies Related to Assistive Environments","978-1-4503-6232-0","","10.1145/3316782.3322767","https://doi.org/10.1145/3316782.3322767","Along with population ageing comes the increasingly intensified phenomenon of a shrinking and ageing workforce. Novel solutions are needed so as to help ageing workers maintain workability and productivity, along with a balance between work and personal life, which supports them into good quality of life, active and healthy ageing. In this line, the ""Ageing@work"" project, initiated by the European Union, develops a novel ICT-based, personalized system to support ageing workers (aged 50+) into designing fit for purpose work environments and managing flexibly their evolving needs. On top of personalized, dynamically adapted worker and workplace models, computational intelligence will assess user specificities and needs i.r.t. work conditions, both in terms of ergonomics, health and safety issues and task assignments. Recommendations will then be provided both to the worker and company, under strict privacy restrictions, on how the working conditions must adapt. The worker models will be populated by unobtrusive worker sensing, both at work, at home and on the move. To foster workability and productivity, personalized, intuitive, age-friendly productivity, co-design enhancement tools will be developed, including ones for AR/VR-based context-awareness and telepresence, lifelong learning and knowledge sharing. On top of these, a novel Ambient Virtual Coach (AVC) will encompass an empathic mirroring avatar for subtle notifications provision, an adaptive Visual Analytics - based personal dashboard, and a reward-based motivation system targeting positive and balanced worker behavior at work and personal life, towards a novel paradigm of ambient support into workability and well-being. The integrated system will be developed by user-centered design and will be evaluated at two pilot sites, related to core Industry 4.0 processes of mining and machines production.","2019","2021-05-19 12:52:56","2021-05-19 12:52:56","","442–447","","","","","","","PETRA '19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Rhodes, Greece","","","","age-friendly workforce management; ageing workforce; eHealth; virtual user models; workability","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3F4UG3JK","conferencePaper","2017","Portela, Manuel; Granell-Canut, Carlos","A New Friend in Our Smartphone? Observing Interactions with Chatbots in the Search of Emotional Engagement","Proceedings of the XVIII International Conference on Human Computer Interaction","978-1-4503-5229-1","","10.1145/3123818.3123826","https://doi.org/10.1145/3123818.3123826","We present the findings of a quantitative and qualitative empirical research to understand the possibilities of engagement and affection in the use of conversational agents (chatbots). Based on an experiment with 13 participants, we explored on one hand the correlation between the user expectation, user experience and intended use and, on the other, whether users feel keen and engaged in having a personal, empathic relation with an intelligent system like chatbots. We used psychological questionnaires to semi-structured interviews for disentangle the meaning of the interaction. In particular, the personal psychological background of participants was found critical while the experience itself allowed them to imagine new possible relations with chatbots. Our results show some insights on how people understand and empathize with future interactions with conversational agents and other non-visual interfaces.","2017","2021-05-19 12:52:56","2021-05-19 12:52:56","","","","","","","","","Interacción '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Cancun, Mexico","","","","conversational agents; emotional engagement; empathic relations; mixed-method analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5MAHCZAT","conferencePaper","2021","Urakami, Jacqueline; Sutthithatip, Sujitra","Building a Collaborative Relationship between Human and Robot through Verbal and Non-Verbal Interaction","Companion of the 2021 ACM/IEEE International Conference on Human-Robot Interaction","978-1-4503-8290-8","","10.1145/3434074.3447171","https://doi.org/10.1145/3434074.3447171","Interpersonal communication and relationship building promote successful collaborations. This study investigated the effect of conversational nonverbal and verbal interactions of a robot on bonding and relationship building with a human partner.Participants interacted with two robots that differed in their nonverbal and verbal expressiveness. The interactive robot actively engaged the participant in a conversation before, during and after a collaborative task whereas the non-interactive robot remained passive. The robots' nonverbal and verbal interactions increased participants' perception of the robot as a social actor and strengthened bonding and relationship building between human and robot. The results of our study indicate that the evaluation of the collaboration improves when the robot maintains eye contact, the robot is attributed a certain personality, and the robot is perceived as being alive.Our study could not show that an interactive robot receives more help by the collaboration partner. Future research should investigate additional factors that facilitate helpful behavior among humans, such as similarity, attributional judgement and empathy.","2021","2021-05-19 12:52:56","2021-05-19 12:52:56","","257–261","","","","","","","HRI '21 Companion","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Boulder, CO, USA","","","","social presence; helping; human robot collaboration; relationship building","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"97CQ6AKM","conferencePaper","2018","Tan, Xiang Zhi; Vázquez, Marynel; Carter, Elizabeth J.; Morales, Cecilia G.; Steinfeld, Aaron","Inducing Bystander Interventions During Robot Abuse with Social Mechanisms","Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction","978-1-4503-4953-6","","10.1145/3171221.3171247","https://doi.org/10.1145/3171221.3171247","We explored whether a robot can leverage social influences to motivate nearby bystanders to intervene and defend them from human abuse. We designed a between-subjects study where 48 participants took part in a memorization task and observed a confederate mistreating a robot both verbally and physically. The robot was either empathetic towards the participant»s performance in the task or indifferent. When the robot was mistreated, it ignored the abuse, shut down in response to it, or reacted emotionally. We found that the majority of the participants intervened to help the robot after it was abused. Interventions happened for a wide range of reasons. Interestingly, the empathetic robot increased the proportion of participants that self-reported intervening in comparison to the indifferent robot, but more participants moved the robot as a response to abuse in the latter case. The participants also perceived the robot being verbally mistreated more and reported higher levels of personal distress when the robot briefly shut down after abuse in comparison to when it reacted emotionally or did not react at all.","2018","2021-05-19 12:52:56","2021-05-19 12:52:56","","169–177","","","","","","","HRI '18","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Chicago, IL, USA","","","","robots; empathy; human-robot interaction; abuse; bullying; peer intervention","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4ZBUKLFG","conferencePaper","2016","Deshmukh, Amol; Janarthanam, Srinivasan; Hastie, Helen; Lim, Mei Yii; Aylett, Ruth; Castellano, Ginevra","How Expressiveness of a Robotic Tutor is Perceived by Children in a Learning Environment","The Eleventh ACM/IEEE International Conference on Human Robot Interaction","978-1-4673-8370-7","","","","We present a study investigating the expressiveness of two different types of robots in a tutoring task. The robots used were i) the EMYS robot, with facial expression capabilities, and ii) the NAO robot, without facial expressions but able to perform expressive gestures. Preliminary results show that the NAO robot was perceived to be more friendly, pleasant and empathic than the EMYS robot as a tutor in a learning environment.","2016","2021-05-19 12:52:56","2021-05-19 12:52:56","","423–424","","","","","","","HRI '16","","","","IEEE Press","","","","","","","","","event-place: Christchurch, New Zealand","","","","empathy; robotic tutors; child-robot interaction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KDRSIJSC","conferencePaper","2019","Sanoubari, Elaheh; Seo, Stela H.; Garcha, Diljot; Young, James E.; Loureiro-Rodríguez, Verónica","Good Robot Design or Machiavellian? An in-the-Wild Robot Leveraging Minimal Knowledge of Passersby's Culture","Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction","978-1-5386-8555-6","","","","Social robots are being designed to use human-like communication techniques, including body language, social signals, and empathy, to work effectively with people. Just as between people, some robots learn about people and adapt to them. In this paper we present one such robot design: we developed Sam, a robot that learns minimal information about a person's background, and adapts to this background. Our in-the-wild study found that people helped Sam for significantly longer when it adapted to match their background. While initially we saw this as a success, in re-considering our study we started seeing a different angle. Our robot effectively deceived people (changed its story and text), based on some knowledge of their background, to get more work from them. There was little direct benefit to the person from this adaptation, yet the robot stood to gain free labor. We would like to pose the question to the community: is this simply good robot design, or, is our robot being manipulative? Where does the ethical line lay between a robot leveraging social techniques to improve interaction, and the more negative framing of a robot or algorithm taking advantage of people? How can we decide what is good here, and what is less desirable?","2019","2021-05-19 12:52:56","2021-05-19 12:52:56","","382–391","","","","","","","HRI '19","","","","IEEE Press","","","","","","","","","event-place: Daegu, Republic of Korea","","","","culture; social robots; in the wild; persuasive robots","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"73L27RZD","conferencePaper","2020","Connolly, Joe; Mocz, Viola; Salomons, Nicole; Valdez, Joseph; Tsoi, Nathan; Scassellati, Brian; Vázquez, Marynel","Prompting Prosocial Human Interventions in Response to Robot Mistreatment","Proceedings of the 2020 ACM/IEEE International Conference on Human-Robot Interaction","978-1-4503-6746-2","","10.1145/3319502.3374781","https://doi.org/10.1145/3319502.3374781","Inspired by the benefits of human prosocial behavior, we explore whether prosocial behavior can be extended to a Human-Robot Interaction (HRI) context. More specifically, we study whether robots can induce prosocial behavior in humans through a 1x2 between-subjects user study (N=30) in which a confederate abused a robot. Through this study, we investigated whether the emotional reactions of a group of bystander robots could motivate a human to intervene in response to robot abuse. Our results show that participants were more likely to prosocially intervene when the bystander robots expressed sadness in response to the abuse as opposed to when they ignored these events, despite participants reporting similar perception of robot mistreatment and levels of empathy for the abused robot. Our findings demonstrate possible effects of group social influence through emotional cues by robots in human-robot interaction. They reveal a need for further research regarding human prosocial behavior within HRI.","2020","2021-05-19 12:52:56","2021-05-19 12:52:56","","211–220","","","","","","","HRI '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Cambridge, United Kingdom","","","","human-robot interaction; prosocial behavior; robot abuse","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WLRPP9DB","conferencePaper","2015","Jeong, Sooyeon; Santos, Kristopher Dos; Graca, Suzanne; O'Connell, Brianna; Anderson, Laurel; Stenquist, Nicole; Fitzpatrick, Katie; Goodenough, Honey; Logan, Deirdre; Weinstock, Peter; Breazeal, Cynthia","Designing a Socially Assistive Robot for Pediatric Care","Proceedings of the 14th International Conference on Interaction Design and Children","978-1-4503-3590-4","","10.1145/2771839.2771923","https://doi.org/10.1145/2771839.2771923","We present the design of the Huggable robot that can playfully interact with children and provide socio-emotional support for them in pediatric care context. Our design takes into consideration that many young patients are nervous, intimidated, and are socio-emotionally vulnerable at hospitals. The Huggable robot has a childish and furry look be perceived friendly and can perform swift and smooth motions. It uses a smart phone device for its computational power and internal sensors. The robot's haptic sensors perceive physical touch and can use the information in meaningful ways. The modular arm component allows easy sensor replacement and increases the usability of the Huggable robot for various pediatric care services. From a preliminary pilot user study with two healthy and two ill children, all participants enjoyed playing with the robot but the two children with medical conditions showed caring and empathetic behaviors than the two health children. We learned various types of physical touch occurred during the child-robot interaction, and will continue to develop more intelligent haptic sensory system for the Huggable robot to better assist and support child patients' socio-emotional needs.","2015","2021-05-19 12:52:56","2021-05-19 12:52:56","","387–390","","","","","","","IDC '15","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Boston, Massachusetts","","","","child-robot interaction; healthcare robotics; pediatric care; robot design; socially assistive robotics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CEQQ4RCH","conferencePaper","2015","Ji, Sang Hoon; YOU, Su Jeong; Cho, Hye-Kyung","Design of Emotional Conversations with a Child for a Role Playing Robot","Proceedings of the Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction Extended Abstracts","978-1-4503-3318-4","","10.1145/2701973.2702009","https://doi.org/10.1145/2701973.2702009","The children who suffer from psychological and emotional disorder are unaccustomed to cooperation, shared meaning, sympathy, empathy, and magnanimity. In recent, several attempts has been tried at increasing children's social skills by emotional role-playing game with robots because the robotic system can offer dynamic, adaptive and autonomous interaction for learning of imitation skills with real-time performance evaluation and feedback. But there are limits in robot technologies. Especially, it is very difficult to understand the children's word and take suitable behaviors for the children's intents. Therefore, we suggest a method of guiding an emotional robot playing robot conversations with a child in this paper. For the purpose, we design a human-robot-interaction software and a special human intervention device (HID). And finally, we implement our suggested method with a commercial humanoid robot.","2015","2021-05-19 12:52:56","2021-05-19 12:52:56","","73–74","","","","","","","HRI'15 Extended Abstracts","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Portland, Oregon, USA","","","","emotional role playing robot; human intervention device; human-robot-interaction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CUJ77IIN","conferencePaper","2018","Kang, Dahyun; Kim, SunKyoung; Kwak, Sonya S.","The Effects of the Physical Contact in the Functional Intimate Distance on User's Acceptance toward Robots","Companion of the 2018 ACM/IEEE International Conference on Human-Robot Interaction","978-1-4503-5615-2","","10.1145/3173386.3177023","https://doi.org/10.1145/3173386.3177023","We investigated the effects of physical contact of robots on the user's acceptance in the functional intimate distance. We conducted a two (robot interaction types: interaction with physical contact vs. interaction with a tool) within-participants experiment (N=18). This study was a video-based observation study. According to the experimental results, the evaluation of participants on the empathy and sociability of the robot was not affected by physical contact in the functional intimate zone. On the other hand, the participants felt secure and perceived that the robot was knowledgeable when the robot measured the patient's temperature with a thermometer instead of its hand.","2018","2021-05-19 12:52:56","2021-05-19 12:52:56","","143–144","","","","","","","HRI '18","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Chicago, IL, USA","","","","empathy; human-robot interaction; functional intimacy; knowledgeableness; physical contact; safety; sociability; social distance","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HE7HXLVZ","conferencePaper","2018","Björling, Elin A.; Rose, Emma; Ren, Rachel","Teen-Robot Interaction: A Pilot Study of Engagement with a Low-Fidelity Prototype","Companion of the 2018 ACM/IEEE International Conference on Human-Robot Interaction","978-1-4503-5615-2","","10.1145/3173386.3177068","https://doi.org/10.1145/3173386.3177068","Today's teens will most likely be the first generation to spend a lifetime living and interacting with both mechanical and social robots. Although human-robot interaction has been explored in children, adults, and seniors, examination of teen-robot interaction has been minimal. Using human-centered design, our team is developing a social robot to gather stress and mood data from teens in a public high school. As part of our preliminary design stage, we conducted a interaction pilot study in the wild to explore and capture teens' initial interactions with a low-fidelity social robot prototype. We observed strong engagement and expressions of empathy from teens during our qualitative, interaction studies.","2018","2021-05-19 12:52:56","2021-05-19 12:52:56","","69–70","","","","","","","HRI '18","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Chicago, IL, USA","","","","engagement; prototype; teen-robot interaction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5P66JAJX","conferencePaper","2015","Hoffman, Guy; Zuckerman, Oren; Hirschberger, Gilad; Luria, Michal; Shani Sherman, Tal","Design and Evaluation of a Peripheral Robotic Conversation Companion","Proceedings of the Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction","978-1-4503-2883-8","","10.1145/2696454.2696495","https://doi.org/10.1145/2696454.2696495","We present the design, implementation, and evaluation of a peripheral empathy-evoking robotic conversation companion, Kip1. The robot's function is to increase people's awareness to the effect of their behavior towards others, potentially leading to behavior change. Specifically, Kip1 is designed to promote non-aggressive conversation between people. It monitors the conversation's nonverbal aspects and maintains an emotional model of its reaction to the conversation. If the conversation seems calm, Kip1 responds by a gesture designed to communicate curious interest. If the conversation seems aggressive, Kip1 responds by a gesture designed to communicate fear. We describe the design process of Kip1, guided by the principles of peripheral and evocative. We detail its hardware and software systems, and a study evaluating the effects of the robot's autonomous behavior on couples' conversations. We find support for our design goals. A conversation companion reacting to the conversation led to more gaze attention, but not more verbal distraction, compared to a robot that moves but does not react to the conversation. This suggests that robotic devices could be designed as companions to human-human interaction without compromising the natural communication flow between people. Participants also rated the reacting robot as having significantly more social human character traits and as being significantly more similar to them. This points to the robot's potential to elicit people's empathy.","2015","2021-05-19 12:52:56","2021-05-19 12:52:56","","3–10","","","","","","","HRI '15","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Portland, Oregon, USA","","","","empathy; human-robot interaction; design; behavior change; ambient kinetic tangibles; robotic companions; smartphone robots","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7L4WANDM","conferencePaper","2020","Kim, Jinwook; Baek, Kyungwon; Jang, Jinkyu","Petbe: Projecting a Real Being onto a Social Robot Using Contextual Data for a Pet Monitoring Method","Companion of the 2020 ACM/IEEE International Conference on Human-Robot Interaction","978-1-4503-7057-8","","10.1145/3371382.3378236","https://doi.org/10.1145/3371382.3378236","The demand for pet monitoring devices is growing due to the increasing number of one-person households raising pets. However, current monitoring methods using video camera entail various problems, which may lead to discontinued usage. To overcome this problem, we propose Petbe, a social robot that projects your own pet using a context-aware approach based on BLE beacons and Raspberry Pis. The corresponding smartphone application provides various robot status updates (robot head) and movements (robot body). With the development of Petbe, we conducted an exploratory study to verify the advancement of the above issues on monitoring user's own pets with the following factors: privacy concern, companionship, awareness, connectivity, and satisfaction. The outcomes indicate that Petbe helps to reduce privacy concerns and build companionship through empathetic interaction.","2020","2021-05-19 12:52:56","2021-05-19 12:52:56","","290–292","","","","","","","HRI '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Cambridge, United Kingdom","","","","social robot; context aware; pet monitoring; projection","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UNJC4X4P","conferencePaper","2015","Franco, Gloria Adriana Mendoza","Evaluation of the Emotional Answer in HRI on a Game Situation","Proceedings of the Latin American Conference on Human Computer Interaction","978-1-4503-3960-5","","10.1145/2824893.2824897","https://doi.org/10.1145/2824893.2824897","This project has as purpose to propose an adequate method for the assessment of the emotional answer after an interaction with a social and emotional robot. A lottery game application has been developed for playing with the robot Nao, and through an experimental scenario the empathy towards a robot has been demonstrated. As a result, the Emocards are presented as a promising assessment method for the emotional answer of the users.","2015","2021-05-19 12:52:56","2021-05-19 12:52:56","","","","","","","","","CLIHC '15","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Córdoba, Argentina","","","","empathy; interaction design; HRI; Emocards; emotional evaluation; emotional reciprocity; lottery application","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BBQGR6GS","conferencePaper","2015","Jeong, Seongmi; Gu, Jihyang; Shin, Dong-Hee","I Am Interested in What You Are Saying: Role of Nonverbal Immediacy Cues in Listening","Proceedings of the Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction Extended Abstracts","978-1-4503-3318-4","","10.1145/2701973.2702040","https://doi.org/10.1145/2701973.2702040","Immediacy plays a key role in interpersonal communication. Some of immediate behaviors in human-human interaction (i. e. gaze and nodding) have received much attention in HRI, however, others (i. e. body posture) don't. This study investigates whether robot's posture (lean forward vs. upright) and nodding manner (small and fast vs. large and slow) can affect perception of the robot. The current study argues that the lean forward and nodding manner are likely to have significant effects on psychological and behavior outcomes, including perceived empathy, human-likeness, and likability of the robot.","2015","2021-05-19 12:52:56","2021-05-19 12:52:56","","129–130","","","","","","","HRI'15 Extended Abstracts","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Portland, Oregon, USA","","","","hri; immediacy; nodding; nonverbal behavior; posture","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6D7YETPP","conferencePaper","2016","Kim, Jungwoo; Kim, Hyesook; Choi, Jaeboong","Development of Smart Product, DUET Using SQFD and Storytelling","Proceedings of HCI Korea","978-89-6848-791-0","","10.17210/hcik.2016.01.298","https://doi.org/10.17210/hcik.2016.01.298","This paper presents a smart product design process for a wearable device to provide empathy and fun to users. As the first step, keywords were extracted using open-coding methods from text WebData of online sites for wearable devices, Smardi, Sblog, and Wsite. The Smart Quality Function Deployment (SQFD) was then applied to prioritize the keywords and corresponding user requirements. The key user requirements such as 'separable band from core module' and 'function for media control' were then materialized into a wearable band, DUET, using rapid prototyping, and refined through three stages of user evaluation. DUET connectable to iOS and Android smartphones was introduced by a storytelling transmedia videoclip by experts with a theme of empathy and fun. It was also advertised on a cloud funding site, Indiegogo, and through a PPL in S entertainment program, and received positive responses. Further detailed analysis of user responses was performed for 72 days through the operation of facebook-DUET site and for 10 days through Google keyword marketing which derived various levels of user activities.","2016","2021-05-19 12:52:56","2021-05-19 12:52:56","","298–306","","","","","","","HCIK '16","","","","Hanbit Media, Inc.","Seoul, KOR","","","","","","","","event-place: Jeongseon, Republic of Korea","","","","RP; SQFD","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E7K8B82Z","conferencePaper","2019","Roy, Sayanti; Kieson, Emily; Abramson, Charles; Crick, Christopher","Mutual Reinforcement Learning with Robot Trainers","Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction","978-1-5386-8555-6","","","","The researchers in this study have developed a novel approach using mutual reinforcement learning (MRL) where both the robot and human act as empathetic individuals who function as reinforcement learning agents for each other to achieve a particular task over continuous communication and feedback. This shared model not only has a collective impact but improves human cognition and helps in building a successful human-robot relationship. In our current work, we compared our learned reinforcement model with a baseline non-reinforcement and random approach in a robotics domain to identify the significance and impact of MRL. MRL contributed to improved skill transfer, and the robot was able successfully to predict which reinforcement behaviors would be most valuable to its human partners.","2019","2021-05-19 12:52:56","2021-05-19 12:52:56","","572–573","","","","","","","HRI '19","","","","IEEE Press","","","","","","","","","event-place: Daegu, Republic of Korea","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UU3HJXVC","conferencePaper","2019","Vertesi, Janet","Seeing like a Rover: Team Work and Human-Robot Relations","Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction","978-1-5386-8555-6","","","","How do you work with a robot millions of miles away to make scientific discoveries on a planet you've never set foot on? Although much work in human-robot interaction describes the meaningful relationships that humans forge with their robots in one-on-one encounters, when robots venture into places where humans cannot go — in search and rescue operations, ocean voyages, or even into space — they do so as part of a large human team. Decisions about what the robot should do and where it should go are therefore the result of large group interactions instead of individual human cognition: the realm of organizational sociology.This talk draws on over a decade of ethnography with NASA's robotic spacecraft missions, specifically focusing on the Mars Exploration Rover mission. Going behind the scenes of the mission, I show the meaning-making, emotional connections, and embodied synergy that scientists develop as they work with their robots millions of miles away on a daily basis. This begins by learning to see through the robots' ""eyes"" on another planet, yet the peculiar social arrangement of mission work produces a deeper connection to the robot explorers too: one that is no doubt responsible for the extraordinary success and unexpected longevity of the mission team.Studying robotic spacecraft teams demonstrates how humans build a particular and peculiar empathy with their robotic colleagues that goes beyond anthropomorphism. Instead, this case points to the many and unusual ways in which organizations participate in our interactions with, understanding of, and ultimately care for the robots we work with in everyday life.","2019","2021-05-19 12:52:57","2021-05-19 12:52:57","","152","","","","","","","HRI '19","","","","IEEE Press","","","","","","","","","event-place: Daegu, Republic of Korea","","","","human-robot interaction; teamwork","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S7THQFNL","journalArticle","2020","Bagheri, Elahe; Esteban, Pablo G.; Cao, Hoang-Long; Beir, Albert De; Lefeber, Dirk; Vanderborght, Bram","An Autonomous Cognitive Empathy Model Responsive to Users’ Facial Emotion Expressions","ACM Trans. Interact. Intell. Syst.","","2160-6455","10.1145/3341198","https://doi.org/10.1145/3341198","Successful social robot services depend on how robots can interact with users. The effective service can be obtained through smooth, engaged, and humanoid interactions in which robots react properly to a user’s affective state. This article proposes a novel Automatic Cognitive Empathy Model, ACEM, for humanoid robots to achieve longer and more engaged human-robot interactions (HRI) by considering humans’ emotions and replying to them appropriately. The proposed model continuously detects the affective states of a user based on facial expressions and generates desired, either parallel or reactive, empathic behaviors that are already adapted to the user’s personality. Users’ affective states are detected using a stacked autoencoder network that is trained and tested on the RAVDESS dataset.The overall proposed empathic model is verified throughout an experiment, where different emotions are triggered in participants and then empathic behaviors are applied based on proposed hypothesis. The results confirm the effectiveness of the proposed model in terms of related social and friendship concepts that participants perceived during interaction with the robot.","2020-11","2021-05-19 12:52:57","2021-05-19 12:52:57","","","","3","10","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Empathy; social robots; human robot interaction; adaptive interaction; facial emotion detection; non-verbal behavior","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LR7R6T2K","conferencePaper","2015","Hood, Deanna; Lemaignan, Séverin; Dillenbourg, Pierre","When Children Teach a Robot to Write: An Autonomous Teachable Humanoid Which Uses Simulated Handwriting","Proceedings of the Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction","978-1-4503-2883-8","","10.1145/2696454.2696479","https://doi.org/10.1145/2696454.2696479","This article presents a novel robotic partner which children can teach handwriting. The system relies on the learning by teaching paradigm to build an interaction, so as to stimulate meta-cognition, empathy and increased self-esteem in the child user. We hypothesise that use of a humanoid robot in such a system could not just engage an unmotivated student, but could also present the opportunity for children to experience physically-induced benefits encountered during human-led handwriting interventions, such as motor mimicry. By leveraging simulated handwriting on a synchronised tablet display, a NAO humanoid robot with limited fine motor capabilities has been configured as a suitably embodied handwriting partner. Statistical shape models derived from principal component analysis of a dataset of adult-written letter trajectories allow the robot to draw purposefully deformed letters. By incorporating feedback from user demonstrations, the system is then able to learn the optimal parameters for the appropriate shape models. Preliminary in situ studies have been conducted with primary school classes to obtain insight into children's use of the novel system. Children aged 6-8 successfully engaged with the robot and improved its writing to a level which they were satisfied with. The validation of the interaction represents a significant step towards an innovative use for robotics which addresses a widespread and socially meaningful challenge in education.","2015","2021-05-19 12:52:57","2021-05-19 12:52:57","","83–90","","","","","","","HRI '15","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Portland, Oregon, USA","","","","education; human-robot interaction; learning by teaching","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UFDLZAS9","conferencePaper","2020","Pollmann, Kathrin; Ziegler, Daniel","Personal Quizmaster: A Pattern Approach to Personalized Interaction Experiences with the MiRo Robot","Proceedings of the Conference on Mensch Und Computer","978-1-4503-7540-5","","10.1145/3404983.3410414","https://doi.org/10.1145/3404983.3410414","In Human-Robot Interaction, personalization has been proposed as a strategy to increase acceptance for social robots. The present paper describes how behavioral design patterns can be used to tailor the interaction experience to the individual user's characteristics and needs. To demonstrate this approach, we designed a quiz game application for the MiRo robot. The robot acts as the quizmaster and shows different behaviors (coach-like/empathic vs. challenging/provocative) depending on the type of user who is playing the game (community-focused vs. competition-focused player). We describe the process of creating the two quizmaster personalities and related behavioral patterns as well as the technical background for integrating them with the interaction model for the quiz game. The result is a Wizard-of-Oz demonstration of the personalizable quiz game that is accompanied by an interactive video prototype remote for user studies and demo purposes.","2020","2021-05-19 12:52:57","2021-05-19 12:52:57","","485–489","","","","","","","MuC '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Magdeburg, Germany","","","","social robot; behavioral patterns; multimodal behavioral expressions; personalized human-robot interaction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JPKDHXJZ","conferencePaper","2021","Chirapornchai, Chatchai; Bremner, Paul; Daly, Joseph E.","Helper's High with a Robot Pet","Companion of the 2021 ACM/IEEE International Conference on Human-Robot Interaction","978-1-4503-8290-8","","10.1145/3434074.3447165","https://doi.org/10.1145/3434074.3447165","Helper's high is the phenomenon that helping someone or something else can lead to psychological benefits such as mood improvement. This study investigates if a robot pet can, like a real pet, induce helpers high in people interacting with it. A Vector robot was programmed to express the need for daily exercise and attention, and participants were instructed how to help the robot meet those needs. Our within subjects design had two conditions: with and without emotional behaviour modifiers to the robot's behaviour. Our primary research question is whether behaviours that conveyed emotion as well as needs would lead to empathy in the participants, which would create a stronger helper's high effect than purely functional need expression behaviours. We present a long-term (4 day) remote study design that not only facilitates the kind of interactions needed for helper's high, but abides by government guidelines on Covid-19 safety (under which a laboratory study is not possible). Preliminary results suggest that Vector was able to improve the mood of some participants, and mood changes tend to be greater when Vector expressed behaviours with emotional components. Our post-study interview data suggests that individual differences in living environment and mood impacting external factors, affected Vector's efficacy in mood influencing.","2021","2021-05-19 12:52:57","2021-05-19 12:52:57","","229–233","","","","","","","HRI '21 Companion","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Boulder, CO, USA","","","","empathy; helper's high; mood improvement; robot pet; vector","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JVBLT7IY","conferencePaper","2016","Spaulding, Samuel; Gordon, Goren; Breazeal, Cynthia","Affect-Aware Student Models for Robot Tutors","Proceedings of the 2016 International Conference on Autonomous Agents &amp; Multiagent Systems","978-1-4503-4239-1","","","","Computational tutoring systems, such as educational software or interactive robots, have the potential for great societal benefit. Such systems track and assess students' knowledge via inferential methods, such as the popular Bayesian Knowledge Tracing (BKT) algorithm. However, these methods do not typically draw on the affective signals that human teachers use to assess knowledge, such as indications of discomfort, engagement, or frustration.In this paper we present a novel extension to the BKT model that uses affective data, derived autonomously from video records of children playing an interactive story-telling game with a robot, to infer student knowledge of reading skills. We find that, compared to a control group of children who played the game with only a tablet, children who interacted with an embodied social robot generated stronger affective data signals of engagement and enjoyment during the interaction. We then show that incorporating this affective data into model training improves the quality of the learned knowledge inference models.These results suggest that physically embodied, affect-aware robot tutors can provide more effective and empathic educational experiences for children, and advance both algorithmic and human-centered motivations for further development of systems that tightly integrate affect understanding and complex models of inference with interactive, educational robots.","2016","2021-05-19 12:52:57","2021-05-19 12:52:57","","864–872","","","","","","","AAMAS '16","","","","International Foundation for Autonomous Agents and Multiagent Systems","Richland, SC","","","","","","","","event-place: Singapore, Singapore","","","","affective computing; child-robot interaction; socially assistive robots; educational robots","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BFCKYF8N","conferencePaper","2018","Spaulding, Samuel; Chen, Huili; Ali, Safinah; Kulinski, Michael; Breazeal, Cynthia","A Social Robot System for Modeling Children's Word Pronunciation: Socially Interactive Agents Track","Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems","","","","","Autonomous educational social robots can be used to help promote literacy skills in young children. Such robots, which emulate the emotive, perceptual, and empathic abilities of human teachers, are capable of replicating some of the benefits of one-on-one tutoring from human teachers, in part by leveraging individual student's behavior and task performance data to infer sophisticated models of their knowledge. These student models are then used to provide personalized educational experiences by, for example, determining the optimal sequencing of curricular material. In this paper we introduce an integrated system for autonomously analyzing and assessing children's speech and pronunciation in the context of an interactive word game between a social robot and a child. We present a novel game environment and its computational formulation, an integrated pipeline for capturing and analyzing children's speech in real-time, and an autonomous robot that models children's word pronunciation via Gaussian Process Regression (GPR), augmented with an Active Learning protocol that informs the robot's behavior. We show that the system is capable of autonomously assessing children's pronunciation ability, with ground truth determined by a post-experiment evaluation by human raters. We also compare phoneme- and word-level GPR models and discuss trade-offs of each approach in modeling children's pronunciation. Finally, we describe and analyze a pipeline for automatic analysis of children's speech and pronunciation, including an evaluation of SpeechAce as a tool for future development of autonomous, speech-based language tutors.","2018","2021-05-19 12:52:57","2021-05-19 12:52:57","","1658–1666","","","","","","","AAMAS '18","","","","International Foundation for Autonomous Agents and Multiagent Systems","Richland, SC","","","","","","","","event-place: Stockholm, Sweden","","","","social robot; human-robot interaction; intelligent tutoring systems; gaussian processl; speech-based systems; student modeling","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GFY8RIRN","conferencePaper","2018","Wen, James; Stewart, Amanda; Billinghurst, Mark; Dey, Arindam; Tossell, Chad; Finomore, Victor","He Who Hesitates is Lost (...in Thoughts over a Robot)","Proceedings of the Technology, Mind, and Society","978-1-4503-5420-2","","10.1145/3183654.3183703","https://doi.org/10.1145/3183654.3183703","In a team, the strong bonds that can form between teammates are often seen as critical for reaching peak performance. This perspective may need to be reconsidered, however, if some team members are autonomous robots since establishing bonds with fundamentally inanimate and expendable objects may prove counterproductive. Previous work has measured empathic responses towards robots as singular events at the conclusion of experimental sessions. As relationships extend over long periods of time, sustained empathic behavior towards robots would be of interest. In order to measure user actions that may vary over time and are affected by empathy towards a robot teammate, we created the TEAMMATE simulation system. Our findings suggest that inducing empathy through a back story narrative can significantly change participant decisions in actions that may have consequences for a robot companion over time. The results of our study can have strong implications for the overall performance of human machine teams.","2018","2021-05-19 12:52:57","2021-05-19 12:52:57","","","","","","","","","TechMindSociety '18","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Washington, DC, USA","","","","Robotics; Empathy; Anthropomorphism; Human Machine Team; User Study","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JS25ETMC","conferencePaper","2015","Hood, Deanna; Lemaignan, Séverin; Dillenbourg, Pierre","The CoWriter Project: Teaching a Robot How to Write","Proceedings of the Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction Extended Abstracts","978-1-4503-3318-4","","10.1145/2701973.2702091","https://doi.org/10.1145/2701973.2702091","This video (that accompanies the paper ""When Children Teach a Robot to Write: An Autonomous Teachable Humanoid Which Uses Simulated Handwriting"" by the same authors, and presented as well during this conference) presents the first results of the EPFL CoWriter project. The project aims at building a robotic partner which children can teach handwriting. The system allows for the learning by teaching paradigm to be employed in the interaction, so as to stimulate meta-cognition, empathy and increased self-esteem in the child user. It is hypothesised that use of a humanoid robot in such a system could not just engage an unmotivated student, but could also present the opportunity for children to experience physically-induced benefits encountered during human-led handwriting interventions, such as motor mimicry.","2015","2021-05-19 12:52:57","2021-05-19 12:52:57","","269","","","","","","","HRI'15 Extended Abstracts","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Portland, Oregon, USA","","","","education; human-robot interaction; learning by teaching","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FCHKUGGN","conferencePaper","2019","Zelenskaya, Maria; Harvey, Louise","Virtual Avatars as a Tool for Audience Engagement","The 17th International Conference on Virtual-Reality Continuum and Its Applications in Industry","978-1-4503-7002-8","","10.1145/3359997.3365717","https://doi.org/10.1145/3359997.3365717","Modern motion capture tools can be used to animate sophisticated digital characters in real time. Through these virtual avatars human performers can communicate with live audience, creating a promising new area of application for public engagement. This study describes a social experiment where a real-time multimedia setup was used to facilitate an interaction between a digital character and visitors at a public venue. The technical implementation featured some innovative elements, such as using iPhone TrueDepth Camera as part of the performance capture pipeline. The study examined public reactions during the experiment in order to explore the empathic potential of virtual avatars and assess their ability to engage live audience.","2019","2021-05-19 12:52:57","2021-05-19 12:52:57","","","","","","","","","VRCAI '19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Brisbane, QLD, Australia","","","","audience engagement; Real-time motion capture; virtual avatar","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QXV58F5S","conferencePaper","2018","Correia, Filipa; Mascarenhas, Samuel; Prada, Rui; Melo, Francisco S.; Paiva, Ana","Group-Based Emotions in Teams of Humans and Robots","Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction","978-1-4503-4953-6","","10.1145/3171221.3171252","https://doi.org/10.1145/3171221.3171252","Providing social robots an internal model of emotions can help them guide their behaviour in a more humane manner by simulating the ability to feel empathy towards others. Furthermore, the growing interest in creating robots that are capable of collaborating with other humans in team settings provides an opportunity to explore another side of human emotion, namely, group-based emotions. This paper contributes with the first model on group-based emotions in social robotic partners. We defined a model of group-based emotions for social robots that allowed us to create two distinct robotic characters that express either individual or group-based emotions. This paper also contributes with a user study where two autonomous robots embedded the previous characters, and formed two human-robot teams to play a competitive game. Our results showed that participants perceived the robot that expresses group-based emotions as more likeable and attributed higher levels of group identification and group trust towards their teams, when compared to the robotic partner that expresses individual-based emotions.","2018","2021-05-19 12:52:57","2021-05-19 12:52:57","","261–269","","","","","","","HRI '18","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Chicago, IL, USA","","","","emotion; trust; group effects; identification; inter-group interactions; self-categorisation; human-robot teamwork","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FI4R3CWW","conferencePaper","2020","Li, Yuanchao; Zhao, Tianyu; Shen, Xun","Attention-Based Multimodal Fusion for Estimating Human Emotion in Real-World HRI","Companion of the 2020 ACM/IEEE International Conference on Human-Robot Interaction","978-1-4503-7057-8","","10.1145/3371382.3378261","https://doi.org/10.1145/3371382.3378261","Toward empathetic and harmonious human-robot interaction (HRI), automatic estimation of human emotion has attracted increasing attention from multidisciplinary research fields. In this report, we propose an attention-based multimodal fusion approach that explores the space between traditional early and late fusion approaches, to deal with the problem of asynchronous multimodal inputs while considering their relatedness. The proposed approach enables the robot to align the human's visual and speech signals (more specifically, facial, acoustic, and lexical information) extracted by its cameras, microphones, and processing modules and is expected to achieve robust estimation performance in real-world HRI.","2020","2021-05-19 12:52:57","2021-05-19 12:52:57","","340–342","","","","","","","HRI '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Cambridge, United Kingdom","","","","attention mechanism; emotion estimation; human-robot interaction (hri); multimodal fusion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D9RNQPH3","conferencePaper","2018","Lehmann, Hagen; Broz, Frank","Contagious Yawning in Human-Robot Interaction","Companion of the 2018 ACM/IEEE International Conference on Human-Robot Interaction","978-1-4503-5615-2","","10.1145/3173386.3177063","https://doi.org/10.1145/3173386.3177063","This late breaking report introduces an approach to measure yawning contagion between robots and humans. Understanding to what extent yawning can be contagious between robots and humans will help to generate more believable interaction behaviors for social robots and contribute to a better understanding of cognitive phenomena like empathy and their application in HRI. We will give an overview of an experiment which used an EMYS robot for the presentation of the yawning stimulus. We will present the results of our preliminary analysis of the collected data.","2018","2021-05-19 12:52:57","2021-05-19 12:52:57","","173–174","","","","","","","HRI '18","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Chicago, IL, USA","","","","empathy; human-robot interaction; behavior contagion; yawning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EA92IU3F","conferencePaper","2017","Lin, Chaolan; Faas, Travis; Dombrowski, Lynn; Brady, Erin","Beyond Cute: Exploring User Types and Design Opportunities of Virtual Reality Pet Games","Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology","978-1-4503-5548-3","","10.1145/3139131.3139132","https://doi.org/10.1145/3139131.3139132","Virtual pet games, such as handheld games like Tamagotchi or video games like Petz, provide players with artificial pet companions or entertaining pet-raising simulations. Prior research has found that virtual pets have the potential to promote learning, collaboration, and empathy among users. While virtual reality (VR) has become an increasingly popular game medium, litle is known about users' expectations regarding game avatars, gameplay, and environments for VR-enabled pet games. We surveyed 780 respondents in an online survey and interviewed 30 participants to understand users' motivation, preferences, and game behavior in pet games played on various medium, and their expectations for VR pet games. Based on our findings, we generated three user types that reflect users' preferences and gameplay styles in VR pet games. We use these types to highlight key design opportunities and recommendations for VR pet games.","2017","2021-05-19 12:52:57","2021-05-19 12:52:57","","","","","","","","","VRST '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Gothenburg, Sweden","","","","pet game; user types; virtual pet; virtual reality","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PGF7LAMM","conferencePaper","2017","Valverde, Isabel; Cochrane, Todd","Senses Places: Soma-Tech Mixed-Reality Participatory Performance Installation/Environment","Proceedings of the 8th International Conference on Digital Arts","978-1-4503-5273-4","","10.1145/3106548.3106613","https://doi.org/10.1145/3106548.3106613","We present the latest developments of the art-tech research project Senses Places, a somatic-technological (soma-tech) mixed-reality participatory performance installation/environment, engaging expanded modes of embodied physical-virtual interaction. This ongoing somatic-technological dance/performance collaborative trans-disciplinary approach gathers artists and developer researchers, working remotely and physically in analogical-digital intermedia interfaces and their expanded experience design and choreography. The sensorial expansion and integration sought through human-computer interaction links participants, avatars, images and physical-virtual environments. They constitute different organic-artificial sensorial-expressive channels of visual, audio, tactile, and somatic/kinesthetic shared tuning/engagement/experience. At the core of this long-term intervention lies the common urging desire for more encompassing and empathic embodied interactivity among physical and remote subjects and places. With a cross-cultural somatic and dance practices, Senses Places critically experiments with different informational, communicational and biomedical technologies available, wishing to contribute to understand the world's and humans becoming through what we have been conceiving as posthuman corporealities [1] within a posthuman condition and emerging somatic epistemology [2].","2017","2021-05-19 12:52:57","2021-05-19 12:52:57","","195–197","","","","","","","ARTECH2017","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Macau, China","","","","Dance-technology; Interaction design; Interactive art; Posthuman corporealities; Somatic epistemology; Somatics; Virtual and mixed-reality","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HTPGB722","conferencePaper","2015","Encinas, Enrique; Mitchell, Robb","Cyrafour: An Experiential Activity Facilitating Empathic Distant Communication among Copresent Individuals","Proceedings of the 6th Augmented Human International Conference","978-1-4503-3349-8","","10.1145/2735711.2735815","https://doi.org/10.1145/2735711.2735815","Distant communication relies mostly on a non-embodied representation of participants (e.g. textual in chats, photographic in videoconference, auditory in telephony, etc) that lessens the sensory richness of conversational interactions. Cyrafour is a novel activity that explores the implications of using human avatars (cyranoids) for empathic interpersonal remote communication. An unscripted conversation between two individuals (the sources) is transmitted through radio waves and reproduced by two copresent subjects (the cyranoids) following certain conversational guidelines. In particular, the Sources were invited to discuss about a topic, play a conversation game and comment on an opinionated video. All Cyrafour sessions were video-taped and participants interviewed afterwards in order to support analysis and discussion. Cyrafour could be considered as a playful embodied identity game in which cyranoids are simultaneously together in and aside from a conversation generated elsewhere. This puzzling circumstance seems to allow for an empathic embodiment of the meaning transmitted and appears to create a frame for further discussion on the topics raised.","2015","2021-05-19 12:52:57","2021-05-19 12:52:57","","165–166","","","","","","","AH '15","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Singapore, Singapore","","","","embodied cognition; serious games; copresence; cyranoids; human avatars; telepresence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GN2A7HDK","conferencePaper","2020","Troiano, Giovanni Maria; Wood, Matthew; Harteveld, Casper","""And This, Kids, Is How I Met Your Mother"": Consumerist, Mundane, and Uncanny Futures with Sex Robots","Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems","978-1-4503-6708-0","","10.1145/3313831.3376598","https://doi.org/10.1145/3313831.3376598","Sex Robots are no longer science fiction and may soon be-come widespread. While much discussion has developed in academia on their moral and social impact, sex robots have yet to be examined from a critical design perspective and are under-explored in HCI. We use the Story Completion Method(SCM) to explore commonplace assumptions around futures with sex robots and discuss those from a critical design perspective. Thirty five participants completed a story stem of a human encountering a sex robot or vice-versa. Through thematic analysis, we show narratives of consumerist relation-ships between humans and sex robots, stories that describe sex robots as highly-efficient sex workers that (out)perform humans in routinal sex activities, and narratives that explore sex robots as empathetic and sentient beings. Our participant-created stories both reinforce and challenge established norms of sex robots and raise questions that concern responsible design and ethics in HCI. Finally, we show opportunities and limitations of using multiple-perspective story stems in SCM","2020","2021-05-19 12:52:57","2021-05-19 12:52:57","","1–17","","","","","","","CHI '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Honolulu, HI, USA","","","","ethics; human-robot interaction; research fiction; sex robots; sexual HCI; speculative design; story completion method","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZF7GC7B8","conferencePaper","2020","Arnett, Marcus; Luo, Zhenyang; Paladugula, Pradeep Kumar; Cardenas, Irvin Steve; Kim, Jong-Hoon","Robots Teaching Recycling: Towards Improving Environmental Literacy of Children","Companion of the 2020 ACM/IEEE International Conference on Human-Robot Interaction","978-1-4503-7057-8","","10.1145/3371382.3379462","https://doi.org/10.1145/3371382.3379462","The present pollution problem can be partially attributed to the lack of empathy for learning any ecological and environmental literacy skills. Although robotics in education is increasing, there has been a lack of interest towards developing devices designed to teach children how to be environmentally conscious, and in particular, how to recycle. This gap is the basis for our robot, which we call the Smart Trash Junior, a mechatronic trashcan that uses vision recognition to identify recyclable objects and enters into a dialogue that educates children, within elementary schools, how to recycle.","2020","2021-05-19 12:52:57","2021-05-19 12:52:57","","615–616","","","","","","","HRI '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Cambridge, United Kingdom","","","","educational robotics; children robot interaction; eco-literacy; environmental literacy","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EFWZLJVA","conferencePaper","2016","Shinohara, Yumiko; Kubo, Katsuhiro; Nozawa, Momoyo; Yoshizaki, Misa; Takahashi, Tomomi; Hayakawa, Hirofumi; Hirota, Atsushi; Nishizaki, Yukiko; Oka, Natsuki","The Optimum Rate of Mimicry in Human-Agent Interaction","Proceedings of the Fourth International Conference on Human Agent Interaction","978-1-4503-4508-8","","10.1145/2974804.2980506","https://doi.org/10.1145/2974804.2980506","The importance of building rapport between a human and an agent is increasing with the burgeoning development of robot technology. Several recent studies have focused on the chameleon effect, using psychological concepts to investigate human-agent interaction. However, the validity of the chameleon effect in human-agent interaction is controversial. Few studies have explored the influence of individual cognitive ability and the rate of mimicry on the human-agent interaction. We explored the optimal rate of mimicry and the relationship between mimicry rate and individual empathic ability. We controlled the amount of agent mimicry and examined the effect on participants classified as high- and low-perspective takers. We found that, overall, participants preferred agents that mimicked their behavior 83% of the time. Moreover, high-, but not low-, perspective takers tended to be influenced by the mimicry rate.","2016","2021-05-19 12:52:57","2021-05-19 12:52:57","","367–370","","","","","","","HAI '16","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Biopolis, Singapore","","","","human-agent interaction; perspective taking; mimicry; impression of robot; the chameleon effect","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ARGJYRMP","conferencePaper","2018","Brueckner, Sophia","Empathy Amulet: A Wearable to Connect with Strangers","Proceedings of the 2018 ACM International Symposium on Wearable Computers","978-1-4503-5967-2","","10.1145/3267242.3267301","https://doi.org/10.1145/3267242.3267301","The Empathy Amulet is a wearable interpretation of Philip K. Dick's empathy box from his novel Do Androids Dream of Electronic Sheep? [3]. In the novel, thousands of people were anonymously connected with each other both haptically and emotionally when they grabbed the handles of their empathy boxes. The Empathy Amulet similarly networks a group of strangers together through shared experiences of physical warmth. It is not yet another technology for staying in touch with people you already know (and falling short). Rather, it encourages its wearer to make a deliberate and generous choice to invest their time and energy in connection with strangers, and it incorporates reciprocity into its design, such that helping oneself means helping other people. In today's world, people are less likely to feel empathy towards those not in their immediate network of family and friends, and, despite a proliferation of connective technologies, loneliness is on the rise [2, 5]. Surprisingly, it is the perceived sense of loneliness, and not actually being physically alone that has numerous health consequences for a significant portion of the population. Lakoff and Johnson's theory of embodied mind asserts that our physical and subjective experiences are inextricably linked, and the Empathy Amulet leverages the powerful connection between the physical experience of warmth and the subjective experience of social connectedness to combat loneliness and cultivate a stronger sense of connection with strangers [1, 4].","2018","2021-05-19 12:52:57","2021-05-19 12:52:57","","248–253","","","","","","","ISWC '18","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Singapore, Singapore","","","","embodied cognition; haptic I/O; internet of things; prototyping; wearable electronics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZRZ235JD","conferencePaper","2018","Spaulding, Samuel","Personalized Robot Tutors That Learn from Multimodal Data","Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems","","","","","As the cost of sensors decreases and ability to model and learn from multi-modal data increases, researchers are exploring how to use the unique qualities of physically embodied robots to help engage students and promote learning. These robots are designed to emulate the emotive, perceptual, and empathic abilities of human teachers, and are capable of replicating some of the benefits of one-on-one tutoring from human teachers. My thesis research focuses on developing methods for robots to analyze and integrate multimodal data including speech, facial expressions, and task performance to build rich models of the user's knowledge and preferences. These student models are then used to provide personalized educational experiences, such as optimal curricular sequencing, or leaning preferences for educational style. In this abstract, we summarize past projects in this area and discuss applications such as learning from affective signals and model transfer across tasks.","2018","2021-05-19 12:52:57","2021-05-19 12:52:57","","1781–1783","","","","","","","AAMAS '18","","","","International Foundation for Autonomous Agents and Multiagent Systems","Richland, SC","","","","","","","","event-place: Stockholm, Sweden","","","","human-robot interaction; social robotics; multimodal interaction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WEYTY4TW","conferencePaper","2018","Hieida, Chie; Horii, Takato; Nagai, Takayuki","Decision-Making in Emotion Model","Companion of the 2018 ACM/IEEE International Conference on Human-Robot Interaction","978-1-4503-5615-2","","10.1145/3173386.3177048","https://doi.org/10.1145/3173386.3177048","Having emotions is essential for robots to understand and sympathize with the feelings of people. In addition, it may allow the robots to be accepted into human society. The role of emotions in decision-making is another important perspective. In this paper, a model of emotions based on various neurological and psychological findings that are related to empathic communication between humans and robots is proposed. Subsequently, a mechanism of decision-making that is based on affects using convolutional LSTM and deep Q-network is examined.","2018","2021-05-19 12:52:57","2021-05-19 12:52:57","","127–128","","","","","","","HRI '18","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Chicago, IL, USA","","","","decision-making; model of emotion; empathic hri","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C5Y6JTSN","conferencePaper","2021","Herdel, Viviane; Kuzminykh, Anastasia; Hildebrandt, Andrea; Cauchard, Jessica R.","Drone in Love: Emotional Perception of Facial Expressions on Flying Robots","Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems","978-1-4503-8096-6","","10.1145/3411764.3445495","https://doi.org/10.1145/3411764.3445495","Drones are rapidly populating human spaces, yet little is known about how these flying robots are perceived and understood by humans. Recent works suggested that their acceptance is predicated upon their sociability. This paper explores the use of facial expressions to represent emotions on social drones. We leveraged design practices from ground robotics and created a set of rendered robotic faces that convey basic emotions. We evaluated individuals’ response to these emotional facial expressions on drones in two empirical studies (N = 98, N = 98). Our results demonstrate that individuals accurately recognize five drone emotional expressions, as well as make sense of intensities within emotion categories. We describe how participants were emotionally affected by the drone, showed empathy towards it, and created narratives to interpret its emotions. As a consequence, we formulate design recommendations for social drones and discuss methodological insights on the use of static versus dynamic stimuli in affective robotics studies.","2021","2021-05-19 12:53:15","2021-05-19 12:53:15","","","","","","","","","CHI '21","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Yokohama, Japan","","","","Affective Computing; Emotion Recognition; Anthropomorphism; Robot; Facial Expressions; Human-Drone Interaction; UAV.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YIIBZ78F","conferencePaper","2019","Kuang, Quincy; Zhang, Jiaxin; Druga, Stefania","Ballbit Adventure: A Physical Game for a Collaborative Racing","Extended Abstracts of the Annual Symposium on Computer-Human Interaction in Play Companion Extended Abstracts","978-1-4503-6871-1","","10.1145/3341215.3356982","https://doi.org/10.1145/3341215.3356982","Playtime accounts for one of the most critical learning periods for children, as they learn how to interact and socialize with their playmates. In this paper, we present a new kind of cooperation-based physical game called Ballbit Adventure. Our game provides a collaborative environment for children to communicate, cooperate, and empathize through solving challenges in an interactive maze. Each player must drive a robotic ball and work together to complete different tasks that would ultimately lead them to the finish line. Through the format of a physical racing game, Ballbit Adventure hopes to show the value of face-to-face play experience to counterbalance the disconnected online interactions that children have with video games.","2019","2021-05-19 12:53:16","2021-05-19 12:53:16","","97–103","","","","","","","CHI PLAY '19 Extended Abstracts","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Barcelona, Spain","","","","cooperation based game; hybrid game; social gaming; strategic gameplay; tangible interaction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LAZ8Y4WI","conferencePaper","2018","de Jesus Santos, Fabrícia; de Almeida, Antonio Lucas; Santos, Breno Santana; de Souza, Caio César Alves; Santos, Marcos Neto","Empathic Computer Science: A Systematic Mapping","Proceedings of the 17th Brazilian Symposium on Human Factors in Computing Systems","978-1-4503-6601-4","","10.1145/3274192.3274238","https://doi.org/10.1145/3274192.3274238","Described as the capability to understand the emotional state of an individual and often express a response that resonates with it, empathy is a crucial factor for social interactions. However, the ability to express empathy diminishes as people rely more and more on technological resources to interact. Due to fact of being an essential component to become a more effective social relationship between humans and computers, there are several approaches, techniques, methods or mechanisms to promote empathy in these interactions. This paper proposes to identify and systematize mechanisms used in Computer Science to promote empathy. Thus, it was carried out a systematic mapping on the main research databases of the area. We identified the main approaches used to promote empathy, such as Empathic Robotic Agent/Device and Empathic Virtual Agent, as well as the countries that hold the most research in this line, especially the United Kingdom, Japan and USA. In addition, it was found that this area of research is still not being explored in any significant way.","2018","2021-05-19 12:53:16","2021-05-19 12:53:16","","","","","","","","","IHC 2018","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Belém, Brazil","","","","Empathy; Rapport; Computer Science; Secondary Study; Systematic Mapping","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CXIXI4SJ","conferencePaper","2016","Aylett, Ruth","Am I Bovvered? Fifteen Years of Empathic Agents","Proceedings of the 2016 International Conference on Autonomous Agents &amp; Multiagent Systems","978-1-4503-4239-1","","","","In 2001, the EU-funded project VICTEC pioneered the concept of an Empathic Agent. This was reported at AAMAS in 2004 in a well-cited paper 'Caring for Agents and Agents that Care: Building Empathic Relations with Synthetic Agents' (Paiva, Dias, Sobral, Aylett, Sobreperez, Woods, Zoll, Hall). It advanced two goals for embodied empathic agents: characters that, by their actions and behaviours, are able to show empathy (or not) for other characters; and characters that, by their appearance, situation, and behaviour, are able to trigger empathic reactions in the user.In this talk we discuss how far Embodied Empathic Agents - whether graphical or robotic - are succeeding; what we can now do, and what open research questions remain. What are the key theoretical and technological advances already made and which are still needed? What applications are Empathic Agents `good' for, and how do we know they are? And how do they relate to the broader field of social agents?","2016","2021-05-19 12:53:16","2021-05-19 12:53:16","","4","","","","","","","AAMAS '16","","","","International Foundation for Autonomous Agents and Multiagent Systems","Richland, SC","","","","","","","","event-place: Singapore, Singapore","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""